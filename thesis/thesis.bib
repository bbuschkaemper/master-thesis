
@inproceedings{yang_literature-driven_2013,
	location = {Hissar, Bulgaria},
	title = {Literature-driven Curation for Taxonomic Name Databases},
	url = {https://aclanthology.org/W13-5207/},
	eventtitle = {{SWAIE} 2013},
	pages = {25--32},
	booktitle = {Proceedings of the Joint Workshop on {NLP}\&{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction},
	publisher = {{INCOMA} Ltd. Shoumen, {BULGARIA}},
	author = {Yang, Hui and Willis, Alistair and Morse, David and de Roeck, Anne},
	editor = {Maynard, Diana and van Erp, Marieke and Davis, Brian and Osenova, Petya and Simov, Kiril and Georgiev, Georgi and Nakov, Preslav},
	urldate = {2025-08-17},
	date = {2013-09},
}

@inproceedings{jurgens_semeval-2016_2016,
	location = {San Diego, California},
	title = {{SemEval}-2016 Task 14: Semantic Taxonomy Enrichment},
	url = {https://aclanthology.org/S16-1169/},
	doi = {10.18653/v1/S16-1169},
	shorttitle = {{SemEval}-2016 Task 14},
	eventtitle = {{SemEval} 2016},
	pages = {1092--1102},
	booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation ({SemEval}-2016)},
	publisher = {Association for Computational Linguistics},
	author = {Jurgens, David and Pilehvar, Mohammad Taher},
	editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
	urldate = {2025-08-17},
	date = {2016-06},
}

@inproceedings{bordea_semeval-2016_2016,
	location = {San Diego, California},
	title = {{SemEval}-2016 Task 13: Taxonomy Extraction Evaluation ({TExEval}-2)},
	url = {https://aclanthology.org/S16-1168/},
	doi = {10.18653/v1/S16-1168},
	shorttitle = {{SemEval}-2016 Task 13},
	eventtitle = {{SemEval} 2016},
	pages = {1081--1091},
	booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation ({SemEval}-2016)},
	publisher = {Association for Computational Linguistics},
	author = {Bordea, Georgeta and Lefever, Els and Buitelaar, Paul},
	editor = {Bethard, Steven and Carpuat, Marine and Cer, Daniel and Jurgens, David and Nakov, Preslav and Zesch, Torsten},
	urldate = {2025-08-17},
	date = {2016-06},
}

@misc{gunn_creating_2024,
	title = {Creating a Fine Grained Entity Type Taxonomy Using {LLMs}},
	url = {http://arxiv.org/abs/2402.12557},
	doi = {10.48550/arXiv.2402.12557},
	abstract = {In this study, we investigate the potential of {GPT}-4 and its advanced iteration, {GPT}-4 Turbo, in autonomously developing a detailed entity type taxonomy. Our objective is to construct a comprehensive taxonomy, starting from a broad classification of entity types - including objects, time, locations, organizations, events, actions, and subjects - similar to existing manually curated taxonomies. This classification is then progressively refined through iterative prompting techniques, leveraging {GPT}-4's internal knowledge base. The result is an extensive taxonomy comprising over 5000 nuanced entity types, which demonstrates remarkable quality upon subjective evaluation. We employed a straightforward yet effective prompting strategy, enabling the taxonomy to be dynamically expanded. The practical applications of this detailed taxonomy are diverse and significant. It facilitates the creation of new, more intricate branches through pattern-based combinations and notably enhances information extraction tasks, such as relation extraction and event argument extraction. Our methodology not only introduces an innovative approach to taxonomy creation but also opens new avenues for applying such taxonomies in various computational linguistics and {AI}-related fields.},
	number = {{arXiv}:2402.12557},
	publisher = {{arXiv}},
	author = {Gunn, Michael and Park, Dohyun and Kamath, Nidhish},
	urldate = {2025-08-17},
	date = {2024-02-19},
	eprinttype = {arxiv},
	eprint = {2402.12557 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{chen_prompting_2023,
	title = {Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction},
	url = {http://arxiv.org/abs/2309.01715},
	doi = {10.48550/arXiv.2309.01715},
	shorttitle = {Prompting or Fine-tuning?},
	abstract = {Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing ({NLP}) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models ({LLMs}) have demonstrated that appropriate user inputs (called prompting) can effectively guide {LLMs}, such as {GPT}-3, in diverse {NLP} tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.},
	number = {{arXiv}:2309.01715},
	publisher = {{arXiv}},
	author = {Chen, Boqi and Yi, Fandi and Varró, Dániel},
	urldate = {2025-08-17},
	date = {2023-09-04},
	eprinttype = {arxiv},
	eprint = {2309.01715 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{kargupta_taxoadapt_2025,
	title = {{TaxoAdapt}: Aligning {LLM}-Based Multidimensional Taxonomy Construction to Evolving Research Corpora},
	url = {http://arxiv.org/abs/2506.10737},
	doi = {10.48550/arXiv.2506.10737},
	shorttitle = {{TaxoAdapt}},
	abstract = {The rapid evolution of scientific fields introduces challenges in organizing and retrieving scientific literature. While expert-curated taxonomies have traditionally addressed this need, the process is time-consuming and expensive. Furthermore, recent automatic taxonomy construction methods either (1) over-rely on a specific corpus, sacrificing generalizability, or (2) depend heavily on the general knowledge of large language models ({LLMs}) contained within their pre-training datasets, often overlooking the dynamic nature of evolving scientific domains. Additionally, these approaches fail to account for the multi-faceted nature of scientific literature, where a single research paper may contribute to multiple dimensions (e.g., methodology, new tasks, evaluation metrics, benchmarks). To address these gaps, we propose {TaxoAdapt}, a framework that dynamically adapts an {LLM}-generated taxonomy to a given corpus across multiple dimensions. {TaxoAdapt} performs iterative hierarchical classification, expanding both the taxonomy width and depth based on corpus' topical distribution. We demonstrate its state-of-the-art performance across a diverse set of computer science conferences over the years to showcase its ability to structure and capture the evolution of scientific fields. As a multidimensional method, {TaxoAdapt} generates taxonomies that are 26.51\% more granularity-preserving and 50.41\% more coherent than the most competitive baselines judged by {LLMs}.},
	number = {{arXiv}:2506.10737},
	publisher = {{arXiv}},
	author = {Kargupta, Priyanka and Zhang, Nan and Zhang, Yunyi and Zhang, Rui and Mitra, Prasenjit and Han, Jiawei},
	urldate = {2025-08-17},
	date = {2025-06-12},
	eprinttype = {arxiv},
	eprint = {2506.10737 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
}

@inproceedings{firmani_building_2024,
	title = {Building Taxonomies with Triplet Queries},
	url = {https://www.semanticscholar.org/paper/Building-Taxonomies-with-Triplet-Queries-Firmani-Galhotra/91e314dd5df505d036aa49ddeb3562551770afb6},
	abstract = {The organization of records referring to different entities into a taxonomy is crucial for capturing their relationships. Nevertheless, the automatic identification of such relationships often faces inaccuracies due to noise and heterogeneity of records across various sources. Simultaneously, manual maintenance of these relationships proves impractical and lacks scalability. This study addresses these challenges by adopting a weak supervision strategy, in the form of an oracle, to solve a novel Hierarchical Entity Resolution task. Within our framework, records are organized into a tree-like structure that encompasses records at the bottom level and encapsulates entities and categories at the higher levels. To make the most effective use of supervision, we employ a triplet comparison oracle, which takes three records as input and output the most similar pair(s). Finally, we introduce {HierER} , a querying strategy utilizing record pair similarities to minimize the number of oracle queries while simultaneously maximizing the identification of the hierarchical structure. Theoretical and empirical analyses demonstrate the effectiveness and efficiency of {HierER} with noisy datasets with millions of records.},
	eventtitle = {Sistemi Evoluti per Basi di Dati},
	author = {Firmani, D. and Galhotra, Sainyam and Saha, B. and Srivastava, Divesh},
	urldate = {2025-08-17},
	date = {2024},
}

@article{zhang_overview_2018,
	title = {An overview of multi-task learning},
	volume = {5},
	issn = {2095-5138},
	url = {https://doi.org/10.1093/nsr/nwx105},
	doi = {10.1093/nsr/nwx105},
	abstract = {As a promising area in machine learning, multi-task learning ({MTL}) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of {MTL} by first giving a definition of {MTL}. Then several different settings of {MTL} are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative {MTL} models are presented. In order to speed up the learning process, parallel and distributed {MTL} models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use {MTL} to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for {MTL} are presented.},
	pages = {30--43},
	number = {1},
	journaltitle = {National Science Review},
	shortjournal = {Natl Sci Rev},
	author = {Zhang, Yu and Yang, Qiang},
	urldate = {2025-08-17},
	date = {2018-01-01},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2025-08-17},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhuang_comprehensive_2020,
	title = {A Comprehensive Survey on Transfer Learning},
	url = {http://arxiv.org/abs/1911.02685},
	doi = {10.48550/arXiv.1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	number = {{arXiv}:1911.02685},
	publisher = {{arXiv}},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	urldate = {2025-08-17},
	date = {2020-06-23},
	eprinttype = {arxiv},
	eprint = {1911.02685 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pan_survey_2010,
	title = {A Survey on Transfer Learning},
	volume = {22},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/5288526},
	doi = {10.1109/TKDE.2009.191},
	abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
	pages = {1345--1359},
	number = {10},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Pan, Sinno Jialin and Yang, Qiang},
	urldate = {2025-08-17},
	date = {2010-10},
	keywords = {Data mining, Knowledge engineering, Knowledge transfer, Labeling, Learning systems, Machine learning, Machine learning algorithms, Space technology, Testing, Training data, Transfer learning, data mining., machine learning, survey},
}

@inproceedings{netzer_reading_2011,
	title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
	url = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
	booktitle = {{NIPS} Workshop on Deep Learning and Unsupervised Feature Learning 2011},
	author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
	date = {2011},
}

@article{deng_mnist_2012,
	title = {The mnist database of handwritten digit images for machine learning research},
	volume = {29},
	pages = {141--142},
	number = {6},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Deng, Li},
	date = {2012},
}

@online{noauthor_wordnet_nodate,
	title = {{WordNet}},
	url = {https://wordnet.princeton.edu/homepage},
	abstract = {Any opinions, findings, and conclusions or recommendations expressed in this material are those of the creators of {WordNet} and do not necessarily reflect the views of any funding agency or Princeton University.What is {WordNet}?Current Status of the {WordNet} {ProjectPrinceton} {WordNet} is no longer developed, though the database and all tools are freely},
	urldate = {2025-08-06},
	langid = {english},
}

@misc{loshchilov_decoupled_2017,
	title = {Decoupled Weight Decay Regularization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.05101},
	doi = {10.48550/ARXIV.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	publisher = {{arXiv}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2025-06-21},
	date = {2017},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE}), Optimization and Control (math.{OC})},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	series = {{ICML}'13},
	abstract = {Deep and recurrent neural networks ({DNNs} and {RNNs} respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both {DNNs} and {RNNs} (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	pages = {III--1139--III--1147},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	publisher = {{JMLR}.org},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	date = {2013},
	note = {event-place: Atlanta, {GA}, {USA}},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1207.0580},
	doi = {10.48550/ARXIV.1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	publisher = {{arXiv}},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	urldate = {2025-06-21},
	date = {2012},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE})},
}

@article{he_identity_2016,
	title = {Identity Mappings in Deep Residual Networks},
	journaltitle = {{arXiv} preprint {arXiv}:1603.05027},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2016},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1512.03385},
	doi = {10.48550/ARXIV.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \&amp; {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2025-06-20},
	date = {2015},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{griffin_caltech_2022,
	title = {Caltech 256},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://data.caltech.edu/records/20087},
	doi = {10.22002/D1.20087},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	publisher = {{CaltechDATA}},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	urldate = {2025-06-20},
	date = {2022-04-06},
	langid = {english},
	keywords = {computer vision, machine learning},
}

@inproceedings{radford_learning_2021,
	title = {Learning transferable visual models from natural language supervision},
	pages = {8748--8763},
	booktitle = {International conference on machine learning},
	publisher = {{PmLR}},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and {others}},
	date = {2021},
}

@book{tanimoto_elementary_1958,
	title = {An Elementary Mathematical Theory of Classification and Prediction},
	pagetotal = {10},
	publisher = {International Business Machines Corporation},
	author = {Tanimoto, T. T.},
	date = {1958},
	langid = {english},
	note = {Google-Books-{ID}: yp34HAAACAAJ},
}

@article{jaccard_distribution_1912,
	title = {The Distribution of the Flora in the Alpine Zone.},
	volume = {11},
	issn = {1469-8137},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8137.1912.tb05611.x},
	doi = {10.1111/j.1469-8137.1912.tb05611.x},
	pages = {37--50},
	number = {2},
	journaltitle = {New Phytologist},
	author = {Jaccard, Paul},
	urldate = {2025-06-01},
	date = {1912},
	langid = {english},
}

@misc{horn_inaturalist_2018,
	title = {The {iNaturalist} Species Classification and Detection Dataset},
	url = {http://arxiv.org/abs/1707.06642},
	doi = {10.48550/arXiv.1707.06642},
	abstract = {Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the {iNaturalist} species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67\% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.},
	number = {{arXiv}:1707.06642},
	publisher = {{arXiv}},
	author = {Horn, Grant Van and Aodha, Oisin Mac and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	urldate = {2025-05-03},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1707.06642 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kuznetsova_open_2020,
	title = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},
	journaltitle = {{IJCV}},
	author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
	date = {2020},
}

@book{fellbaum_wordnet_1998,
	title = {{WordNet}: An Electronic Lexical Database},
	isbn = {9780262272551},
	url = {https://doi.org/10.7551/mitpress/7287.001.0001},
	abstract = {{WordNet} is an on-line lexical reference system whose design isinspired by current psycholinguistic theories of human lexical memory;version 1.6 is the most up-to-date version of the system.{WordNet}, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets.The purpose of this volume is twofold. First, it discusses the design of {WordNet} and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains. Contributors Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen {VoorheesBradford} Books imprint},
	publisher = {The {MIT} Press},
	author = {Fellbaum, Christiane},
	date = {1998-05},
	doi = {10.7551/mitpress/7287.001.0001},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A Large-Scale Hierarchical Image Database},
	booktitle = {{CVPR}09},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
	date = {2009},
}

@misc{uijlings_missing_2022,
	title = {The Missing Link: Finding label relations across datasets},
	url = {http://arxiv.org/abs/2206.04453},
	doi = {10.48550/arXiv.2206.04453},
	shorttitle = {The Missing Link},
	abstract = {Computer vision is driven by the many datasets available for training or evaluating novel methods. However, each dataset has a different set of class labels, visual definition of classes, images following a specific distribution, annotation protocols, etc. In this paper we explore the automatic discovery of visual-semantic relations between labels across datasets. We aim to understand how instances of a certain class in a dataset relate to the instances of another class in another dataset. Are they in an identity, parent/child, overlap relation? Or is there no link between them at all? To find relations between labels across datasets, we propose methods based on language, on vision, and on their combination. We show that we can effectively discover label relations across datasets, as well as their type. We apply our method to four applications: understand label relations, identify missing aspects, increase label specificity, and predict transfer learning gains. We conclude that label relations cannot be established by looking at the names of classes alone, as they depend strongly on how each of the datasets was constructed.},
	number = {{arXiv}:2206.04453},
	publisher = {{arXiv}},
	author = {Uijlings, Jasper and Mensink, Thomas and Ferrari, Vittorio},
	urldate = {2025-04-23},
	date = {2022-08-09},
	eprinttype = {arxiv},
	eprint = {2206.04453 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bevandic_automatic_2022,
	title = {Automatic universal taxonomies for multi-domain semantic segmentation},
	url = {http://arxiv.org/abs/2207.08445},
	doi = {10.48550/arXiv.2207.08445},
	abstract = {Training semantic segmentation models on multiple datasets has sparked a lot of recent interest in the computer vision community. This interest has been motivated by expensive annotations and a desire to achieve proficiency across multiple visual domains. However, established datasets have mutually incompatible labels which disrupt principled inference in the wild. We address this issue by automatic construction of universal taxonomies through iterative dataset integration. Our method detects subset-superset relationships between dataset-specific labels, and supports learning of sub-class logits by treating super-classes as partial labels. We present experiments on collections of standard datasets and demonstrate competitive generalization performance with respect to previous work.},
	number = {{arXiv}:2207.08445},
	publisher = {{arXiv}},
	author = {Bevandić, Petra and Šegvić, Siniša},
	urldate = {2025-04-21},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2207.08445 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hagberg_exploring_2008,
	location = {Pasadena, {CA} {USA}},
	title = {Exploring Network Structure, Dynamics, and Function using {NetworkX}},
	pages = {11 -- 15},
	booktitle = {Proceedings of the 7th Python in Science Conference},
	author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J.},
	editor = {Varoquaux, Gaël and Vaught, Travis and Millman, Jarrod},
	date = {2008},
}

@misc{devries_improved_2017,
	title = {Improved Regularization of Convolutional Neural Networks with Cutout},
	url = {http://arxiv.org/abs/1708.04552},
	doi = {10.48550/arXiv.1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the {CIFAR}-10, {CIFAR}-100, and {SVHN} datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
	number = {{arXiv}:1708.04552},
	publisher = {{arXiv}},
	author = {{DeVries}, Terrance and Taylor, Graham W.},
	urldate = {2025-04-01},
	date = {2017-11-29},
	eprinttype = {arxiv},
	eprint = {1708.04552 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{henry_leveraging_2019,
	title = {Leveraging the bfloat16 Artificial Intelligence Datatype For Higher-Precision Computations},
	url = {http://arxiv.org/abs/1904.06376},
	doi = {10.48550/arXiv.1904.06376},
	abstract = {In recent years fused-multiply-add ({FMA}) units with lower-precision multiplications and higher-precision accumulation have proven useful in machine learning/artificial intelligence applications, most notably in training deep neural networks due to their extreme computational intensity. Compared to classical {IEEE}-754 32 bit ({FP}32) and 64 bit ({FP}64) arithmetic, these reduced precision arithmetic can naturally be sped up disproportional to their shortened width. The common strategy of all major hardware vendors is to aggressively further enhance their performance disproportionately. One particular {FMA} operation that multiplies two {BF}16 numbers while accumulating in {FP}32 has been found useful in deep learning, where {BF}16 is the 16-bit floating point datatype with {IEEE} {FP}32 numerical range but 8 significant bits of precision. In this paper, we examine the use this {FMA} unit to implement higher-precision matrix routines in terms of potential performance gain and implications on accuracy. We demonstrate how a decomposition into multiple smaller datatypes can be used to assemble a high-precision result, leveraging the higher precision accumulation of the {FMA} unit. We first demonstrate that computations of vector inner products and by natural extension, matrix-matrix products can be achieved by decomposing {FP}32 numbers in several {BF}16 numbers followed by appropriate computations that can accommodate the dynamic range and preserve accuracy compared to standard {FP}32 computations, while projecting up to 5.2x speed-up. Furthermore, we examine solution of linear equations formulated in the residual form that allows for iterative refinement. We demonstrate that the solution obtained to be comparable to those offered by {FP}64 under a large range of linear system condition numbers.},
	number = {{arXiv}:1904.06376},
	publisher = {{arXiv}},
	author = {Henry, Greg and Tang, Ping Tak Peter and Heinecke, Alexander},
	urldate = {2025-03-31},
	date = {2019-04-12},
	eprinttype = {arxiv},
	eprint = {1904.06376 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Numerical Analysis},
}

@misc{kolesnikov_big_2020,
	title = {Big Transfer ({BiT}): General Visual Representation Learning},
	url = {http://arxiv.org/abs/1912.11370},
	doi = {10.48550/arXiv.1912.11370},
	shorttitle = {Big Transfer ({BiT})},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer ({BiT}). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. {BiT} performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. {BiT} achieves 87.5\% top-1 accuracy on {ILSVRC}-2012, 99.4\% on {CIFAR}-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark ({VTAB}). On small datasets, {BiT} attains 76.8\% on {ILSVRC}-2012 with 10 examples per class, and 97.0\% on {CIFAR}-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	number = {{arXiv}:1912.11370},
	publisher = {{arXiv}},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	urldate = {2025-03-31},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {1912.11370 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@software{falcon_pytorch_2019,
	title = {{PyTorch} Lightning},
	url = {https://github.com/Lightning-AI/lightning},
	version = {1.4},
	author = {Falcon, William and {The PyTorch Lightning team}},
	date = {2019-03},
	doi = {10.5281/zenodo.3828935},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	url = {http://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	urldate = {2025-03-19},
	date = {2009},
}

@misc{li_caltech_2022,
	title = {Caltech 101},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://data.caltech.edu/records/20086},
	doi = {10.22002/D1.20086},
	abstract = {Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a {MATLAB} script to view the annotations, 'show\_annotations.m'.},
	version = {1.0},
	publisher = {{CaltechDATA}},
	author = {Li, Fei-Fei and Andreeto, Marco and Ranzato, Marc'Aurelio and Perona, Pietro},
	urldate = {2025-03-19},
	date = {2022-04-06},
	langid = {english},
	keywords = {computer vision, machine learning},
}

@misc{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	url = {http://arxiv.org/abs/1409.0575},
	doi = {10.48550/arXiv.1409.0575},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	number = {{arXiv}:1409.0575},
	publisher = {{arXiv}},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2025-03-19},
	date = {2015-01-30},
	eprinttype = {arxiv},
	eprint = {1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bevandic_weakly_2024,
	title = {Weakly supervised training of universal visual concepts for multi-domain semantic segmentation},
	volume = {132},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/2212.10340},
	doi = {10.1007/s11263-024-01986-z},
	abstract = {Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on multiple datasets becomes a method of choice towards strong generalization in usual scenes and graceful performance degradation in edge cases. Unfortunately, different datasets often have incompatible labels. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. Furthermore, many datasets have overlapping labels. For instance, pickups are labeled as trucks in {VIPER}, cars in Vistas, and vans in {ADE}20k. We address this challenge by considering labels as unions of universal visual concepts. This allows seamless and principled learning on multi-domain dataset collections without requiring any relabeling effort. Our method achieves competitive within-dataset and cross-dataset generalization, as well as ability to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the {WildDash} 2 benchmark.},
	pages = {2450--2472},
	number = {7},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Bevandić, Petra and Oršić, Marin and Grubišić, Ivan and Šarić, Josip and Šegvić, Siniša},
	urldate = {2025-03-19},
	date = {2024-07},
	eprinttype = {arxiv},
	eprint = {2212.10340 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
