
@misc{loshchilov_decoupled_2017,
	title = {Decoupled Weight Decay Regularization},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1711.05101},
	doi = {10.48550/ARXIV.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	publisher = {{arXiv}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2025-06-21},
	date = {2017},
	keywords = {{FOS}: Computer and information sciences, {FOS}: Mathematics, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE}), Optimization and Control (math.{OC})},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	series = {{ICML}'13},
	abstract = {Deep and recurrent neural networks ({DNNs} and {RNNs} respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both {DNNs} and {RNNs} (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	pages = {III--1139--III--1147},
	booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	publisher = {{JMLR}.org},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	date = {2013},
	note = {event-place: Atlanta, {GA}, {USA}},
}

@misc{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1207.0580},
	doi = {10.48550/ARXIV.1207.0580},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
	publisher = {{arXiv}},
	author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
	urldate = {2025-06-21},
	date = {2012},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), Neural and Evolutionary Computing (cs.{NE})},
}

@article{he_identity_2016,
	title = {Identity Mappings in Deep Residual Networks},
	journaltitle = {{arXiv} preprint {arXiv}:1603.05027},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2016},
}

@misc{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1512.03385},
	doi = {10.48550/ARXIV.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \&amp; {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	publisher = {{arXiv}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2025-06-20},
	date = {2015},
	keywords = {Computer Vision and Pattern Recognition (cs.{CV}), {FOS}: Computer and information sciences},
}

@misc{griffin_caltech_2022,
	title = {Caltech 256},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://data.caltech.edu/records/20087},
	doi = {10.22002/D1.20087},
	abstract = {We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.},
	publisher = {{CaltechDATA}},
	author = {Griffin, Gregory and Holub, Alex and Perona, Pietro},
	urldate = {2025-06-20},
	date = {2022-04-06},
	langid = {english},
	keywords = {computer vision, machine learning},
}

@inproceedings{radford_learning_2021,
	title = {Learning transferable visual models from natural language supervision},
	pages = {8748--8763},
	booktitle = {International conference on machine learning},
	publisher = {{PmLR}},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and {others}},
	date = {2021},
}

@book{tanimoto_elementary_1958,
	title = {An Elementary Mathematical Theory of Classification and Prediction},
	pagetotal = {10},
	publisher = {International Business Machines Corporation},
	author = {Tanimoto, T. T.},
	date = {1958},
	langid = {english},
	note = {Google-Books-{ID}: yp34HAAACAAJ},
}

@article{jaccard_distribution_1912,
	title = {The Distribution of the Flora in the Alpine Zone.},
	volume = {11},
	issn = {1469-8137},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8137.1912.tb05611.x},
	doi = {10.1111/j.1469-8137.1912.tb05611.x},
	pages = {37--50},
	number = {2},
	journaltitle = {New Phytologist},
	author = {Jaccard, Paul},
	urldate = {2025-06-01},
	date = {1912},
	langid = {english},
}

@misc{horn_inaturalist_2018,
	title = {The {iNaturalist} Species Classification and Detection Dataset},
	url = {http://arxiv.org/abs/1707.06642},
	doi = {10.48550/arXiv.1707.06642},
	abstract = {Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the {iNaturalist} species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67\% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.},
	number = {{arXiv}:1707.06642},
	publisher = {{arXiv}},
	author = {Horn, Grant Van and Aodha, Oisin Mac and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
	urldate = {2025-05-03},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1707.06642 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{kuznetsova_open_2020,
	title = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},
	journaltitle = {{IJCV}},
	author = {Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and Duerig, Tom and Ferrari, Vittorio},
	date = {2020},
}

@book{fellbaum_wordnet_1998,
	title = {{WordNet}: An Electronic Lexical Database},
	isbn = {9780262272551},
	url = {https://doi.org/10.7551/mitpress/7287.001.0001},
	abstract = {{WordNet} is an on-line lexical reference system whose design isinspired by current psycholinguistic theories of human lexical memory;version 1.6 is the most up-to-date version of the system.{WordNet}, an electronic lexical database, is considered to be the most important resource available to researchers in computational linguistics, text analysis, and many related areas. Its design is inspired by current psycholinguistic and computational theories of human lexical memory. English nouns, verbs, adjectives, and adverbs are organized into synonym sets, each representing one underlying lexicalized concept. Different relations link the synonym sets.The purpose of this volume is twofold. First, it discusses the design of {WordNet} and the theoretical motivations behind it. Second, it provides a survey of representative applications, including word sense identification, information retrieval, selectional preferences of verbs, and lexical chains. Contributors Reem Al-Halimi, Robert C. Berwick, J. F. M. Burg, Martin Chodorow, Christiane Fellbaum, Joachim Grabowski, Sanda Harabagiu, Marti A. Hearst, Graeme Hirst, Douglas A. Jones, Rick Kazman, Karen T. Kohl, Shari Landes, Claudia Leacock, George A. Miller, Katherine J. Miller, Dan Moldovan, Naoyuki Nomura, Uta Priss, Philip Resnik, David St-Onge, Randee Tengi, Reind P. van de Riet, Ellen {VoorheesBradford} Books imprint},
	publisher = {The {MIT} Press},
	author = {Fellbaum, Christiane},
	date = {1998-05},
	doi = {10.7551/mitpress/7287.001.0001},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: A Large-Scale Hierarchical Image Database},
	booktitle = {{CVPR}09},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
	date = {2009},
}

@misc{uijlings_missing_2022,
	title = {The Missing Link: Finding label relations across datasets},
	url = {http://arxiv.org/abs/2206.04453},
	doi = {10.48550/arXiv.2206.04453},
	shorttitle = {The Missing Link},
	abstract = {Computer vision is driven by the many datasets available for training or evaluating novel methods. However, each dataset has a different set of class labels, visual definition of classes, images following a specific distribution, annotation protocols, etc. In this paper we explore the automatic discovery of visual-semantic relations between labels across datasets. We aim to understand how instances of a certain class in a dataset relate to the instances of another class in another dataset. Are they in an identity, parent/child, overlap relation? Or is there no link between them at all? To find relations between labels across datasets, we propose methods based on language, on vision, and on their combination. We show that we can effectively discover label relations across datasets, as well as their type. We apply our method to four applications: understand label relations, identify missing aspects, increase label specificity, and predict transfer learning gains. We conclude that label relations cannot be established by looking at the names of classes alone, as they depend strongly on how each of the datasets was constructed.},
	number = {{arXiv}:2206.04453},
	publisher = {{arXiv}},
	author = {Uijlings, Jasper and Mensink, Thomas and Ferrari, Vittorio},
	urldate = {2025-04-23},
	date = {2022-08-09},
	eprinttype = {arxiv},
	eprint = {2206.04453 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{bevandic_automatic_2022,
	title = {Automatic universal taxonomies for multi-domain semantic segmentation},
	url = {http://arxiv.org/abs/2207.08445},
	doi = {10.48550/arXiv.2207.08445},
	abstract = {Training semantic segmentation models on multiple datasets has sparked a lot of recent interest in the computer vision community. This interest has been motivated by expensive annotations and a desire to achieve proficiency across multiple visual domains. However, established datasets have mutually incompatible labels which disrupt principled inference in the wild. We address this issue by automatic construction of universal taxonomies through iterative dataset integration. Our method detects subset-superset relationships between dataset-specific labels, and supports learning of sub-class logits by treating super-classes as partial labels. We present experiments on collections of standard datasets and demonstrate competitive generalization performance with respect to previous work.},
	number = {{arXiv}:2207.08445},
	publisher = {{arXiv}},
	author = {Bevandić, Petra and Šegvić, Siniša},
	urldate = {2025-04-21},
	date = {2022-10-26},
	eprinttype = {arxiv},
	eprint = {2207.08445 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hagberg_exploring_2008,
	location = {Pasadena, {CA} {USA}},
	title = {Exploring Network Structure, Dynamics, and Function using {NetworkX}},
	pages = {11 -- 15},
	booktitle = {Proceedings of the 7th Python in Science Conference},
	author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J.},
	editor = {Varoquaux, Gaël and Vaught, Travis and Millman, Jarrod},
	date = {2008},
}

@misc{devries_improved_2017,
	title = {Improved Regularization of Convolutional Neural Networks with Cutout},
	url = {http://arxiv.org/abs/1708.04552},
	doi = {10.48550/arXiv.1708.04552},
	abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the {CIFAR}-10, {CIFAR}-100, and {SVHN} datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
	number = {{arXiv}:1708.04552},
	publisher = {{arXiv}},
	author = {{DeVries}, Terrance and Taylor, Graham W.},
	urldate = {2025-04-01},
	date = {2017-11-29},
	eprinttype = {arxiv},
	eprint = {1708.04552 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{henry_leveraging_2019,
	title = {Leveraging the bfloat16 Artificial Intelligence Datatype For Higher-Precision Computations},
	url = {http://arxiv.org/abs/1904.06376},
	doi = {10.48550/arXiv.1904.06376},
	abstract = {In recent years fused-multiply-add ({FMA}) units with lower-precision multiplications and higher-precision accumulation have proven useful in machine learning/artificial intelligence applications, most notably in training deep neural networks due to their extreme computational intensity. Compared to classical {IEEE}-754 32 bit ({FP}32) and 64 bit ({FP}64) arithmetic, these reduced precision arithmetic can naturally be sped up disproportional to their shortened width. The common strategy of all major hardware vendors is to aggressively further enhance their performance disproportionately. One particular {FMA} operation that multiplies two {BF}16 numbers while accumulating in {FP}32 has been found useful in deep learning, where {BF}16 is the 16-bit floating point datatype with {IEEE} {FP}32 numerical range but 8 significant bits of precision. In this paper, we examine the use this {FMA} unit to implement higher-precision matrix routines in terms of potential performance gain and implications on accuracy. We demonstrate how a decomposition into multiple smaller datatypes can be used to assemble a high-precision result, leveraging the higher precision accumulation of the {FMA} unit. We first demonstrate that computations of vector inner products and by natural extension, matrix-matrix products can be achieved by decomposing {FP}32 numbers in several {BF}16 numbers followed by appropriate computations that can accommodate the dynamic range and preserve accuracy compared to standard {FP}32 computations, while projecting up to 5.2x speed-up. Furthermore, we examine solution of linear equations formulated in the residual form that allows for iterative refinement. We demonstrate that the solution obtained to be comparable to those offered by {FP}64 under a large range of linear system condition numbers.},
	number = {{arXiv}:1904.06376},
	publisher = {{arXiv}},
	author = {Henry, Greg and Tang, Ping Tak Peter and Heinecke, Alexander},
	urldate = {2025-03-31},
	date = {2019-04-12},
	eprinttype = {arxiv},
	eprint = {1904.06376 [cs]},
	keywords = {Computer Science - Mathematical Software, Computer Science - Numerical Analysis},
}

@misc{kolesnikov_big_2020,
	title = {Big Transfer ({BiT}): General Visual Representation Learning},
	url = {http://arxiv.org/abs/1912.11370},
	doi = {10.48550/arXiv.1912.11370},
	shorttitle = {Big Transfer ({BiT})},
	abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer ({BiT}). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. {BiT} performs well across a surprisingly wide range of data regimes -- from 1 example per class to 1M total examples. {BiT} achieves 87.5\% top-1 accuracy on {ILSVRC}-2012, 99.4\% on {CIFAR}-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark ({VTAB}). On small datasets, {BiT} attains 76.8\% on {ILSVRC}-2012 with 10 examples per class, and 97.0\% on {CIFAR}-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
	number = {{arXiv}:1912.11370},
	publisher = {{arXiv}},
	author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
	urldate = {2025-03-31},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {1912.11370 [cs]},
	note = {version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@software{falcon_pytorch_2019,
	title = {{PyTorch} Lightning},
	url = {https://github.com/Lightning-AI/lightning},
	version = {1.4},
	author = {Falcon, William and {The PyTorch Lightning team}},
	date = {2019-03},
	doi = {10.5281/zenodo.3828935},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	url = {http://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf},
	author = {Krizhevsky, Alex and Hinton, Geoffrey},
	urldate = {2025-03-19},
	date = {2009},
}

@misc{li_caltech_2022,
	title = {Caltech 101},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://data.caltech.edu/records/20086},
	doi = {10.22002/D1.20086},
	abstract = {Pictures of objects belonging to 101 categories. About 40 to 800 images per category. Most categories have about 50 images. Collected in September 2003 by Fei-Fei Li, Marco Andreetto, and Marc'Aurelio Ranzato. The size of each image is roughly 300 x 200 pixels. We have carefully clicked outlines of each object in these pictures, these are included under the 'Annotations.tar'. There is also a {MATLAB} script to view the annotations, 'show\_annotations.m'.},
	version = {1.0},
	publisher = {{CaltechDATA}},
	author = {Li, Fei-Fei and Andreeto, Marco and Ranzato, Marc'Aurelio and Perona, Pietro},
	urldate = {2025-03-19},
	date = {2022-04-06},
	langid = {english},
	keywords = {computer vision, machine learning},
}

@misc{russakovsky_imagenet_2015,
	title = {{ImageNet} Large Scale Visual Recognition Challenge},
	url = {http://arxiv.org/abs/1409.0575},
	doi = {10.48550/arXiv.1409.0575},
	abstract = {The {ImageNet} Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	number = {{arXiv}:1409.0575},
	publisher = {{arXiv}},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	urldate = {2025-03-19},
	date = {2015-01-30},
	eprinttype = {arxiv},
	eprint = {1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bevandic_weakly_2024,
	title = {Weakly supervised training of universal visual concepts for multi-domain semantic segmentation},
	volume = {132},
	issn = {0920-5691, 1573-1405},
	url = {http://arxiv.org/abs/2212.10340},
	doi = {10.1007/s11263-024-01986-z},
	abstract = {Deep supervised models have an unprecedented capacity to absorb large quantities of training data. Hence, training on multiple datasets becomes a method of choice towards strong generalization in usual scenes and graceful performance degradation in edge cases. Unfortunately, different datasets often have incompatible labels. For instance, the Cityscapes road class subsumes all driving surfaces, while Vistas defines separate classes for road markings, manholes etc. Furthermore, many datasets have overlapping labels. For instance, pickups are labeled as trucks in {VIPER}, cars in Vistas, and vans in {ADE}20k. We address this challenge by considering labels as unions of universal visual concepts. This allows seamless and principled learning on multi-domain dataset collections without requiring any relabeling effort. Our method achieves competitive within-dataset and cross-dataset generalization, as well as ability to learn visual concepts which are not separately labeled in any of the training datasets. Experiments reveal competitive or state-of-the-art performance on two multi-domain dataset collections and on the {WildDash} 2 benchmark.},
	pages = {2450--2472},
	number = {7},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Bevandić, Petra and Oršić, Marin and Grubišić, Ivan and Šarić, Josip and Šegvić, Siniša},
	urldate = {2025-03-19},
	date = {2024-07},
	eprinttype = {arxiv},
	eprint = {2212.10340 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
