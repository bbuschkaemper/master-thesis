\chapter{Results}

\section{Domain-Model Training}

\subsection{Datasets}

To start with our taxonomy generation,
we first need a set of datasets that we can use to train and evaluate our domain models:

\begin{itemize}
      \item \textbf{Caltech-101 and Caltech-256}~\cite{li_caltech_2022,griffin_caltech_2022}:
            The Caltech-101 dataset contains 101 general object categories with 40 to 800 images per category,
            while the Caltech-256 dataset extends this to 256 categories with at least 80 images per category.
            Both datasets have been widely used for image classification tasks\footnote{Over 500 open-access papers have cited the datasets, according to Papers with Code: \url{https://paperswithcode.com/dataset/caltech-101} and \url{https://paperswithcode.com/dataset/caltech-256}}.
            The images are roughly 300x200 pixels in size and contain annotated outlines
            for each object in the image, which we will not need for our purposes.
            The dataset has no predefined train/test split,
            so we will use a 80/10/10 split for training, validation, and testing.
      \item \textbf{CIFAR-100}~\cite{krizhevsky_learning_2009}:
            The CIFAR-100 datasets contains 100 classes grouped into 20 superclasses,
            with 600 images per class.
            Each image is 32x32 pixels in size, which is significantly smaller than the Caltech datasets.
            The dataset is one of the most popular datasets for image classification tasks\footnote{Over 5000 open-access papers have cited the dataset, according to Papers with Code: \url{https://paperswithcode.com/dataset/cifar-100}}.
            The dataset has a train/test split of 50000 training images and 10000 test images,
            which we will further split by dividing the training set into 80\% for training and 20\% for validation.
      \item \textbf{Synthetic Datasets}:
            To have a ground truth for our taxonomy generation,
            we will also create synthetic datasets based on the Caltech-101 and CIFAR-100 datasets.
            These datasets will be used to evaluate our cross-domain relationship graph generation
            methods (see Section~\ref{sec:graph_construction}).
            We will create synthetic datasets of varying sizes and complexity and evaluate
            how well our methods perform for different challenges.
\end{itemize}

\subsection{Training Domain Models}

\subsubsection{Model Architecture}

To now start our training of domain models,
we first need to define the architecture of our models.
The ResNet architecture\cite{he_deep_2015,he_identity_2016} is a popular choice for image classification tasks
and has been shown to perform well on a variety of datasets.
It also has the advantage of being pre-trained on the ImageNet dataset~\cite{deng_imagenet_2009,russakovsky_imagenet_2015},
which will save us the effort of training a model from scratch.
From the available ResNet architecture sizes,
we decide for the leaner ResNet-50 architecture to meet our resource constraints.

We adapt the ResNet-50 architecture to our datasets by switching out
the final fully connected layer with a funnel architecture that ends
in an output layer matching the number of classes per dataset (see Figure~\ref{fig:resnet_funnel}).

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/resnet_funnel.pdf}
      \caption{Our ResNet-50 architecture with a funnel layer for classification.
            Blue blocks represent the input from the ResNet-50 architecture,
            red blocks represent the final output layer,
            and green blocks represent our new funnel layers.}
      \label{fig:resnet_funnel}
\end{figure}

\subsubsection{Training Procedure}

In our initial training runs,
we observe severe overfitting on the training data as can be seen in Figure~\ref{fig:overfitting}.
To mitigate this, we apply several regularisation techniques:
\begin{itemize}
      \item \textbf{Dropout}~\cite{hinton_improving_2012}:
            As can be seen in Figure~\ref{fig:resnet_funnel},
            our fully connected layers contain dropout layers between them at rates of 0.5 and 0.2.
            Dropout is a regularisation technique that randomly sets a fraction of the input units to zero during training,
            which reinforces the model to learn more robust features and thereby reduces overfitting.
      \item \textbf{Data Augmentation}:
            We apply data augmentation techniques to our training data,
            such as random cropping, horizontal flipping, random erasing, and color jittering.
            These techniques artificially increase the size of our training dataset
            and help the model to generalise better by exposing it to a wider variety of input data.
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/cifar100_overfitting.pgf}}
      \caption{Overfitting on the CIFAR-100 dataset during training.
            The blue line represents the training accuracy,
            while the orange line represents the validation accuracy.
            The model overfits on the training data, resulting in a significant gap
            between the training and validation accuracy.}
      \label{fig:overfitting}
\end{figure}

We now train our models on the Caltech-101, Caltech-256, and CIFAR-100 datasets
(i.e. their synthetic variants).

For the easier Caltech-101 and Caltech-256 datasets,
we use the SGD optimiser~\cite{sutskever_importance_2013} with a learning rate of 0.01,
a Nesterov momentum of 0.9, and a weight decay of 0.0001 and train all variants for 50 epochs.
We also use a batch size of 64 for the datasets.

For our more complex CIFAR-100 dataset,
we use the AdamW optimiser~\cite{loshchilov_decoupled_2017} with an initial learning rate of 0.001, a weight decay of 0.001.
We train for 100 epochs with a multistep learning rate scheduler that reduces the learning rate by a factor of 0.1 at epochs 30, 60 and 80.
For the smaller CIFAR-100 images we use a batch size of 256.

The training is performed on a single NVIDIA RTX 3070 GPU with 8GB of VRAM
using the PyTorch Lightning framework~\cite{falcon_pytorch_2019}.
The training process takes approximately 5 hours for the Caltech-101 and Caltech-256
synthetic dataset variants and approximately 3 hours for the CIFAR-100 synthetic dataset variants.

\subsection{Model Performance}

\subsubsection{Synthetic Variants}

For our evaluation of relationship selection methods (see Section~\ref{sec:relationship_selection}),
we need synthetic dataset variants to calculate evaluation metrics on.

We select the general-purpose Caltech-256 and CIFAR-100 datasets
and create synthetic variants of these datasets:
\begin{itemize}
      \item \textbf{Caltech-256 2-Domain Variant 1}:
            We create a basic 2-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_2domain}.
      \item \textbf{Caltech-256 2-Domain Variant 2}:
            We create a simpler 2-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=200$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=2$, and $\sigma^2_{\text{classes}}=1$.
            This variant has fewer concepts per class and therefore fewer relationships
            between the classes, which might be more similar to simple real-world datasets.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_2domain_variant}.
      \item \textbf{Caltech-256 3-Domain Variant}:
            We create a more complex 3-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=5$, and $\sigma^2_{\text{classes}}=1$.
            This variant has more concepts per class and therefore more relationships
            between the classes, which makes it more challenging for our relationship selection methods.
            These extreme numbers should be seen less as a realistic dataset
            and more as a stress test for our methods.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_3domain}.
      \item \textbf{CIFAR-100 2-Domain Variant}:
            For the CIFAR-100 dataset,
            our 2-domain variant has parameters $\mu_{\text{concepts}}=50$, $\sigma^2_{\text{concepts}}=5$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            In the Caltech-256 dataset variants we have used approximately $70\%$ of the classes as concepts,
            while in the CIFAR-100 dataset variants we use approximately $50\%$ of the classes as concepts.
            This results in a smaller, more manageable relationship graph
            that can be better manually inspected.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:cifar100_2domain}.
\end{itemize}

\begin{figure}[ht]
      \centering
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_2domain.png}
            \caption{Caltech-256 2-domain variant 1\\
            ($\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=3$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_2domain}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_2domain_variant.png}
            \caption{Caltech-256 2-domain variant 2\\
            ($\mu_{\text{concepts}}=200$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=2$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_2domain_variant}
      \end{subfigure}

      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_3domain.png}
            \caption{Caltech-256 3-domain variant\\
            ($\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=5$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_3domain}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/cifar100_2domain.png}
            \caption{CIFAR-100 2-domain variant\\
            ($\mu_{\text{concepts}}=50$, $\sigma^2_{\text{concepts}}=5$,\\
            $\mu_{\text{classes}}=3$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:cifar100_2domain}
      \end{subfigure}

      \caption{Synthetic dataset variants showing their relationship graphs before applying universal taxonomy algorithms.
            The number of concepts and classes per concept are sampled from truncated normal distributions with the parameters shown in each subfigure caption.}
      \label{fig:synthetic_variants}
\end{figure}

\subsubsection{Model Accuracy}

Let us now take a look at the accuracy of our models trained on the synthetic dataset variants.
We use checkpoints to save the model after each epoch and pick the model checkpoint with the lowest validation loss
for our final evaluation.

We can see our final training runs in Figure~\ref{fig:all_training_runs}.
It can be observed that our overfitting mitigation techniques have worked sufficiently well,
as our training and validation accuracy curves do not diverge significantly.
We present the final model accuracies on the test sets in Table~\ref{tab:evaluation_results}.
Multiple things can be observed:
\begin{itemize}
      \item All the models achieve an accuracy of around 0.8 on the test set,
            which is an average performance for these datasets.
            Our focus is not on achieving state-of-the-art performance,
            but rather on creating models suitable for our cross-domain prediction task.
            It should be noted that a lower model accuracy will lead to worse performance
            in our relationship selection methods, but since we will compare the methods against each other
            using the same models, this should not be a problem.
      \item The CIFAR-100 variants have a slightly lower accuracy than the Caltech-256 variants,
            which is expected since the CIFAR-100 dataset has closely related classes
            categorised into superclasses, which make it harder for a model to distinguish between them.
      \item The number of concepts (i.e. classes) in the original dataset that get merged
            into a new class in the synthetic dataset variants does not seem to have a significant impact
            on the model accuracy: The Caltech-256 2-domain variant 2 has a $\mu_{\text{classes}}=2$,
            while the Caltech-256 3-domain variant has a $\mu_{\text{classes}}=5$,
            but the deviation in accuracy is negligible ($\leq 0.05$).
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/all_training_runs.pgf}}

      \caption{Accuracy curves for all synthetic dataset variants.
            Each subplot shows training and validation accuracy over training steps for all domains in that variant.
            The models achieve final training accuracies of approximately 0.96-0.98 and validation accuracies of approximately 0.73-0.83.}
      \label{fig:all_training_runs}
\end{figure}

\input{figures/evaluation_results.tex}

Now that we have sufficiently good domain models,
we can use them to evaluate our relationship selection methods
and generate taxonomies from the relationship graphs we create.

\section{Taxonomy Generation}

\subsection{Relationship Selection Methods} \label{sec:relationship_selection}

\subsubsection{Synthetic Dataset Variants}

\graphicspath{{figures/}}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/naive_threshold_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{naive thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:naive_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/density_threshold_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{density thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:density_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/hypothesis_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{hypothesis method}
            for different upper bounds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:hypothesis_method_precision_recall}
\end{figure}

\input{figures/relationship_methods_results.tex}

Now that we have trained our domain models on the synthetic dataset variants,
we can use them to evaluate our different relationship selection methods.

A good relationship selection method needs to be versatile enough
to work on a range of different datasets with different characteristics:
\begin{itemize}
      \item The number of relationships between classes can vary significantly,
            depending on the number of concepts and classes in the dataset.
            We therefore need a method that adapts to the probability distribution of the relationships
            instead of picking a fixed number of relationships.
      \item The number of classes in the dataset can also vary significantly,
            which means that some datasets might have classes with few, high probability relationships,
            while others might have classes with many, low probability relationships.
            Our method needs to be able to handle both cases
            and not be biased towards either.
\end{itemize}

Our edge difference ratio metric (see Section~\ref{sec:edr})
is a good indicator of the overall accuracy of the relationship selection methods
since it measures not only the number of relationships selected,
but also the correctness (i.e. the edge weights) of the relationships.

However, since every selected relationship will later result
in a node in the universal model,
we need to give special attention to the correctness of the relationships selected.
Therefore, we will also evaluate the precision and recall of the selected relationships
for each method on the synthetic dataset variants.

We will evalute every relationship selection method on every dataset variant
by selecting the method parameters that yield the best edge difference ratio
on that dataset variant.

When looking at the results in Table~\ref{tab:relationship_methods_best_edr},
we can observe two things:
\begin{itemize}
      \item The naive thresholding method consistently performs best on all dataset variants,
            closely followed by the relationship hypothesis method.
            However, the threshold parameter for the naive thresholding method
            stays relatively consistent across the dataset variants ($0.10 - 0.15$),
            while the relationship hypothesis method has a much wider range of upper bounds ($4 - 6$).
      \item The original most common foreign prediction method
            from the Bevandic et al. paper~\cite{bevandic_automatic_2022} performs significantly
            worse than the other methods with an edge difference ratio of over $0.5$ on all dataset variants.
            This was to be expected since our synthetic dataset variants rarely have
            classes with only a single outgoing relationship.
\end{itemize}

Let us take a deeper look at each of the methods and their performance
on the synthetic dataset variants using precision-recall curves for different parameters:
\begin{itemize}
      \item The \textbf{naive thresholding method} (see Figure~\ref{fig:naive_thresholding_precision_recall})
            performs well on all dataset variants, achieving a high precision and recall
            for thresholds between $0.10$ and $0.15$.
            The displayed precision-recall curves stay consistent across the dataset variants,
            which indicates that the method is robust for different dataset characteristics.
      \item The \textbf{density thresholding method} (see Figure~\ref{fig:density_thresholding_precision_recall})
            performs worse than the naive thresholding method,
            but still achieves good performance.
            Similar to the naive thresholding method, the CIFAR-100 dataset variant
            has a slightly lower precision and recall than the Caltech-256 dataset variants,
            which is expected since the CIFAR-100 dataset has more closely related classes
            that are difficult to distinguish.
      \item The \textbf{relationship hypothesis method} (see Figure~\ref{fig:hypothesis_method_precision_recall})
            performs similar to the density thresholding method,
            but has a conspicuous drop for the CIFAR-100 dataset variant
            (prominently more so than the other methods).
            This is likely due to the fact that this method assumes that
            every true relationship has an equal probability,
            which is not the case for the CIFAR-100 dataset variant
            (where some merged classes, due to having similar concepts that are hard to distinguish,
            have a much higher probability than others).
\end{itemize}

When using our method on real-world datasets,
we do not have a ground truth to adjust the parameters to.
Therefore, we need a single parameter configuration for every method
that works universally across all datasets.

We create a new evaluation on the dataset variants using the parameters
that yield the best edge difference ratio averaged across all dataset variants.
The results are shown in Table~\ref{tab:relationship_methods_global_optimal}.

The best performing naive thresholding method
achieves an edge difference ratio of $0.463$
with a noticably higher recall ($0.889$) than precision ($0.675$).
This means that while we hit almost 90\% of the true relationships,
we also select a lot of false relationships.

It can be observed that, for precsion, the mcfp method performs best
with a precision of $0.865$,
which is expected since it only selects relationships
that has the highest probability.
However, this comes at the cost of a very low recall of $0.421$
which ultimately results in a lower F1 score compared to the naive thresholding method.

We can conclude that, if you want to create singular high-confidence relationships,
the mcfp method is the best choice (especially for single-hierarchy taxonomies).
However, if you want to capture as many shared concepts as possible,
the naive thresholding method is the best choice, closely followed by the relationship hypothesis method
(maybe as a possible alternative if the naive thresholding method does not yield good results).

\input{figures/relationship_methods_global_optimal.tex}

However, these synthetic dataset variants might not be representative of real-world datasets,
due to their shared image data as well as their \enquote{clean} relationships.
But since our final goal is to create a universal model that works on multiple domains
without separate head models, we can simply compare the universal model performance
with our best candidate relationship selection methods
(i.e. naive thresholding vs. most common foreign prediction).

\subsection{Universal Taxonomy Generation}

% TODO Manual look at caltech101-caltech256 taxonomy with mcfp (easier to look at).
% TODO Also explain and plot training
% TODO Explain the original taxonomy algorithm from Bevandic paper and what we changed (classification vs. segmentation, weighted vs. unweighted, multi-domain vs. 2 domains, etc.)

\section{Universal Models}

% TODO - New model with universal classes as output layer
% TODO - We train on domain datasets:
% TODO  - Map prediction from universal classes to domain classes by using relationship probabilities
% TODO  - Loss function is cross-entropy between predicted mapping and ground truth domain classes
% TODO  - Compare accuracy of universal taxonomy model on domain datasets with baseline models trained only on the domain datasets
% TODO - Question: How are taxonomy correctness and model accuracy on domain datasets related?
% TODO - Question: How does the model accuracy on domain datasets change with different universal taxonomy algorithms?