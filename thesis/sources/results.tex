\chapter{Results}

\section{Domain Models}

\subsection{Datasets}

To start with our taxonomy generation,
we first need a set of datasets that we can use to train and evaluate our domain models:

\begin{itemize}
      \item \textbf{Caltech-101 and Caltech-256}~\cite{li_caltech_2022,griffin_caltech_2022}:
            The Caltech-101 dataset contains 101 general object categories with 40 to 800 images per category,
            while the Caltech-256 dataset extends this to 256 categories with at least 80 images per category.
            Both datasets have been widely used for image classification tasks\footnote{Over 500 open-access papers have cited the datasets, according to Papers with Code: \url{https://paperswithcode.com/dataset/caltech-101} and \url{https://paperswithcode.com/dataset/caltech-256}}.
            The images are roughly 300x200 pixels in size and contain annotated outlines
            for each object in the image, which we will not need for our purposes.
            The dataset has no predefined train/test split,
            so we will use a 80/10/10 split for training, validation, and testing.
      \item \textbf{CIFAR-100}~\cite{krizhevsky_learning_2009}:
            The CIFAR-100 datasets contains 100 classes grouped into 20 superclasses,
            with 600 images per class.
            Each image is 32x32 pixels in size, which is significantly smaller than the Caltech datasets.
            The dataset is one of the most popular datasets for image classification tasks\footnote{Over 5000 open-access papers have cited the dataset, according to Papers with Code: \url{https://paperswithcode.com/dataset/cifar-100}}.
            The dataset has a train/test split of 50000 training images and 10000 test images,
            which we will further split by dividing the training set into 80\% for training and 20\% for validation.
      \item \textbf{Synthetic Datasets}:
            To have a ground truth for our taxonomy generation,
            we will also create synthetic datasets based on the Caltech-101 and CIFAR-100 datasets.
            These datasets will be used to evaluate our cross-domain relationship graph generation
            methods (see Section~\ref{sec:graph_construction}).
            We will create synthetic datasets of varying sizes and complexity and evaluate
            how well our methods perform for different challenges.
\end{itemize}

\subsection{Training}

\subsubsection{Model Architecture}

To now start our training of domain models,
we first need to define the architecture of our models.
The ResNet architecture\cite{he_deep_2015,he_identity_2016} is a popular choice for image classification tasks
and has been shown to perform well on a variety of datasets.
It also has the advantage of being pre-trained on the ImageNet dataset~\cite{deng_imagenet_2009,russakovsky_imagenet_2015},
which will save us the effort of training a model from scratch.
From the available ResNet architecture sizes,
we decide for the leaner ResNet-50 architecture to meet our resource constraints.

We adapt the ResNet-50 architecture to our datasets by switching out
the final fully connected layer with a funnel architecture that ends
in an output layer matching the number of classes per dataset (see Figure~\ref{fig:resnet_funnel}).

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/resnet_funnel.pdf}
      \caption{Our ResNet-50 architecture with a funnel layer for classification.
            Blue blocks represent the input from the ResNet-50 architecture,
            red blocks represent the final output layer,
            and green blocks represent our new funnel layers.}
      \label{fig:resnet_funnel}
\end{figure}

\subsubsection{Training Procedure}

In our initial training runs,
we observe severe overfitting on the training data as can be seen in Figure~\ref{fig:overfitting}.
To mitigate this, we apply several regularisation techniques:
\begin{itemize}
      \item \textbf{Dropout}~\cite{hinton_improving_2012}:
            As can be seen in Figure~\ref{fig:resnet_funnel},
            our fully connected layers contain dropout layers between them at rates of 0.5 and 0.2.
            Dropout is a regularisation technique that randomly sets a fraction of the input units to zero during training,
            which reinforces the model to learn more robust features and thereby reduces overfitting.
      \item \textbf{Data Augmentation}:
            We apply data augmentation techniques to our training data,
            such as random cropping, horizontal flipping, random erasing, and color jittering.
            These techniques artificially increase the size of our training dataset
            and help the model to generalise better by exposing it to a wider variety of input data.
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/cifar100_overfitting.pgf}}
      \caption{Overfitting on the CIFAR-100 dataset during training.
            The blue line represents the training accuracy,
            while the orange line represents the validation accuracy.
            The model overfits on the training data, resulting in a significant gap
            between the training and validation accuracy.}
      \label{fig:overfitting}
\end{figure}

We now train our models on the Caltech-101, Caltech-256, and CIFAR-100 datasets
(i.e. their synthetic variants).

For the easier Caltech-101 and Caltech-256 datasets,
we use the SGD optimiser~\cite{sutskever_importance_2013} with a learning rate of 0.01,
a Nesterov momentum of 0.9, and a weight decay of 0.0001 and train all variants for 50 epochs.
We also use a batch size of 64 for the datasets.

For our more complex CIFAR-100 dataset,
we use the AdamW optimiser~\cite{loshchilov_decoupled_2017} with an initial learning rate of 0.001, a weight decay of 0.001.
We train for 100 epochs with a multistep learning rate scheduler that reduces the learning rate by a factor of 0.1 at epochs 30, 60 and 80.
For the smaller CIFAR-100 images we use a batch size of 256.

The training is performed on a single NVIDIA RTX 3070 GPU with 8GB of VRAM
using the PyTorch Lightning framework~\cite{falcon_pytorch_2019}.
The training process takes approximately 5 hours for the Caltech-101 and Caltech-256
synthetic dataset variants and approximately 3 hours for the CIFAR-100 synthetic dataset variants.

\subsection{Model Performance}

\subsubsection{Synthetic Variants}

For our evaluation of relationship selection methods (see Section~\ref{sec:relationship_selection}),
we need synthetic dataset variants to calculate evaluation metrics on.

We select the general-purpose Caltech-256 and CIFAR-100 datasets
and create synthetic variants of these datasets:
\begin{itemize}
      \item \textbf{Caltech-256 2-Domain Variant 1}:
            We create a basic 2-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_2domain}.
      \item \textbf{Caltech-256 2-Domain Variant 2}:
            We create a simpler 2-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=200$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=2$, and $\sigma^2_{\text{classes}}=1$.
            This variant has fewer concepts per class and therefore fewer relationships
            between the classes, which might be more similar to simple real-world datasets.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_2domain_variant}.
      \item \textbf{Caltech-256 3-Domain Variant}:
            We create a more complex 3-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=5$, and $\sigma^2_{\text{classes}}=1$.
            This variant has more concepts per class and therefore more relationships
            between the classes, which makes it more challenging for our relationship selection methods.
            These extreme numbers should be seen less as a realistic dataset
            and more as a stress test for our methods.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_3domain}.
      \item \textbf{CIFAR-100 2-Domain Variant}:
            For the CIFAR-100 dataset,
            our 2-domain variant has parameters $\mu_{\text{concepts}}=50$, $\sigma^2_{\text{concepts}}=5$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            In the Caltech-256 dataset variants we have used approximately $70\%$ of the classes as concepts,
            while in the CIFAR-100 dataset variants we use approximately $50\%$ of the classes as concepts.
            This results in a smaller, more manageable relationship graph
            that can be better manually inspected.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:cifar100_2domain}.
\end{itemize}

\begin{figure}[ht]
      \centering
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_2domain.png}
            \caption{Caltech-256 2-domain variant 1\\
            ($\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=3$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_2domain}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_2domain_variant.png}
            \caption{Caltech-256 2-domain variant 2\\
            ($\mu_{\text{concepts}}=200$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=2$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_2domain_variant}
      \end{subfigure}

      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/caltech256_3domain.png}
            \caption{Caltech-256 3-domain variant\\
            ($\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,\\
            $\mu_{\text{classes}}=5$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:caltech256_3domain}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/cifar100_2domain.png}
            \caption{CIFAR-100 2-domain variant\\
            ($\mu_{\text{concepts}}=50$, $\sigma^2_{\text{concepts}}=5$,\\
            $\mu_{\text{classes}}=3$, $\sigma^2_{\text{classes}}=1$)}
            \label{fig:cifar100_2domain}
      \end{subfigure}

      \caption{Synthetic dataset variants showing their relationship graphs before applying universal taxonomy algorithms.
            The number of concepts and classes per concept are sampled from truncated normal distributions with the parameters shown in each subfigure caption.}
      \label{fig:synthetic_variants}
\end{figure}

\subsubsection{Model Accuracy}

Let us now take a look at the accuracy of our models trained on the synthetic dataset variants.
We use checkpoints to save the model after each epoch and pick the model checkpoint with the lowest validation loss
for our final evaluation.

We can see our final training runs in Figure~\ref{fig:all_training_runs}.
It can be observed that our overfitting mitigation techniques have worked sufficiently well,
as our training and validation accuracy curves do not diverge significantly.
We present the final model accuracies on the test sets in Table~\ref{tab:evaluation_results}.
Multiple things can be observed:
\begin{itemize}
      \item All the models achieve an accuracy of around 0.8 on the test set,
            which is an average performance for these datasets.
            Our focus is not on achieving state-of-the-art performance,
            but rather on creating models suitable for our cross-domain prediction task.
            It should be noted that a lower model accuracy will lead to worse performance
            in our relationship selection methods, but since we will compare the methods against each other
            using the same models, this should not be a problem.
      \item The CIFAR-100 variants have a slightly lower accuracy than the Caltech-256 variants,
            which is expected since the CIFAR-100 dataset has closely related classes
            categorised into superclasses, which make it harder for a model to distinguish between them.
      \item The number of concepts (i.e. classes) in the original dataset that get merged
            into a new class in the synthetic dataset variants does not seem to have a significant impact
            on the model accuracy: The Caltech-256 2-domain variant 2 has a $\mu_{\text{classes}}=2$,
            while the Caltech-256 3-domain variant has a $\mu_{\text{classes}}=5$,
            but the deviation in accuracy is negligible ($\leq 0.05$).
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/all_training_runs.pgf}}

      \caption{Accuracy curves for all synthetic dataset variants.
            Each subplot shows training and validation accuracy over training steps for all domains in that variant.
            The models achieve final training accuracies of approximately 0.96-0.98 and validation accuracies of approximately 0.73-0.83.}
      \label{fig:all_training_runs}
\end{figure}

\input{figures/evaluation_results.tex}

Now that we have sufficiently good domain models,
we can use them to evaluate our relationship selection methods
and generate taxonomies from the relationship graphs we create.

\section{Taxonomy Generation}

\subsection{Relationship Selection Methods} \label{sec:relationship_selection}

\subsubsection{Synthetic Dataset Variants}

\graphicspath{{figures/}}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/naive_threshold_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{naive thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:naive_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/density_threshold_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{density thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:density_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/hypothesis_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{hypothesis method}
            for different upper bounds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:hypothesis_method_precision_recall}
\end{figure}

\input{figures/relationship_methods_results.tex}

Now that we have trained our domain models on the synthetic dataset variants,
we can use them to evaluate our different relationship selection methods.

A good relationship selection method needs to be versatile enough
to work on a range of different datasets with different characteristics:
\begin{itemize}
      \item The number of relationships between classes can vary significantly,
            depending on the number of concepts and classes in the dataset.
            We therefore need a method that adapts to the probability distribution of the relationships
            instead of picking a fixed number of relationships.
      \item The number of classes in the dataset can also vary significantly,
            which means that some datasets might have classes with few, high probability relationships,
            while others might have classes with many, low probability relationships.
            Our method needs to be able to handle both cases
            and not be biased towards either.
\end{itemize}

Our edge difference ratio metric (see Section~\ref{sec:edr})
is a good indicator of the overall accuracy of the relationship selection methods
since it measures not only the number of relationships selected,
but also the correctness (i.e. the edge weights) of the relationships.

However, since every selected relationship will later result
in a node in the universal model,
we need to give special attention to the correctness of the relationships selected.
Therefore, we will also evaluate the precision and recall of the selected relationships
for each method on the synthetic dataset variants.

We will evalute every relationship selection method on every dataset variant
by selecting the method parameters that yield the best edge difference ratio
on that dataset variant.

When looking at the results in Table~\ref{tab:relationship_methods_best_edr},
we can observe two things:
\begin{itemize}
      \item The naive thresholding method consistently performs best on all dataset variants,
            closely followed by the relationship hypothesis method.
            However, the threshold parameter for the naive thresholding method
            stays relatively consistent across the dataset variants ($0.10 - 0.15$),
            while the relationship hypothesis method has a much wider range of upper bounds ($4 - 6$).
      \item The original most common foreign prediction method
            from the Bevandic et al. paper~\cite{bevandic_automatic_2022} performs significantly
            worse than the other methods with an edge difference ratio of over $0.5$ on all dataset variants.
            This was to be expected since our synthetic dataset variants rarely have
            classes with only a single outgoing relationship.
\end{itemize}

Let us take a deeper look at each of the methods and their performance
on the synthetic dataset variants using precision-recall curves for different parameters:
\begin{itemize}
      \item The \textbf{naive thresholding method} (see Figure~\ref{fig:naive_thresholding_precision_recall})
            performs well on all dataset variants, achieving a high precision and recall
            for thresholds between $0.10$ and $0.15$.
            The displayed precision-recall curves stay consistent across the dataset variants,
            which indicates that the method is robust for different dataset characteristics.
      \item The \textbf{density thresholding method} (see Figure~\ref{fig:density_thresholding_precision_recall})
            performs worse than the naive thresholding method,
            but still achieves good performance.
            Similar to the naive thresholding method, the CIFAR-100 dataset variant
            has a slightly lower precision and recall than the Caltech-256 dataset variants,
            which is expected since the CIFAR-100 dataset has more closely related classes
            that are difficult to distinguish.
      \item The \textbf{relationship hypothesis method} (see Figure~\ref{fig:hypothesis_method_precision_recall})
            performs similar to the density thresholding method,
            but has a conspicuous drop for the CIFAR-100 dataset variant
            (prominently more so than the other methods).
            This is likely due to the fact that this method assumes that
            every true relationship has an equal probability,
            which is not the case for the CIFAR-100 dataset variant
            (where some merged classes, due to having similar concepts that are hard to distinguish,
            have a much higher probability than others).
\end{itemize}

When using our method on real-world datasets,
we do not have a ground truth to adjust the parameters to.
Therefore, we need a single parameter configuration for every method
that works universally across all datasets.

We create a new evaluation on the dataset variants using the parameters
that yield the best edge difference ratio averaged across all dataset variants.
The results are shown in Table~\ref{tab:relationship_methods_global_optimal}.

The best performing naive thresholding method
achieves an edge difference ratio of $0.463$
with a noticably higher recall ($0.889$) than precision ($0.675$).
This means that while we hit almost 90\% of the true relationships,
we also select a lot of false relationships.

It can be observed that, for precsion, the mcfp method performs best
with a precision of $0.865$,
which is expected since it only selects relationships
that has the highest probability.
However, this comes at the cost of a very low recall of $0.421$
which ultimately results in a lower F1 score compared to the naive thresholding method.

We can conclude that, if you want to create singular high-confidence relationships,
the mcfp method is the best choice (especially for single-hierarchy taxonomies).
However, if you want to capture as many shared concepts as possible,
the naive thresholding method is the best choice, closely followed by the relationship hypothesis method
(maybe as a possible alternative if the naive thresholding method does not yield good results).

\input{figures/relationship_methods_global_optimal.tex}

However, these synthetic dataset variants might not be representative of real-world datasets,
due to their shared image data as well as their \enquote{clean} relationships.
But since our final goal is to create a universal model that works on multiple domains
without separate head models, we can simply compare the universal model performance
with our best candidate relationship selection methods
(i.e. naive thresholding vs. most common foreign prediction).

\subsubsection{Critique on Synthetic Dataset Variants}

We can criticise our findings on the synthetic dataset variants
by stating that the domains of the datasets are too similar
and therefore do not represent a meaningful evaluation of our methods
for real-world datasets:
Since the underlying images are the same, the models have a very high accuracy,
which makes methods like naive thresholding very effective - even with a low threshold
(which will not be the case for real-world datasets with different domains).

To verify or disprove our findings, we will try to create new, more realistic ground truth taxonomies:
\begin{itemize}
      \item \textbf{WordNet Synsets:} The WordNet synsets~\cite{fellbaum_wordnet_1998,noauthor_wordnet_nodate}
            represents a lexicon of english words with semantic relationships to each other.
            This allows us to create a taxonomy using WordNet with weighted relationships by calculating
            the Wu \& Palmer similarity score (range 0-1) between two labels (i.e. WordNet words).
            The score measures the semantic similarity (1 means equal, 0 means no semantic relationship).
      \item \textbf{SVHN-MNIST:} MNIST~\cite{deng_mnist_2012} and SVHN~\cite{netzer_reading_2011} are both labelled datasets
            for digits. The MNIST dataset contains greyscale images of handwritten digits,
            while the SVHN dataset contains colour images of house numbers.
            This allows us to create a ground truth mapping between the two datasets
            by using the labels of the digits as the classes, while the domain of the images
            is different.
      \item \textbf{Domain-Shift Synthetic Datasets:} We can try to improve our synthetic dataset variants
            by applying different transformations to the images of the variants (e.g. greyscale, rotation, etc.).
            This will change the domain of the images while keeping the classes and relationships intact
            and therefore create a more \enquote{difficult} dataset for our methods.
\end{itemize}

\subsubsection{Alternate Evaluation Methods - WordNet Synsets} \label{sec:wordnet_synsets}

We test the usage of WordNet Synsets on the Caltech-101 and Caltech-256 datasets to build a relationship graph.
It can be observed that the relationship weights (Wu \& Palmer similarity scores) are very high (ranging between 0.8 and 1.0),
even for weakly related synsets (e.g. between the Caltech256 class \textit{snake} and the Caltech101 class \textit{crocodile},
we observe a 0.88 relationship weight in both directions).

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.35\textwidth]{figures/wordnet.png}

      \caption{An example of a bad WordNet cluster in a relationship graph between Caltech256 and Caltech101.
            These false positives occur commonly and make WordNet unsuitable as a ground truth taxonomy.}
      \label{fig:wordnet}
\end{figure}

Upon further observation, this problem turns out to be even more severe:
WordNet's synsets build relations based on usage patterns in text corpora,
which does not align with our visual relationships based on shared concepts and attributes.
In Figure~\ref{fig:wordnet}, we see a relationship cluster with the classes \textit{gas pump}, \textit{anchor} and \textit{iris}.
These words do not share any visual similarities or attributes, yet they are clustered together based on their textual relationships
(we guess that \textit{gas pump} and \textit{anchor} are related through their usage in similar contexts like ships and maritime activities).
Unfortunately upon manual inspection, these false positives occur frequently.

Since we try to build a ground truth taxonomy to evaluate relationship selection methods,
these false positives can significantly skew the results and make WordNet unsuitable for our purposes.

\subsubsection{Alternate Evaluation Methods - SVHN-MNIST}

Next, we try to use the SVHN and MNIST datasets to create a relationship graph.
These datasets provide a special usability for our purposes,
since the relationships between the classes can be manually defined.
Also, since the datasets have a sufficiently shifted domain (i.e. drawn, greyscale images vs. real-world images of house numbers
from different angles, lighting conditions, etc.),
the differences are large enough to make the task of relationship selection more difficult and realistic.

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.35\textwidth]{figures/svhn_mnist.png}

      \caption{This figure shows the complete relationship graph between the SVHN and MNIST datasets.
            The relationships are unequivocal and straightforward, as the classes are identical across both datasets;
            however, they lack complexity.}
      \label{fig:svhn_mnist}
\end{figure}

In Figure~\ref{fig:svhn_mnist}, we can see the complete relationship graph between the SVHN and MNIST datasets.
Every relationship is a simple connection between two classes with a weight of 1.0.

While we could rerun our relationship selection method evaluation on this taxonomy,
we decide against this method upon further reflection:
\begin{itemize}
      \item Real-world taxonomies will not be made up of single, isolated relationships
            with a weight of 1.0. Complex subsets, mismatches or overlaps between classes are completely missing.
      \item This taxonomy will have only ten universal classes, which makes the sample size too small to draw meaningful conclusions.
      \item Our previous datasets contained a wide range of general-purpose classes as well as more specific ones, which allowed for a more nuanced evaluation of our methods.
            This taxonomy will be limited to numbers.
\end{itemize}

\subsubsection{Alternate Evaluation Methods - Domain-Shift Synthetic Datasets}

The main advantage of our synthetic dataset variants is their ability to create a proven correct ground truth taxonomy.
Instead of trying to derive a ground truth using new, suboptimal methods,
we instead try to fix the underlying issues with our existing method.

Our synthetic variants contain the same underlying images,
which result in a near-perfect, unrealistic cross-domain model prediction accuracy.
Due to this high model confidence, the best performing relationship selection methods are likely to be overly optimistic
and therefore perform poorly in real-world scenarios where models will not be able to perform as well across domains.


\begin{figure}[ht]
      \centering
      \includegraphics[width=\textwidth]{figures/domain_shift_visualization.png}

      \caption{Example images from the domain-shifted synthetic variants.
            We apply different transformations to the images of the original synthetic dataset
            to create two new domains. The first domain (Domain A) is a noisy greyscale variant,
            while the second domain (Domain B) is a rotated, blurry version of the original images,
            and the third domain (Domain C) has random erasings, shifted perspectives, and color jitter.}
      \label{fig:domain_shift}
\end{figure}

To mitigate this issue, we try to synthetically create a different domain for each variant by applying transforms to the
underlying dataset images.
An example subset of the domain-shifted images is shown in Figure~\ref{fig:domain_shift}:
Using different transforms, the images appear in unique styles and distortions,
synthetically changing their visual characteristics
while retaining the same classes and relationships.

\input{figures/evaluation_results_domain_shifted.tex}
\input{figures/relationship_methods_global_optimal_domain_shifted.tex}

The training performance of these new domain-shifted models can be seen in Table~\ref{tab:evaluation_results_domain_shifted}.
We can observe a $10\%$ decrease in accuracy on the test data compared to the original synthetic dataset variants.
This was to be expected due to the increased complexity of the domain-shifted tasks.

When comparing the new averaged relationship method evaluation results in Table~\ref{tab:relationship_methods_global_optimal_domain_shifted}
to our original non-domain-shifted results in Table~\ref{tab:relationship_methods_global_optimal},
we can observe significant differences:

\begin{itemize}
      \item Our new best performing relationship selection method,
            based on averaged maximum EDR scores, changes from MCFP to the relationship hypothesis method.
      \item In general, all methods have a lower EDR score than their non-domain-shifted counterparts.
            This is a direct result of the decreased domain model accuracy and was to be expected.
      \item The MCFP method still has the highest relative precision with $0.49$.
            If precision is more important than EDR for a high universal model accuracy
            will be evaluated in the universal model training section (see Section~\ref{sec:universal_models}).
\end{itemize}

We will use the most promising relationship selection methods,
MCFP and relationship hypothesis, for our universal model training.
Our final evaluation will focus on the model accuracy of the trained universal Models
built with the universal taxonomies of the two preselected relationship selection methods
(using the selection method parameters that have the best EDR score).

\subsection{Universal Taxonomy Generation}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.35\textwidth]{figures/taxonomy.png}

      \caption{This figure shows the complete Caltech101-Caltech256 universal taxonomy.
            While most classes build smaller clusters build smaller clusters of 2-3 classes,
            some large clusters can be observed.}
      \label{fig:taxonomy}
\end{figure}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.35\textwidth]{figures/wheel_concept.png}

      \caption{In the Caltech101-Caltech256 universal taxonomy,
            this cluster only contains vehicles. The universal classes created can be interpreted as
            sharing a common concept of \enquote{wheel}.}
      \label{fig:wheel_concept}
\end{figure}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.35\textwidth]{figures/bad_taxonomy.png}

      \caption{This example in the Caltech101-Caltech256 universal taxonomy shows a wrong relationship
            cluster towards the Caltech101 class \enquote{binocular}. Many of the Caltech256 classes
            do not have a suitable representation in the Caltech101 dataset
            and then connect to the Caltech101 class \enquote{binocular} instead.}
      \label{fig:bad_taxonomy}
\end{figure}

We now have our domain models trained and our relationship selection methods evaluated.
In the next step, we run our universal taxonomy generation algorithm
(see Section~\ref{sec:universal_taxonomy_algorithm}) on the Caltech-101 and
Caltech-256 datasets. To make the results more interpretable,
we will use the most common foreign prediction method to have a sparse relationship graph
that is easier to look at
(for our later universal model training, we will of course work with both the naive thresholding method and the most common foreign prediction method).

The overall universal taxonomy generated from the Caltech-101 and Caltech-256 datasets
is shown in Figure~\ref{fig:taxonomy}.
We can observe many smaller clusters of 2-3 classes which build from a single domain class in
the Caltech-101 dataset and multiple domain classes in the Caltech-256 dataset
(since the Caltech-256 dataset has more granular classes, this is to be expected).

An example of a \enquote{good} cluster is shown in Figure~\ref{fig:wheel_concept}:
This cluster centers around the Caltech101 class \enquote{wheelchair} and contains
multiple Caltech256 classes that share the concept of \enquote{wheel} (e.g. \enquote{mountain bike}, \enquote{segway}, \enquote{touring bike}, etc.).
The universal classes between the Caltech101 and Caltech256 classes
will provide a useful mapping for our universal model training.

However, we can also observe \enquote{bad} clusters in the universal taxonomy,
such as the one shown in Figure~\ref{fig:bad_taxonomy}:
The Caltech101 class \enquote{binocular} is connected to multiple Caltech256 classes
of a variety of different concepts.
While many of these classes do share concepts with binoculars (e.g. \enquote{sextant}, \enquote{telescope}, etc.),
many of the Caltech256 classes in the cluster do not have an obvious connection to binoculars
(e.g. \enquote{boxing glove}, \enquote{fire extinguisher}, etc.).
Since it is likely that the Caltech256 dataset contains classes that do not have a suitable representation in the Caltech101 dataset,
we \enquote{force} a relationship to a Caltech101 class by using the most common foreign prediction method.
However, the incorrect relationships do have a low relationship weight which makes
the error less severe for the universal model training.

Addressing these issues is a complex task, since a suitable threshold for the relationship weights
depends on the individual composition of the datasets.


\section{Universal Models} \label{sec:universal_models}

\subsection{Training}

\subsubsection{Modifications to Baseline Architecture}

Building upon the ResNet-50 architecture used for the individual domain models, we developed a \texttt{UniversalResNetModel} that incorporates several key modifications to enable multi-domain training through our universal taxonomy approach.

The most significant architectural change is the replacement of the standard classification head. While the baseline \texttt{ResNetModel} uses a multi-layer fully connected classifier with dropout regularization that outputs logits for domain-specific classes, the \texttt{UniversalResNetModel} employs a simplified two-layer fully connected head that outputs logits for universal classes:

\begin{itemize}
      \item \textbf{Baseline classifier}: A 6-layer fully connected network with dropout (0.5, 0.2, 0.2, 0.2, 0.2) that progressively reduces dimensionality from ResNet features $\rightarrow$ 1024 $\rightarrow$ 512 $\rightarrow$ 256 $\rightarrow$ 128 $\rightarrow$ domain classes
      \item \textbf{Universal classifier}: A 2-layer fully connected network (ResNet features $\rightarrow$ 1024 $\rightarrow$ universal classes) without dropout regularization
\end{itemize}

The simplified architecture proved to be more effective in training runs vs. the more complex hopper + dropout architecture used in the baseline models.

Another crucial modification is the loss function:
The baseline models use standard cross-entropy loss with one-hot encoded targets,
while the universal model employs cross-entropy for discrete probability distributions for a loss function.
This change accommodates the fact that domain classes may map to multiple universal classes with different weights,
as determined by the taxonomy relationships
(full explanation in Section~\ref{sec:universal_model_learning}).

\subsubsection{Multi-Domain Training Procedure}

Instead of training separate models on individual datasets,
we create a new combined dataset that merges multiple datasets while preserving domain identity.
Each training sample is augmented with a domain identifier,
transforming the standard $(\texttt{image}, \texttt{label})$ pairs into $(\texttt{image}, (\texttt{domain\_id}, \texttt{label}))$ tuples.
This allows the model to handle samples from different datasets within the same batch
(since we need to know the image domain to apply the correct target mapping).

\input{figures/universal_model_results.tex}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/universal_model_training_curves.pgf}}
      \caption{Training curves for universal models using both hypothesis and MCFP taxonomies on the multi-domain Caltech-101 + Caltech-256 dataset. Both models show stable convergence with the hypothesis taxonomy achieving slightly better performance.}
      \label{fig:universal_model_training_curves}
\end{figure}

% TODO Update section if we change anything
% TODO Write that we use val accuracy max instead of val loss min

% TODO Why is val accuracy rising and val loss also?
% TODO WHY is eval acc lower than val acc???

% TODO For all result tables, include wall time as time formatted (also write sth to it).

% TODO Train with the top relationship selection methods and compare their universal model accuracies
% TODO Compare best universal taxonomy model with baseline domain models (calc accuracy per-domain in universal model)
% TODO Compare with other multi-domain methods/papers
% TODO Create universal model for cifar100-caltech256 also
% TODO Create 3-domain model with caltech256-caltech101-cifar100
% TODO Can we derive which metric (EDR, F1, Precision) is most important?
% TODO Might the decrease in accuracy on the domain models influence the performance of the universal model?