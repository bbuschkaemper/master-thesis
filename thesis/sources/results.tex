\chapter{Results}

\section{Domain-Model Training}

\subsection{Datasets}

To start with our taxonomy generation,
we first need a set of datasets that we can use to train and evaluate our domain models:

\begin{itemize}
      \item \textbf{Caltech-101 and Caltech-256}~\cite{li_caltech_2022,griffin_caltech_2022}:
            The Caltech-101 dataset contains 101 general object categories with 40 to 800 images per category,
            while the Caltech-256 dataset extends this to 256 categories with at least 80 images per category.
            Both datasets have been widely used for image classification tasks\footnote{Over 500 open-access papers have cited the datasets, according to Papers with Code: \url{https://paperswithcode.com/dataset/caltech-101} and \url{https://paperswithcode.com/dataset/caltech-256}}.
            The images are roughly 300x200 pixels in size and contain annotated outlines
            for each object in the image, which we will not need for our purposes.
            The dataset has no predefined train/test split,
            so we will use a 80/10/10 split for training, validation, and testing.
      \item \textbf{CIFAR-100}~\cite{krizhevsky_learning_2009}:
            The CIFAR-100 datasets contains 100 classes grouped into 20 superclasses,
            with 600 images per class.
            Each image is 32x32 pixels in size, which is significantly smaller than the Caltech datasets.
            The dataset is one of the most popular datasets for image classification tasks\footnote{Over 5000 open-access papers have cited the dataset, according to Papers with Code: \url{https://paperswithcode.com/dataset/cifar-100}}.
            The dataset has a train/test split of 50000 training images and 10000 test images,
            which we will further split by dividing the training set into 80\% for training and 20\% for validation.
      \item \textbf{Synthetic Datasets}:
            To have a ground truth for our taxonomy generation,
            we will also create synthetic datasets based on the Caltech-101 and CIFAR-100 datasets.
            These datasets will be used to evaluate our cross-domain relationship graph generation
            methods (see Section~\ref{sec:graph_construction}).
            We will create synthetic datasets of varying sizes and complexity and evaluate
            how well our methods perform for different challenges.
\end{itemize}

\subsection{Training Domain Models}

\subsubsection{Model Architecture}

To now start our training of domain models,
we first need to define the architecture of our models.
The ResNet architecture\cite{he_deep_2015,he_identity_2016} is a popular choice for image classification tasks
and has been shown to perform well on a variety of datasets.
It also has the advantage of being pre-trained on the ImageNet dataset~\cite{deng_imagenet_2009,russakovsky_imagenet_2015},
which will save us the effort of training a model from scratch.
From the available ResNet architecture sizes,
we decide for the leaner ResNet-50 architecture to meet our resource constraints.

We adapt the ResNet-50 architecture to our datasets by switching out
the final fully connected layer with a funnel architecture that ends
in an output layer matching the number of classes per dataset (see Figure~\ref{fig:resnet_funnel}).

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/resnet_funnel.pdf}
      \caption{Our ResNet-50 architecture with a funnel layer for classification.
            Blue blocks represent the input from the ResNet-50 architecture,
            red blocks represent the final output layer,
            and green blocks represent our new funnel layers.}
      \label{fig:resnet_funnel}
\end{figure}

\subsubsection{Training Procedure}

In our initial training runs,
we observe severe overfitting on the training data as can be seen in Figure~\ref{fig:overfitting}.
To mitigate this, we apply several regularisation techniques:
\begin{itemize}
      \item \textbf{Dropout}~\cite{hinton_improving_2012}:
            As can be seen in Figure~\ref{fig:resnet_funnel},
            our fully connected layers contain dropout layers between them at rates of 0.5 and 0.2.
            Dropout is a regularisation technique that randomly sets a fraction of the input units to zero during training,
            which reinforces the model to learn more robust features and thereby reduces overfitting.
      \item \textbf{Data Augmentation}:
            We apply data augmentation techniques to our training data,
            such as random cropping, horizontal flipping, random erasing, and color jittering.
            These techniques artificially increase the size of our training dataset
            and help the model to generalise better by exposing it to a wider variety of input data.
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/cifar100_overfitting.pgf}}
      \caption{Overfitting on the CIFAR-100 dataset during training.
            The blue line represents the training accuracy,
            while the orange line represents the validation accuracy.
            The model overfits on the training data, resulting in a significant gap
            between the training and validation accuracy.}
      \label{fig:overfitting}
\end{figure}

We now train our models on the Caltech-101, Caltech-256, and CIFAR-100 datasets
(i.e. their synthetic variants).

For the easier Caltech-101 and Caltech-256 datasets,
we use the SGD optimiser~\cite{sutskever_importance_2013} with a learning rate of 0.01,
a Nesterov momentum of 0.9, and a weight decay of 0.0001 and train all variants for 50 epochs.
We also use a batch size of 64 for the datasets.

For our more complex CIFAR-100 dataset,
we use the AdamW optimiser~\cite{loshchilov_decoupled_2017} with an initial learning rate of 0.001, a weight decay of 0.001.
We train for 100 epochs with a multistep learning rate scheduler that reduces the learning rate by a factor of 0.1 at epochs 30, 60 and 80.
For the smaller CIFAR-100 images we use a batch size of 256.

The training is performed on a single NVIDIA RTX 3070 GPU with 8GB of VRAM
using the PyTorch Lightning framework~\cite{falcon_pytorch_2019}.
The training process takes approximately 5 hours for the Caltech-101 and Caltech-256
synthetic dataset variants and approximately 3 hours for the CIFAR-100 synthetic dataset variants.

\subsection{Model Performance}

\subsubsection{Synthetic Variants}

For our evaluation of relationship selection methods (see Section~\ref{sec:relationship_selection}),
we need synthetic dataset variants to calculate evaluation metrics on.

We select the general-purpose Caltech-256 and CIFAR-100 datasets
and create synthetic variants of these datasets:
\begin{itemize}
      \item \textbf{Caltech-256 2-Domain Variant}:
            We create a basic 2-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_2domain}.
      \item \textbf{Caltech-256 3-Domain Variant}:
            We create a more complex 3-domain variant of the Caltech-256 dataset
            with parameters $\mu_{\text{concepts}}=180$, $\sigma^2_{\text{concepts}}=10$,
            $\mu_{\text{classes}}=5$, and $\sigma^2_{\text{classes}}=1$.
            This variant has more concepts per class and therefore more relationships
            between the classes, which makes it more challenging for our relationship selection methods.
            These extreme numbers should be seen less as a realistic dataset
            and more as a stress test for our methods.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:caltech256_3domain} (rendered in a 3D view since a 2D view is cluttered with edges).
      \item \textbf{CIFAR-100 2-Domain Variant}:
            For the CIFAR-100 dataset,
            our 2-domain variant has parameters $\mu_{\text{concepts}}=50$, $\sigma^2_{\text{concepts}}=5$,
            $\mu_{\text{classes}}=3$, and $\sigma^2_{\text{classes}}=1$.
            In the Caltech-256 dataset variants we have used approximately $70\%$ of the classes as concepts,
            while in the CIFAR-100 dataset variants we use approximately $50\%$ of the classes as concepts.
            This results in a smaller, more manageable relationship graph
            that can be better manually inspected.
            The resulting relationship graph (before applying universal taxonomy algorithms)
            is shown in Figure~\ref{fig:cifar100_2domain}.
\end{itemize}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/caltech256_2domain.png}
      \caption{Caltech-256 2-domain synthetic dataset variant.
      The number of concepts is sampled from a truncated normal distribution
      with a $\mu_{\text{concepts}}=180$ and $\sigma^2_{\text{concepts}}=10$,
      while the number of classes per concept is sampled from a truncated normal distribution
      with a $\mu_{\text{classes}}=3$ and $\sigma^2_{\text{classes}}=1$.}
      \label{fig:caltech256_2domain}
\end{figure}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/caltech256_3domain.png}
      \caption{Caltech-256 3-domain synthetic dataset variant rendered in a 3D scatter plot.
      The number of concepts is sampled from a truncated normal distribution
      with a $\mu_{\text{concepts}}=180$ and $\sigma^2_{\text{concepts}}=10$,
      while the number of classes per concept is sampled from a truncated normal distribution
      with a $\mu_{\text{classes}}=5$ and $\sigma^2_{\text{classes}}=1$.}
      \label{fig:caltech256_3domain}
\end{figure}

\begin{figure}[ht]
      \centering
      \includegraphics[width=0.6\textwidth]{figures/cifar100_2domain.png}
      \caption{CIFAR-100 2-domain synthetic dataset variant.
      The number of concepts is sampled from a truncated normal distribution
      with a $\mu_{\text{concepts}}=50$ and $\sigma^2_{\text{concepts}}=5$,
      while the number of classes per concept is sampled from a truncated normal distribution
      with a $\mu_{\text{classes}}=3$ and $\sigma^2_{\text{classes}}=1$.}
      \label{fig:cifar100_2domain}
\end{figure}

\subsubsection{Model Accuracy}

Let us now take a look at the accuracy of our models trained on the synthetic dataset variants.
We use checkpoints to save the model after each epoch and pick the model checkpoint with the lowest validation loss
for our final evaluation.

We can see our final training runs in Figures~\ref{fig:caltech256_2domain_accuracy}, \ref{fig:caltech256_3domain_accuracy}, and \ref{fig:cifar100_2domain_accuracy}.
It can be observed that our overfitting mitigation techniques have worked sufficiently well,
as our training and validation accuracy curves do not diverge significantly.
We present the final model accuracies on the test sets in Table~\ref{tab:evaluation_results}.
Multiple things can be observed:
\begin{itemize}
      \item All the models achieve an accuracy of around 0.8 on the test set,
            which is an average performance for these datasets.
            Our focus is not on achieving state-of-the-art performance,
            but rather on creating models suitable for our cross-domain prediction task.
            It should be noted that a lower model accuracy will lead to worse performance
            in our relationship selection methods, but since we will compare the methods against each other
            using the same models, this should not be a problem.
      \item The CIFAR-100 variants have a slightly lower accuracy than the Caltech-256 variants,
            which is expected since the CIFAR-100 dataset has closely related classes
            categorised into superclasses, which make it harder for a model to distinguish between them.
      \item The number of concepts (i.e. classes) in the original dataset that get merged
            into a new class in the synthetic dataset variants does not seem to have a significant impact
            on the model accuracy: The Caltech-256 2-domain variant has a $\mu_{\text{classes}}=3$,
            while the Caltech-256 3-domain variant has a $\mu_{\text{classes}}=5$,
            but the deviation in accuracy is negligible ($\leq 0.01$).
\end{itemize}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/caltech256_2domain.pgf}}
      \caption{Accuracy of the final training runs on the Caltech-256 2-domain synthetic dataset variant.
            The blue and green lines represents the training accuracies (domains A and B),
            while the orange and red lines represents the validation accuracies (domains A and B).
            The models achieve final training accuracies of approximately 0.98 and validation accuracies of approximately 0.83.}
      \label{fig:caltech256_2domain_accuracy}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/caltech256_3domain.pgf}}
      \caption{Accuracy of the final training runs on the Caltech-256 3-domain synthetic dataset variant.
            The blue, green, and purple lines represents the training accuracies (domains A, B, and C),
            while the orange, red, and brown lines represents the validation accuracies (domains A, B, and C).
            The models achieve final training accuracies of approximately 0.98 and validation accuracies of approximately 0.82.}
      \label{fig:caltech256_3domain_accuracy}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.6}{\input{figures/cifar100_2domain.pgf}}
      \caption{Accuracy of the final training runs on the CIFAR-100 2-domain synthetic dataset variant.
            The blue and green lines represents the training accuracies (domains A and B),
            while the orange and red lines represents the validation accuracies (domains A and B).
            The models achieve final training accuracies of approximately 0.96 and validation accuracies of approximately 0.73.}
      \label{fig:cifar100_2domain_accuracy}
\end{figure}

\input{figures/evaluation_results.tex}

Now that we have sufficiently good domain models,
we can use them to evaluate our relationship selection methods
and generate taxonomies from the relationship graphs we create.

% TODO Take a manual look at the caltech101-caltech256 taxonomy and show examples of how the model performed.
% TODO Also add them into domain-model training section (how their accuracy is, how they were trained etc.). 

\section{Taxonomy Generation}

\subsection{Relationship Selection Methods} \label{sec:relationship_selection}

\graphicspath{{figures/}}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/naive_thresholding_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{naive thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:naive_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/density_thresholding_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{density thresholding method}
            for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:density_thresholding_precision_recall}
\end{figure}

\begin{figure}[ht]
      \centering
      \scalebox{0.35}{\input{figures/hypothesis_method_precision_recall.pgf}}
      \caption{Precision and recall plot of the \textbf{hypothesis method}
            for different upper bounds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}
      \label{fig:hypothesis_method_precision_recall}
\end{figure}

\input{figures/relationship_methods_results.tex}

% TODO: Display different taxonomies we generated and what is interesting about them.
% TODO: Describe what datasets we used and why we chose them.
% TODO: Compare and reference Bevandic taxonomy generation in contrast to our approach.

% TODO: Subsection to filtering methods and comparing them and their metrics.

\section{Universal Models}

% TODO
