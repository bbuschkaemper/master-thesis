%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methodology}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Universal Taxonomy}

Our main goal is to create a universal taxonomy that connects multiple
image classification datasets.
This taxonomy maps every dataset class to a universal class,
which allows us to analyse the relationships and shared concepts between datasets.

In the end, our taxonomy will allow us to train models that can classify images
from multiple datasets at once, building a robust and flexible system
that can quickly adapt to new domains.

\subsection{Formal Definitions} \label{sec:taxonomy_definitions}

To formalise our algorithm for building a universal taxonomy,
we first need to define some terms:

\begin{itemize}
    \item \textbf{Dataset} $D$: A collection of images and labels
          written as $D = \{(x_1, c_1), (x_2, c_2), \ldots, (x_n, c_n)\}$,
          where $x_i$ is an image and $c_i$ is its label.
          Since we are dealing with multiple datasets, we number them
          as $D_i = \{(x_1^i, c_1^i), (x_2^i, c_2^i), \ldots, (x_n^i, c_n^i)\}$,
          where $D_i$ is the dataset $D$ with index $i$.
          In the same way, we denote the set of all classes in a dataset as $C_i = \{c_1^i, c_2^i, \ldots, c_k^i\}$.
    \item \textbf{Model} $m$: A neural network trained on a dataset $D_I$
          which maps an image $x\in X$ to a class $c_i^I\in C_I$, denoted as $m_I: X \mapsto C_I$.
    \item \textbf{Domain}: Since both models and classes are dataset-specific,
          we define the term \textbf{domain} as the dataset $D_i$ and its classes $C_i$
          that we are working with.
    \item \textbf{Universal Classes}: Our universal taxonomy will contain a set of classes
          that are not specific to any dataset.
          We denote these classes as $C_U = \{c_1^U, c_2^U, \ldots, c_k^U\}$.
          A universal class is a concept represented by a set of domain classes
          that share similar characteristics. We therefore define a function
          $\text{classes}: C_U \mapsto \mathcal{P}(C)$, where $\mathcal{P}(C)$ is the power set of $C$,
          to represent the set of domain classes that belong to a universal class.
    \item \textbf{Graph}: We represent our taxonomy as a directed graph $G = (V, E)$,
          where $V$ is a set of vertices and $E$ is a set of edges.
          Each vertex $v_i$ represents a single class or universal class,
          which we define with $\text{class}: V \mapsto C$.
          Every edge $e_{ij}$ between two vertices $v_i$ and $v_j$ indicates
          a relationship $\text{class}(v_i) \rightarrow \text{class}(v_j)$.
    \item \textbf{Probability}: Every edge $e_{ij}$ has a probability associated with it,
          which indicates the likelihood of classifying an image from class $\text{class}(v_i)$
          as class $\text{class}(v_j)$.
          We denote this as a function $\text{probability}: E \mapsto [0, 1]$.
\end{itemize}

\subsection{Graph Construction} \label{sec:graph_construction}

Before building our universal taxonomy,
we need to construct our initial graph:

\begin{enumerate}
    \item \textbf{Foreign predictions:} For each dataset with its corresponding model,
          we run the model on all images from all other datasets.
          This gives us a set of predictions $P_{ab} = \{(x_i^a, c_j^b)\}$,
          where $x_i^a$ is an image from dataset $D_a$ and $c_j^b$ is the class
          predicted by model $m_b$ for that image.
    \item \textbf{Prediction probabilities:} We count the number of times each class $c_i^a$ was predicted
          as a foreign-domain class $c_j^b$.
          We denote this count in a matrix $M_{ab}\in {\mathbb{N}^+}^{|C_a| \times |C_b|}$,
          where $M_{ab}(i, j)$ is the number of times class $c_i^a$ was predicted
          as class $c_j^b$.
          We then divide each entry in the matrix by its row sum to get the
          probability of classifying an image from class $c_i^a$ as class $c_j^b$:
          \begin{equation*}
              P_{ab}(i, j) = \frac{M_{ab}(i, j)}{\sum_{k=1}^{|C_a|} M_{ab}(i, k)}
          \end{equation*}
          This gives us a matrix $P_{ab}\in [0, 1]^{|C_a| \times |C_b|}$,
          where $P_{ab}(i, j)$ is the probability of classifying an image from class $c_i^a$
          as class $c_j^b$.
    \item \textbf{Graph construction:} We now create a directed graph that represents the relationships between classes and datasets
          by iterating over every dataset $D_a$ with every dataset $D_b$ where $a \neq b$:
          \begin{enumerate}
              \item We collect the indices of the per-row maximum values in the matrix $P_{ab}$:
                    \begin{equation*}
                        I = \left\{\text{argmax}_{j\in\{1,\ldots,|C_b|\}} P_{ab}(i, j) \mid i\in\{1,\ldots,|C_a|\}\right\}
                    \end{equation*}
              \item For every $i\in \{1,\ldots,|C_a|\}$ where $P_{ab}(i, I_i) > 0$:
                    \begin{enumerate}
                        \item We create the vertices $v_k$ and $v_l$ for classes $c_i^a$ and $c_{I_i}^b$ respectively
                              if they do not already exist and add them to the graph
                              (otherwise we find the existing vertices for these classes as $v_k$ and $v_l$).
                        \item We create an edge $e_{kl}$ between the vertices $v_k$ and $v_l$ and add it to the graph.
                        \item We define $\text{probability}(e_{kl}) = P_{ab}(i, I_i)$.
                    \end{enumerate}
          \end{enumerate}
\end{enumerate}

\subsection{Taxonomy Generation}

After constructing our initial graph structure, we now need to transform it into a universal taxonomy
that merges classes from different datasets into universal classes where they share similar concepts.

\subsubsection{Taxonomy Building Rules}

We transform our initial graph of domain-to-domain relationships into a universal taxonomy
through a loop that applies a set of rules until no more changes can be made.
The rules are checked from first to last, starting from the first again after a rule is applied.
The rules are as follows:

\begin{enumerate}
    \item \textbf{Isolated Node Rule:} For any domain class A that has no relationships
          (neither incoming nor outgoing edges), create a new universal class B
          and add the relationship $A \rightarrow B$.
          We also define the probability of the relationship's edge as 1 and
          the classes of the universal class as $\{A\}$.

          This ensures that all domain classes without relationships (which can be created by later rules)
          are still represented in the universal taxonomy.

    \item \textbf{Bidirectional Relationship Rule:} When two classes have bidirectional relationships
          (A $\rightarrow$ B and B $\rightarrow$ A), they likely represent the same concept.
          We resolve this by creating a new universal class C and adding relationships
          $A \rightarrow C$ and $B \rightarrow C$ to the graph.
          The probability of the new relationships is set to the average of the bidirectional relationships and the classes of the universal class
          will be the two classes that were merged (or, if the two classes are universal classes themselves,
          the union of their classes).

    \item \textbf{Transitive Cycle Rule:} If we have relationships A $\rightarrow$ B $\rightarrow$ C
          where A and C are in the same domain, we have a problem since classes within a domain
          are disjoint, which means that one of the relationships must be incorrect.
          We solve this by removing the relationship with the lower probability,
          thus breaking the cycle.

    \item \textbf{Unilateral Relationship Rule:} A uniliteral relationship
          A $\rightarrow$ B indicates that the concepts of class A are a subset of the concepts of class B.
          We therefore create two new universal classes:
          \begin{itemize}
              \item Class C, which contains both classes A and B and has incoming relationships
                    from both classes with the probability of the uniliteral relationship.
                    This universal class represents the union of the two classes.
              \item Class D, which contains only class B and has a relationship from class B with a probability 1.
                    This universal class represents the concepts of class B that are not in class A.
          \end{itemize}
\end{enumerate}

\section{Synthetic Taxonomy Generation}

\subsection{The Need for a Controlled Ground Truth}

To evaluate our taxonomy generation methods, we need a reliable ground truth with known relationships between datasets. This presents a challenge, as most existing image classification datasets lack clear inter-dataset relationships:

\begin{itemize}
    \item \textbf{ImageNet}~\cite{deng_imagenet_2009,russakovsky_imagenet_2015}
          uses WordNet's~\cite{fellbaum_wordnet_1998} hierarchical structure to organize classes.
          However, this strict hierarchy doesn't match our use case where we need to connect
          datasets with different class structures and partial overlaps.

    \item \textbf{Open Images}~\cite{kuznetsova_open_2020} contains approximately 9 million
          images with multiple labels per image generated by Google's Cloud Vision API\footnote{\url{https://cloud.google.com/vision}}.
          This multi-label approach makes it difficult to determine a single class for each image,
          which is required for our evaluation. Additionally, since most labels were automatically
          generated, it doesn't provide the verified ground truth we need.

    \item \textbf{iNaturalist}~\cite{horn_inaturalist_2018} offers a detailed taxonomy of
          plant and animal species, but its domain-specific nature makes it unsuitable
          for developing a general-purpose evaluation framework.
\end{itemize}

\subsection{Our Approach: Building Synthetic Datasets}

Instead of relying on existing taxonomies, we developed a method to generate synthetic datasets with controlled relationships. Our approach:

\begin{enumerate}
    \item Define a set of "atomic concepts" that serve as building blocks for classes
    \item Create multiple domains by sampling these concepts to form classes
    \item Calculate inter-domain relationships based on shared concepts
\end{enumerate}

This method allows us to precisely control the taxonomy structure while creating realistic relationships between domains. To generate images for these synthetic classes, we can leverage existing datasets by treating each original class as an atomic concept.

\subsection{Formal Definitions}

We define our synthetic taxonomy framework on top of the definitions from \autoref{sec:taxonomy_definitions}.

\begin{itemize}
    \item \textbf{Atomic Concepts} $\mathcal{U}=\{1,2,\ldots,n\}$:
          A set of atomic concepts will be a universe of concepts that make up the basis for our synthetic class generation.
    \item \textbf{Synthetic Class}: A class $c^i_j$ will contain a subset of the atomic concepts
          from our universe: $c^i_j \subseteq \mathcal{U}$.
          To maintain disjoint class definitions, we ensure that $c^i_j \cap c^i_k = \emptyset$ for all $j \neq k$.
\end{itemize}

\subsection{Randomized Domain Generation}

To create realistic domains, we use normal distributions to sample the number of classes and concepts per class.
This allows us to generate domains with varying sizes and complexities, mimicking different real-world datasets.

\subsubsection{Parameter Sampling}

We sample the number of classes per domain and the number of concepts per class from truncated normal distributions to ensure realistic variation while maintaining control. Since normal distributions are unbounded, we use a truncated version:

\begin{equation*}
    f(x|\mu, \sigma, a, b) =
    \begin{cases}
        \frac{\phi\left(\frac{x-\mu}{\sigma}\right)}{\sigma\left[\Phi\left(\frac{b-\mu}{\sigma}\right) - \Phi\left(\frac{a-\mu}{\sigma}\right)\right]} & \text{if } a \leq x \leq b \\
        0                                                                                                                                              & \text{otherwise}
    \end{cases}
\end{equation*}

Where:
\begin{itemize}
    \item $\phi$ is the standard normal PDF
    \item $\Phi$ is the standard normal CDF
    \item $a$ and $b$ are lower and upper bounds
\end{itemize}

We implement this using SciPy's \texttt{truncnorm} module\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.truncnorm.html}},
handling SciPy's standardization of bounds internally:

\begin{equation*}
    X \sim \text{TruncNorm}(\mu, \sigma^2, a, b)
\end{equation*}

\subsubsection{Domain Generation Algorithm}

To generate a domain $C_i$, we follow these steps:

\begin{enumerate}
    \item \textbf{Sample set size:} Determine how many concepts $l$ to use for the domain:
          \begin{equation*}
              l \sim \lfloor\text{TruncNorm}(\mu_{\text{classes}}, \sigma^2_{\text{classes}}, 1, n)\rceil
          \end{equation*}

    \item \textbf{Sample concept pool:} Randomly select $l$ concepts from the universe $\mathcal{U}$:
          \begin{align*}
              P = \{a, b, c, \ldots\} \quad \text{where } a, b, c, \ldots \text{ are sampled without replacement from } \mathcal{U}
          \end{align*}

    \item \textbf{Initialise domain:} $C_i = \{\}$

    \item \textbf{Generate classes:} While concepts remain in the pool ($P \neq \emptyset$):
          \begin{enumerate}
              \item Sample class size $s_j$:
                    \begin{equation*}
                        s_j \sim \lfloor\text{TruncNorm}(\mu_{\text{class\_size}}, \sigma^2_{\text{class\_size}}, 1, |P|)\rceil
                    \end{equation*}

              \item Form class $c^i_j$ by selecting $s_j$ concepts randomly from $P$

              \item Remove selected concepts: $P = P \setminus c_j^i$

              \item Add class to domain: $C_i = C_i \cup \{c^i_j\}$
          \end{enumerate}
\end{enumerate}

This algorithm ensures that each concept is assigned to exactly one class within the domain, maintaining our disjointness constraint.

\subsection{Modeling Cross-Domain Relationships}

Once we've generated multiple domains, we need to model the relationships between them to create our ground truth.

\subsubsection{Simulating Neural Network Predictions}

Our taxonomy generation method assumes that neural network classifiers will predict related classes across domains with certain probabilities. To simulate this, we create "perfect" synthetic probabilities based on concept overlap.

\subsubsection{Relationship Calculation}

For any two domains $C_A$ and $C_B$, we calculate the probability of classifying an instance of class $c^A_i$ as class $c^B_j$ using:

\begin{equation}
    \begin{aligned}
        \text{NaiveProbability}(i, j) & = \frac{|c^A_i \cap c^B_j|}{|c^A_i|}                                              \\
        P_{i,j}                       & = \text{NaiveProbability}(i, j) + \frac{1 - \text{NaiveProbability}(i, j)}{|C_B|}
    \end{aligned}
\end{equation}

Where:
\begin{itemize}
    \item $\text{NaiveProbability}(i, j)$ is the proportion of concepts in class $c_i^A$ that also appear in class $c_j^B$
    \item The second term distributes remaining probability mass evenly across all classes in domain $C_B$, simulating the behavior of a neural network when encountering concepts it hasn't seen before
\end{itemize}

\subsubsection{A Concrete Example}

To illustrate this approach, consider two domains:
\begin{itemize}
    \item Domain A: $C_A = \{c^A_1=\{1,2\}, c^A_2=\{3,4\}\}$
    \item Domain B: $C_B = \{c^B_1=\{1,2,4\}, c^B_2=\{5,6\}\}$
\end{itemize}

For the relationship $c^A_1 \rightarrow c^B_1$:
\begin{itemize}
    \item $\text{NaiveProbability}(1,1) = \frac{|\{1,2\} \cap \{1,2,4\}|}{|\{1,2\}|} = \frac{2}{2} = 1$
    \item $P_{1,1} = 1 + \frac{1-1}{2} = 1$
\end{itemize}

For the relationship $c^A_2 \rightarrow c^B_1$:
\begin{itemize}
    \item $\text{NaiveProbability}(2,1) = \frac{|\{3,4\} \cap \{1,2,4\}|}{|\{3,4\}|} = \frac{1}{2} = 0.5$
    \item $P_{2,1} = 0.5 + \frac{1-0.5}{2} = 0.5 + 0.25 = 0.75$
\end{itemize}

This example shows how our framework captures partial relationships between classes
and how it simulates a perfect neural network classifier's behavior.
The resulting probability prediction matrix between two domains can then be used
to build a graph of relationships between classes,
which can then be turned into a universal taxonomy using the methods described in \autoref{sec:graph_construction}.

\subsubsection{No-Prediction Classes}

Some datasets have a special class that indicates that the model could not classify the image.
For these \enquote{no-prediction} classes, we need to adapt the relationship probability calculation:
Instead of distributing the remaining probability mass evenly across all classes,
we simply ignore it and therefore only have the probability of the overlapping concepts.

While this doesn't change the relationship that is picked as the most common foreign prediction,
it does change the probability of the relationship, which is important for thresholding later on.

\section{Universal Taxonomy Difference Metrics}

We created our synthetic datasets to have a known ground truth of relationships between classes.
Now, we need to find metrics to compare the predicted relationships
from our taxonomy generation methods
with the ground truth calculated from the synthetic datasets.

\subsection{Universal Class Graph}

We build our metrics to compare a ground truth synthetic taxonomy with a predicted taxonomy
that uses the same domain classes (i.e. the same nodes in the graph).

In a universal taxonomy graph, every universal class $c^U$ can be understood as an edge
between the domain classes that have incoming edges to it.
Every universal class has either one or two incoming edges, which will always be from different domain classes.
Therefore, we can represent a universal taxonomy as a new graph,
where every domain class is a vertex and every universal class is an edge between two vertices
(or a self-loop if it has only one incoming edge).

We define a concatenation of all domain classes in a taxonomy as $C_X = \bigcup_{i=1}^{n} C_i$.
We can now build an adjacency matrix $A_X\in [0, 1]^{|C_X|\times|C_X|}$ for the new universal class graph.
A connection between two classes $c_i^X$ and $c_j^X$ with $i\neq j$ exists if there is a universal class $c_k^U$ that has incoming edges from both classes.
A self-loop exists if there is a universal class $c_k^U$ that has an incoming edge from class $c_i^X$ without an incoming edge from any other class.
The value of the connection is the average weight of the two (or one) incoming edges to the universal class $c_k^U$.

\subsection{Edge Difference Ratio}

Our first metric is the edge difference ratio (EDR) between two graphs $G_1$ and $G_2$.
The metric measures the difference in edges between two graphs,
normalised by the edge weight sum of the union of the two graphs' edges (i.e. the weight of all edges without double counting).
This normalisation limits the metric to a range of $[0, 1]$,
where 0 indicates that the two graphs are identical and 1 indicates that they have no edges in common.

For two adjacency matrices $A_1$ and $A_2$ of graphs $G_1$ and $G_2$,
we define the edge difference ratio as follows:

\begin{equation}
    \text{EDR}(G_1, G_2) = \frac{\sum_{i \leq j} |A_1(i,j) - A_2(i,j)|}{\sum_{i \leq j} \max(A_1(i,j), A_2(i,j))}
\end{equation}

This definition only considers the upper triangular portion (including the diagonal)
of the adjacency matrices to avoid double-counting symmetric edges,
while still properly accounting for self-loops.

Our EDR metric is similar to the Jaccard index~\cite{jaccard_distribution_1912} as well as the Tanimoto coefficient~\cite{tanimoto_elementary_1958}
when we consider the adjacency matrices as sets of edges.
In contrast to these metrics, however, our EDR metric supports weighted edges,
which allows us to respect the probabilities of relationships between classes.

\subsection{Weighted Precision, Recall, and F1-Score}

While the edge difference ratio provides a comprehensive measure of taxonomy similarity,
we also implement traditional information retrieval metrics adapted for weighted edge prediction.
These metrics treat taxonomy comparison as a weighted edge prediction problem,
where we evaluate how well our predicted taxonomy recovers the ground truth relationships.

For two adjacency matrices $A_{\text{pred}}$ (predicted taxonomy) and $A_{\text{gt}}$ (ground truth taxonomy),
we calculate weighted precision, recall, and F1-score as follows:

\begin{equation}
    \begin{aligned}
        \text{Precision} = \frac{\sum_{i \leq j} \min(A_{\text{pred}}(i,j), A_{\text{gt}}(i,j))}{\sum_{i \leq j} A_{\text{pred}}(i,j)} \\
        \text{Recall} = \frac{\sum_{i \leq j} \min(A_{\text{pred}}(i,j), A_{\text{gt}}(i,j))}{\sum_{i \leq j} A_{\text{gt}}(i,j)}      \\
        \text{F1-Score} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{aligned}
\end{equation}

Where we again use the upper triangular portion (including diagonal) of the matrices
to avoid double-counting symmetric relationships while accounting for self-loops.

This approach provides a nuanced evaluation that respects both the existence and strength
of relationships in the universal taxonomy.
A similar version was used in a paper by Uijlings et al.~\cite{uijlings_missing_2022}
to evaluate their label relationship prediction method.
