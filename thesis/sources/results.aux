\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{21}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Domain Models}{21}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Datasets}{21}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Training}{22}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Model Architecture}{22}{subsubsection*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Training Procedure}{22}{subsubsection*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Our ResNet-50 architecture with a funnel layer for classification. Blue blocks represent the input from the ResNet-50 architecture, red blocks represent the final output layer, and green blocks represent our new funnel layers.}}{23}{figure.caption.34}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:resnet_funnel}{{4.1}{23}{Our ResNet-50 architecture with a funnel layer for classification. Blue blocks represent the input from the ResNet-50 architecture, red blocks represent the final output layer, and green blocks represent our new funnel layers}{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Model Performance}{23}{subsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Synthetic Variants}{23}{subsubsection*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Overfitting on the CIFAR-100 dataset during training. The blue line represents the training accuracy, while the orange line represents the validation accuracy. The model overfits on the training data, resulting in a significant gap between the training and validation accuracy.}}{24}{figure.caption.37}\protected@file@percent }
\newlabel{fig:overfitting}{{4.2}{24}{Overfitting on the CIFAR-100 dataset during training. The blue line represents the training accuracy, while the orange line represents the validation accuracy. The model overfits on the training data, resulting in a significant gap between the training and validation accuracy}{figure.caption.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Model Accuracy}{25}{subsubsection*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Taxonomy Generation}{25}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Relationship Selection Methods}{25}{subsection.4.2.1}\protected@file@percent }
\newlabel{sec:relationship_selection}{{4.2.1}{25}{Relationship Selection Methods}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Synthetic Dataset Variants}{25}{subsubsection*.46}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Evaluation results on test sets. Models were checkpointed after every epoch and evaluated on the validation loss. The model with the lowest validation loss was selected for evaluation on the test set. Training time indicates the total duration from start to finish of model training.}}{26}{table.caption.44}\protected@file@percent }
\newlabel{tab:evaluation_results}{{4.1}{26}{Evaluation results on test sets. Models were checkpointed after every epoch and evaluated on the validation loss. The model with the lowest validation loss was selected for evaluation on the test set. Training time indicates the total duration from start to finish of model training}{table.caption.44}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Best EDR results for relationship discovery methods. For each dataset variant and method, the parameter values that yielded the lowest Edge Difference Ratio (EDR) are shown along with the corresponding F1-score.}}{27}{table.caption.50}\protected@file@percent }
\newlabel{tab:relationship_methods_best_edr}{{4.2}{27}{Best EDR results for relationship discovery methods. For each dataset variant and method, the parameter values that yielded the lowest Edge Difference Ratio (EDR) are shown along with the corresponding F1-score}{table.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Critique on Synthetic Dataset Variants}{28}{subsubsection*.53}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Average performance metrics for relationship discovery methods with globally optimal parameters. Each method uses the parameter value that minimizes the average EDR across all dataset variants. Performance metrics are then averaged across all dataset variants using these optimal parameters.}}{29}{table.caption.51}\protected@file@percent }
\newlabel{tab:relationship_methods_global_optimal}{{4.3}{29}{Average performance metrics for relationship discovery methods with globally optimal parameters. Each method uses the parameter value that minimizes the average EDR across all dataset variants. Performance metrics are then averaged across all dataset variants using these optimal parameters}{table.caption.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Alternate Evaluation Methods - WordNet Synsets}{29}{subsubsection*.55}\protected@file@percent }
\newlabel{sec:wordnet_synsets}{{4.2.1}{29}{Alternate Evaluation Methods - WordNet Synsets}{subsubsection*.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Alternate Evaluation Methods - SVHN-MNIST}{30}{subsubsection*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Alternate Evaluation Methods - Domain-Shift Synthetic Datasets}{31}{subsubsection*.61}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Evaluation results on test sets for domain-shifted experiments. Models were trained with different domain shift transformations and checkpointed after every epoch. The model with the lowest validation loss was selected for evaluation on the test set. Training time indicates the total duration from start to finish of model training.}}{31}{table.caption.63}\protected@file@percent }
\newlabel{tab:evaluation_results_domain_shifted}{{4.4}{31}{Evaluation results on test sets for domain-shifted experiments. Models were trained with different domain shift transformations and checkpointed after every epoch. The model with the lowest validation loss was selected for evaluation on the test set. Training time indicates the total duration from start to finish of model training}{table.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Average performance metrics for relationship discovery methods with globally optimal parameters on domain-shifted experiments. Each method uses the parameter value that minimizes the average EDR across all domain-shifted dataset variants. Performance metrics are then averaged across all dataset variants using these optimal parameters.}}{32}{table.caption.64}\protected@file@percent }
\newlabel{tab:relationship_methods_global_optimal_domain_shifted}{{4.5}{32}{Average performance metrics for relationship discovery methods with globally optimal parameters on domain-shifted experiments. Each method uses the parameter value that minimizes the average EDR across all domain-shifted dataset variants. Performance metrics are then averaged across all dataset variants using these optimal parameters}{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Universal Taxonomy Generation}{32}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Universal Models}{33}{section.4.3}\protected@file@percent }
\newlabel{sec:universal_models}{{4.3}{33}{Universal Models}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Training}{33}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Modifications to Baseline Architecture}{33}{subsubsection*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Multi-Domain Training Procedure}{34}{subsubsection*.71}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Baseline ResNet model performance on individual datasets. These single-domain models serve as reference points for evaluating the universal models. Every baseline model was trained for 50 epochs.}}{35}{table.caption.72}\protected@file@percent }
\newlabel{tab:baseline_model_results}{{4.6}{35}{Baseline ResNet model performance on individual datasets. These single-domain models serve as reference points for evaluating the universal models. Every baseline model was trained for 50 epochs}{table.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Universal model evaluation results on multi-domain test datasets. Two-domain models were trained on Caltech-101 + Caltech-256, while three-domain models were trained on all three datasets. Models were evaluated on individual domains as well as the combined test set (no weighting was applied, the individual test sets were simply concatenated). Domain accuracy values show performance differences compared to single-domain baseline models (see Table~\ref {tab:baseline_model_results}). Best results per column are shown in bold. All accuracy values are shown as percentages.}}{35}{table.caption.73}\protected@file@percent }
\newlabel{tab:universal_model_results}{{4.7}{35}{Universal model evaluation results on multi-domain test datasets. Two-domain models were trained on Caltech-101 + Caltech-256, while three-domain models were trained on all three datasets. Models were evaluated on individual domains as well as the combined test set (no weighting was applied, the individual test sets were simply concatenated). Domain accuracy values show performance differences compared to single-domain baseline models (see Table~\ref {tab:baseline_model_results}). Best results per column are shown in bold. All accuracy values are shown as percentages}{table.caption.73}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Performance}{35}{subsection.4.3.2}\protected@file@percent }
\newlabel{fig:caltech256_2domain}{{4.3a}{37}{Caltech-256 2-domain variant 1\\ ($\mu _{\text {concepts}}=180$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=3$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{sub@fig:caltech256_2domain}{{a}{37}{Caltech-256 2-domain variant 1\\ ($\mu _{\text {concepts}}=180$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=3$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{fig:caltech256_2domain_variant}{{4.3b}{37}{Caltech-256 2-domain variant 2\\ ($\mu _{\text {concepts}}=200$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=2$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{sub@fig:caltech256_2domain_variant}{{b}{37}{Caltech-256 2-domain variant 2\\ ($\mu _{\text {concepts}}=200$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=2$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{fig:caltech256_3domain}{{4.3c}{37}{Caltech-256 3-domain variant\\ ($\mu _{\text {concepts}}=180$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=5$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{sub@fig:caltech256_3domain}{{c}{37}{Caltech-256 3-domain variant\\ ($\mu _{\text {concepts}}=180$, $\sigma ^2_{\text {concepts}}=10$,\\ $\mu _{\text {classes}}=5$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{fig:cifar100_2domain}{{4.3d}{37}{CIFAR-100 2-domain variant\\ ($\mu _{\text {concepts}}=50$, $\sigma ^2_{\text {concepts}}=5$,\\ $\mu _{\text {classes}}=3$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\newlabel{sub@fig:cifar100_2domain}{{d}{37}{CIFAR-100 2-domain variant\\ ($\mu _{\text {concepts}}=50$, $\sigma ^2_{\text {concepts}}=5$,\\ $\mu _{\text {classes}}=3$, $\sigma ^2_{\text {classes}}=1$)}{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Synthetic dataset variants showing their relationship graphs before applying universal taxonomy algorithms. The number of concepts and classes per concept are sampled from truncated normal distributions with the parameters shown in each subfigure caption.}}{37}{figure.caption.40}\protected@file@percent }
\newlabel{fig:synthetic_variants}{{4.3}{37}{Synthetic dataset variants showing their relationship graphs before applying universal taxonomy algorithms. The number of concepts and classes per concept are sampled from truncated normal distributions with the parameters shown in each subfigure caption}{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Accuracy curves for all synthetic dataset variants. Each subplot shows training and validation accuracy over training steps for all domains in that variant. The models achieve final training accuracies of approximately 0.96-0.98 and validation accuracies of approximately 0.73-0.83.}}{38}{figure.caption.43}\protected@file@percent }
\newlabel{fig:all_training_runs}{{4.4}{38}{Accuracy curves for all synthetic dataset variants. Each subplot shows training and validation accuracy over training steps for all domains in that variant. The models achieve final training accuracies of approximately 0.96-0.98 and validation accuracies of approximately 0.73-0.83}{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Precision and recall plot of the \textbf  {naive thresholding method} for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}}{39}{figure.caption.47}\protected@file@percent }
\newlabel{fig:naive_thresholding_precision_recall}{{4.5}{39}{Precision and recall plot of the \textbf {naive thresholding method} for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Precision and recall plot of the \textbf  {density thresholding method} for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}}{40}{figure.caption.48}\protected@file@percent }
\newlabel{fig:density_thresholding_precision_recall}{{4.6}{40}{Precision and recall plot of the \textbf {density thresholding method} for different thresholds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Precision and recall plot of the \textbf  {hypothesis method} for different upper bounds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants.}}{40}{figure.caption.49}\protected@file@percent }
\newlabel{fig:hypothesis_method_precision_recall}{{4.7}{40}{Precision and recall plot of the \textbf {hypothesis method} for different upper bounds on the Caltech-256 2-domain, Caltech-256 3-domain, and CIFAR-100 2-domain synthetic dataset variants}{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces An example of a bad WordNet cluster in a relationship graph between Caltech256 and Caltech101. These false positives occur commonly and make WordNet unsuitable as a ground truth taxonomy.}}{41}{figure.caption.56}\protected@file@percent }
\newlabel{fig:wordnet}{{4.8}{41}{An example of a bad WordNet cluster in a relationship graph between Caltech256 and Caltech101. These false positives occur commonly and make WordNet unsuitable as a ground truth taxonomy}{figure.caption.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces This figure shows the complete relationship graph bet