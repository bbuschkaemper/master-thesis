{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249d7fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caltech-256 classes: 257\n",
      "Caltech-101 classes: 101\n",
      "CIFAR-100 classes: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import Caltech256, Caltech101, CIFAR100\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from library.taxonomy import Taxonomy\n",
    "from library.models.universal_resnet import UniversalResNetModel\n",
    "from library.datasets.caltech101 import Caltech101DataModule\n",
    "from library.datasets.caltech256 import Caltech256DataModule\n",
    "from library.datasets.cifar100 import CIFAR100ScaledDataModule, CIFAR100DataModule\n",
    "from library.datasets.util import CombinedDataModule\n",
    "\n",
    "# Load dataset information\n",
    "caltech256_labels = Caltech256(root=\"datasets/caltech256\", download=False).categories\n",
    "caltech101_labels = Caltech101(root=\"datasets/caltech101\", download=False).categories\n",
    "cifar100_labels = CIFAR100(\n",
    "    root=\"datasets/cifar100\", download=False, train=False\n",
    ").classes\n",
    "\n",
    "print(f\"Caltech-256 classes: {len(caltech256_labels)}\")\n",
    "print(f\"Caltech-101 classes: {len(caltech101_labels)}\")\n",
    "print(f\"CIFAR-100 classes: {len(cifar100_labels)}\")\n",
    "\n",
    "# Reduce the precision of matrix multiplication to speed up training\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25714df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both taxonomies created from the real-world datasets\n",
    "hypothesis_taxonomy = Taxonomy.load(\"taxonomies/caltech256_caltech101_hypothesis.pkl\")\n",
    "mcfp_taxonomy = Taxonomy.load(\"taxonomies/caltech256_caltech101_mcfp.pkl\")\n",
    "mcfp_binary_taxonomy = Taxonomy.load(\"taxonomies/caltech256_caltech101_mcfp_binary.pkl\")\n",
    "\n",
    "# Load the threshold-based taxonomies with optimal parameters\n",
    "density_threshold_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/caltech256_caltech101_density_threshold.pkl\"\n",
    ")\n",
    "naive_threshold_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/caltech256_caltech101_naive_threshold.pkl\"\n",
    ")\n",
    "\n",
    "# Load the three-domain taxonomies\n",
    "three_domain_hypothesis_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/three_domain_hypothesis.pkl\"\n",
    ")\n",
    "three_domain_mcfp_taxonomy = Taxonomy.load(\"taxonomies/three_domain_mcfp.pkl\")\n",
    "three_domain_mcfp_binary_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/three_domain_mcfp_binary.pkl\"\n",
    ")\n",
    "\n",
    "# Load the three-domain threshold-based taxonomies\n",
    "three_domain_density_threshold_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/three_domain_density_threshold.pkl\"\n",
    ")\n",
    "three_domain_naive_threshold_taxonomy = Taxonomy.load(\n",
    "    \"taxonomies/three_domain_naive_threshold.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a9cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Multi-Domain Training\n",
    "\n",
    "# Training configuration\n",
    "TRAIN = False  # Set to True to train model from scratch\n",
    "\n",
    "# Create individual dataset modules\n",
    "caltech101_dm = Caltech101DataModule(batch_size=32)\n",
    "caltech256_dm = Caltech256DataModule(batch_size=32)\n",
    "cifar100_dm = CIFAR100ScaledDataModule(batch_size=32)\n",
    "cifar100_original_dm = CIFAR100DataModule(batch_size=32)\n",
    "\n",
    "# Create combined data module with domain IDs\n",
    "# Domain 0: Caltech-101, Domain 1: Caltech-256\n",
    "dataset_module = CombinedDataModule(\n",
    "    dataset_modules=[caltech101_dm, caltech256_dm],\n",
    "    domain_ids=[0, 1],\n",
    "    batch_size=64,\n",
    "    num_workers=11,\n",
    ")\n",
    "\n",
    "# Create three-domain data module\n",
    "# Domain 0: Caltech-101, Domain 1: Caltech-256, Domain 2: CIFAR-100\n",
    "three_domain_dataset_module = CombinedDataModule(\n",
    "    dataset_modules=[caltech101_dm, caltech256_dm, cifar100_dm],\n",
    "    domain_ids=[0, 1, 2],\n",
    "    batch_size=64,\n",
    "    num_workers=11,\n",
    ")\n",
    "\n",
    "dataset_name = \"Caltech-101 + Caltech-256 (Multi-Domain)\"\n",
    "three_domain_dataset_name = \"Caltech-101 + Caltech-256 + CIFAR-100 (Three-Domain)\"\n",
    "\n",
    "# Configuration for all taxonomies\n",
    "taxonomies_config = {\n",
    "    \"hypothesis\": {\n",
    "        \"taxonomy\": hypothesis_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-hypothesis-multi-domain-min-val-loss\",\n",
    "        \"logger_name\": \"universal_hypothesis_multi_domain\",\n",
    "    },\n",
    "    \"mcfp\": {\n",
    "        \"taxonomy\": mcfp_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-mcfp-multi-domain-min-val-loss\",\n",
    "        \"logger_name\": \"universal_mcfp_multi_domain\",\n",
    "    },\n",
    "    \"mcfp_binary\": {\n",
    "        \"taxonomy\": mcfp_binary_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-mcfp-binary-multi-domain-min-val-loss\",\n",
    "        \"logger_name\": \"universal_mcfp_binary_multi_domain\",\n",
    "    },\n",
    "    \"density_threshold\": {\n",
    "        \"taxonomy\": density_threshold_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-density-threshold-multi-domain-min-val-loss\",\n",
    "        \"logger_name\": \"universal_density_threshold_multi_domain\",\n",
    "    },\n",
    "    \"naive_threshold\": {\n",
    "        \"taxonomy\": naive_threshold_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-naive-threshold-multi-domain-min-val-loss\",\n",
    "        \"logger_name\": \"universal_naive_threshold_multi_domain\",\n",
    "    },\n",
    "    \"three_domain_hypothesis\": {\n",
    "        \"taxonomy\": three_domain_hypothesis_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-three-domain-hypothesis-min-val-loss\",\n",
    "        \"logger_name\": \"universal_three_domain_hypothesis\",\n",
    "    },\n",
    "    \"three_domain_mcfp\": {\n",
    "        \"taxonomy\": three_domain_mcfp_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-three-domain-mcfp-min-val-loss\",\n",
    "        \"logger_name\": \"universal_three_domain_mcfp\",\n",
    "    },\n",
    "    \"three_domain_mcfp_binary\": {\n",
    "        \"taxonomy\": three_domain_mcfp_binary_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-three-domain-mcfp-binary-min-val-loss\",\n",
    "        \"logger_name\": \"universal_three_domain_mcfp_binary\",\n",
    "    },\n",
    "    \"three_domain_density_threshold\": {\n",
    "        \"taxonomy\": three_domain_density_threshold_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-three-domain-density-threshold-min-val-loss\",\n",
    "        \"logger_name\": \"universal_three_domain_density_threshold\",\n",
    "    },\n",
    "    \"three_domain_naive_threshold\": {\n",
    "        \"taxonomy\": three_domain_naive_threshold_taxonomy,\n",
    "        \"model_name\": \"universal-resnet50-three-domain-naive-threshold-min-val-loss\",\n",
    "        \"logger_name\": \"universal_three_domain_naive_threshold\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0c7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration (shared for both models)\n",
    "training_config = {\n",
    "    \"max_epochs\": 50,\n",
    "    \"optim\": \"adamw\",\n",
    "    \"optim_kwargs\": {\n",
    "        \"lr\": 0.00005,  # Reduced from 0.0001\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"eps\": 1e-8,\n",
    "    },\n",
    "    \"lr_scheduler\": \"cosine\",  # Changed from multistep\n",
    "    \"lr_scheduler_kwargs\": {\n",
    "        \"T_max\": 50,  # matches max_epochs\n",
    "        \"eta_min\": 1e-7,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e34cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: universal-resnet50-hypothesis-multi-domain-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   3%|▎         | 2/62 [00:01<00:46,  1.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 64. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 62/62 [00:11<00:00,  5.45it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.8446651101112366\n",
      "        eval_loss            2.242966651916504\n",
      "        hp_metric           0.8446651101112366\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: universal-resnet50-mcfp-multi-domain-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 62/62 [00:07<00:00,  8.28it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy          0.829386293888092\n",
      "        eval_loss           1.6230005025863647\n",
      "        hp_metric            0.829386293888092\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model: universal-resnet50-mcfp-binary-multi-domain-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 62/62 [00:07<00:00,  8.29it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.9022154211997986\n",
      "        eval_loss           1.6127184629440308\n",
      "        hp_metric           0.9022154211997986\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/bjoern/dev/master-thesis/project/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 26.4 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "26.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.4 M    Total params\n",
      "105.664   Total estimated model params size (MB)\n",
      "154       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 492/492 [02:39<00:00,  3.09it/s, v_num=1]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 25. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 492/492 [02:53<00:00,  2.84it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 492/492 [02:54<00:00,  2.82it/s, v_num=1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/bjoern/dev/master-thesis/project/checkpoints/universal-resnet50-density-threshold-multi-domain-min-val-loss.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/bjoern/dev/master-thesis/project/checkpoints/universal-resnet50-density-threshold-multi-domain-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 62/62 [00:07<00:00,  7.88it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.8393175601959229\n",
      "        eval_loss           2.3481202125549316\n",
      "        hp_metric           0.8393175601959229\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 26.4 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "26.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.4 M    Total params\n",
      "105.459   Total estimated model params size (MB)\n",
      "154       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 492/492 [02:50<00:00,  2.88it/s, v_num=0]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 492/492 [02:50<00:00,  2.88it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/bjoern/dev/master-thesis/project/checkpoints/universal-resnet50-naive-threshold-multi-domain-min-val-loss.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/bjoern/dev/master-thesis/project/checkpoints/universal-resnet50-naive-threshold-multi-domain-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 62/62 [00:07<00:00,  7.98it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.8482301831245422\n",
      "        eval_loss            2.269490957260132\n",
      "        hp_metric           0.8482301831245422\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 26.9 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "26.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.9 M    Total params\n",
      "107.492   Total estimated model params size (MB)\n",
      "154       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1117/1117 [06:17<00:00,  2.96it/s, v_num=7]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 41. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  81%|████████▏ | 908/1117 [04:47<01:06,  3.15it/s, v_num=7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/training_epoch_loop.py:295\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    293\u001b[39m     batch = call._call_strategy_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mbatch_to_device\u001b[39m\u001b[33m\"\u001b[39m, batch, dataloader_idx=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_progress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m trainer._logger_connector.on_batch_start(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/progress.py:155\u001b[39m, in \u001b[36m_Progress.increment_ready\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `total` and `current` instances should be of the same class\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mincrement_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mself\u001b[39m.total.ready += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     41\u001b[39m trainer = Trainer(\n\u001b[32m     42\u001b[39m     max_epochs=training_config[\u001b[33m\"\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     43\u001b[39m     logger=tb_logger,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     ],\n\u001b[32m     54\u001b[39m )\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_dataset_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Test the trained model\u001b[39;00m\n\u001b[32m     60\u001b[39m test_results = trainer.test(datamodule=current_dataset_module, ckpt_path=\u001b[33m\"\u001b[39m\u001b[33mbest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Train models for both taxonomies\n",
    "results = {}\n",
    "\n",
    "for taxonomy_name, config in taxonomies_config.items():\n",
    "    if taxonomy_name in [\n",
    "        \"three_domain_density_threshold\",\n",
    "        \"three_domain_naive_threshold\",\n",
    "    ]:\n",
    "        TRAIN = True\n",
    "\n",
    "    # Select appropriate dataset module\n",
    "    if taxonomy_name in [\n",
    "        \"three_domain_hypothesis\",\n",
    "        \"three_domain_mcfp\",\n",
    "        \"three_domain_mcfp_binary\",\n",
    "        \"three_domain_density_threshold\",\n",
    "        \"three_domain_naive_threshold\",\n",
    "    ]:\n",
    "        current_dataset_module = three_domain_dataset_module\n",
    "    else:\n",
    "        current_dataset_module = dataset_module\n",
    "\n",
    "    # Create the Universal ResNet model for this taxonomy\n",
    "    model = UniversalResNetModel(\n",
    "        taxonomy=config[\"taxonomy\"],\n",
    "        architecture=\"resnet50\",\n",
    "        optim=training_config[\"optim\"],\n",
    "        optim_kwargs=training_config[\"optim_kwargs\"],\n",
    "        lr_scheduler=training_config[\"lr_scheduler\"],\n",
    "        lr_scheduler_kwargs=training_config[\"lr_scheduler_kwargs\"],\n",
    "    )\n",
    "\n",
    "    # Setup trainer\n",
    "    if TRAIN:\n",
    "        tb_logger = pl_loggers.TensorBoardLogger(\n",
    "            save_dir=\"logs\", name=config[\"logger_name\"]\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            max_epochs=training_config[\"max_epochs\"],\n",
    "            logger=tb_logger,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(\n",
    "                    dirpath=\"checkpoints\",\n",
    "                    monitor=\"val_accuracy\",\n",
    "                    mode=\"max\",\n",
    "                    save_top_k=1,\n",
    "                    filename=config[\"model_name\"],\n",
    "                    enable_version_counter=False,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.fit(model, datamodule=current_dataset_module)\n",
    "\n",
    "        # Test the trained model\n",
    "        test_results = trainer.test(datamodule=current_dataset_module, ckpt_path=\"best\")\n",
    "\n",
    "    else:\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            enable_checkpointing=False,\n",
    "        )\n",
    "\n",
    "        # Load pre-trained model\n",
    "        print(f\"Loading pre-trained model: {config['model_name']}.ckpt\")\n",
    "        model = UniversalResNetModel.load_from_checkpoint(\n",
    "            f\"checkpoints/{config['model_name']}.ckpt\",\n",
    "            taxonomy=config[\n",
    "                \"taxonomy\"\n",
    "            ],  # Need to pass taxonomy since it's not serialized\n",
    "        )\n",
    "\n",
    "        # Test the loaded model\n",
    "        test_results = trainer.test(model, datamodule=current_dataset_module)\n",
    "\n",
    "    # Store results\n",
    "    results[taxonomy_name] = test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual combined data modules for each domain\n",
    "# These maintain the (target, domain_id) tuple format expected by the universal models\n",
    "caltech101_combined_dm = CombinedDataModule(\n",
    "    dataset_modules=[caltech101_dm],\n",
    "    domain_ids=[0],  # Domain 0 for Caltech-101\n",
    "    batch_size=64,\n",
    "    num_workers=11,\n",
    ")\n",
    "\n",
    "caltech256_combined_dm = CombinedDataModule(\n",
    "    dataset_modules=[caltech256_dm],\n",
    "    domain_ids=[1],  # Domain 1 for Caltech-256\n",
    "    batch_size=64,\n",
    "    num_workers=11,\n",
    ")\n",
    "\n",
    "cifar100_combined_dm = CombinedDataModule(\n",
    "    dataset_modules=[cifar100_dm],\n",
    "    domain_ids=[2],  # Domain 2 for CIFAR-100\n",
    "    batch_size=64,\n",
    "    num_workers=11,\n",
    ")\n",
    "\n",
    "# Test each model on individual domains\n",
    "domain_results = {}\n",
    "for taxonomy_name, config in taxonomies_config.items():\n",
    "    # Load the trained model\n",
    "    print(f\"Loading pre-trained model: {config['model_name']}.ckpt\")\n",
    "    model = UniversalResNetModel.load_from_checkpoint(\n",
    "        f\"checkpoints/{config['model_name']}.ckpt\", taxonomy=config[\"taxonomy\"]\n",
    "    )\n",
    "\n",
    "    # Create trainer for testing\n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    domain_results[taxonomy_name] = {\n",
    "        \"name\": taxonomy_name,\n",
    "    }\n",
    "\n",
    "    # Test on Caltech-101 (Domain 0)\n",
    "    caltech101_results = trainer.test(model, datamodule=caltech101_combined_dm)\n",
    "    domain_results[taxonomy_name][\"caltech101\"] = caltech101_results[0][\"eval_accuracy\"]\n",
    "\n",
    "    # Test on Caltech-256 (Domain 1)\n",
    "    caltech256_results = trainer.test(model, datamodule=caltech256_combined_dm)\n",
    "    domain_results[taxonomy_name][\"caltech256\"] = caltech256_results[0][\"eval_accuracy\"]\n",
    "\n",
    "    # Test on CIFAR-100 (Domain 2) - only for three-domain models\n",
    "    if taxonomy_name in [\n",
    "        \"three_domain_hypothesis\",\n",
    "        \"three_domain_mcfp\",\n",
    "        \"three_domain_mcfp_binary\",\n",
    "    ]:\n",
    "        cifar100_results = trainer.test(model, datamodule=cifar100_combined_dm)\n",
    "        domain_results[taxonomy_name][\"cifar100\"] = cifar100_results[0][\"eval_accuracy\"]\n",
    "\n",
    "        # Test on all three domains together\n",
    "        three_domain_results = trainer.test(\n",
    "            model, datamodule=three_domain_dataset_module\n",
    "        )\n",
    "        domain_results[taxonomy_name][\"unified\"] = three_domain_results[0][\n",
    "            \"eval_accuracy\"\n",
    "        ]\n",
    "    else:\n",
    "        # For two-domain models, CIFAR-100 accuracy is N/A\n",
    "        domain_results[taxonomy_name][\"cifar100\"] = None\n",
    "\n",
    "        # Test on original test (both)\n",
    "        original_results = trainer.test(model, datamodule=dataset_module)\n",
    "        domain_results[taxonomy_name][\"unified\"] = original_results[0][\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6810169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training duration for each taxonomy\n",
    "from csv import DictReader\n",
    "import datetime\n",
    "\n",
    "\n",
    "def calculate_training_duration(file_prefix):\n",
    "    \"\"\"Calculate training duration from walltime in training CSV file\"\"\"\n",
    "    try:\n",
    "        with open(f\"training_results/{file_prefix}_train.csv\", \"r\") as f:\n",
    "            reader = DictReader(f)\n",
    "            rows = list(reader)\n",
    "\n",
    "            if not rows:\n",
    "                return \"N/A\"\n",
    "\n",
    "            # Get first and last walltime\n",
    "            start_time = float(rows[0][\"Wall time\"])\n",
    "            end_time = float(rows[-1][\"Wall time\"])\n",
    "\n",
    "            # Calculate duration in seconds\n",
    "            duration_seconds = end_time - start_time\n",
    "\n",
    "            # Convert to hours and minutes\n",
    "            hours = int(duration_seconds // 3600)\n",
    "            minutes = int((duration_seconds % 3600) // 60)\n",
    "\n",
    "            if hours > 0:\n",
    "                return f\"{hours}h {minutes}m\"\n",
    "            else:\n",
    "                return f\"{minutes}m\"\n",
    "    except FileNotFoundError:\n",
    "        return \"N/A\"\n",
    "\n",
    "\n",
    "# Calculate training durations for all taxonomies\n",
    "training_durations = {}\n",
    "for taxonomy_name, config in taxonomies_config.items():\n",
    "    duration = calculate_training_duration(config[\"logger_name\"])\n",
    "    training_durations[taxonomy_name] = duration\n",
    "    print(f\"{taxonomy_name.capitalize()} taxonomy training duration: {duration}\")\n",
    "\n",
    "# Add training duration to domain_results\n",
    "for taxonomy_name in domain_results:\n",
    "    domain_results[taxonomy_name][\"training_time\"] = training_durations[taxonomy_name]\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(domain_results, orient=\"index\")\n",
    "\n",
    "# Clear index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54466759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline models and create baseline table\n",
    "from library.models.resnet import ResNetModel\n",
    "\n",
    "# Baseline model configurations from the real-world taxonomy notebooks\n",
    "baseline_configs = {\n",
    "    \"Caltech-101\": {\n",
    "        \"checkpoint\": \"resnet50-caltech101-min-val-loss.ckpt\",\n",
    "        \"architecture\": \"ResNet-50\",\n",
    "        \"optimizer\": \"SGD\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"dataset_module\": caltech101_dm,  # Use individual dataset module\n",
    "    },\n",
    "    \"Caltech-256\": {\n",
    "        \"checkpoint\": \"resnet50-caltech256-min-val-loss.ckpt\",\n",
    "        \"architecture\": \"ResNet-50\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dataset_module\": caltech256_dm,  # Use individual dataset module\n",
    "    },\n",
    "    \"CIFAR-100\": {\n",
    "        \"checkpoint\": \"resnet152-cifar100-min-val-loss.ckpt\",\n",
    "        \"architecture\": \"ResNet-152\",\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"dataset_module\": cifar100_original_dm,  # Use original CIFAR-100 dataset module\n",
    "    },\n",
    "}\n",
    "\n",
    "# Evaluate baseline models\n",
    "baseline_results = {}\n",
    "trainer = Trainer(logger=False, enable_checkpointing=False)\n",
    "\n",
    "for dataset_name, config in baseline_configs.items():\n",
    "    print(f\"Evaluating baseline model: {config['checkpoint']}\")\n",
    "\n",
    "    # Load baseline model\n",
    "    baseline_model = ResNetModel.load_from_checkpoint(\n",
    "        f\"checkpoints/{config['checkpoint']}\"\n",
    "    )\n",
    "\n",
    "    # Test on the dataset\n",
    "    test_results = trainer.test(baseline_model, datamodule=config[\"dataset_module\"])\n",
    "    accuracy = test_results[0][\"eval_accuracy\"] * 100  # Convert to percentage\n",
    "\n",
    "    baseline_results[dataset_name] = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Architecture\": config[\"architecture\"],\n",
    "        \"Optimizer\": config[\"optimizer\"],\n",
    "        \"Test Accuracy\": f\"{accuracy:.2f}\",\n",
    "    }\n",
    "\n",
    "# Create baseline models dataframe\n",
    "baseline_df = pd.DataFrame.from_dict(baseline_results, orient=\"index\")\n",
    "baseline_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Baseline Model Results:\")\n",
    "print(baseline_df)\n",
    "\n",
    "# Create LaTeX table for baseline models\n",
    "baseline_latex_table = baseline_df.style.hide(axis=\"index\").to_latex(\n",
    "    caption=\"Baseline ResNet model performance on individual datasets. These single-domain models serve as reference points for evaluating the universal models. Every baseline model was trained for 50 epochs.\",\n",
    "    label=\"tab:baseline_model_results\",\n",
    "    column_format=\"lccc\",\n",
    "    position=\"ht\",\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    ")\n",
    "\n",
    "# Save baseline table to file\n",
    "with open(\"../thesis/figures/baseline_model_results.tex\", \"w\") as f:\n",
    "    f.write(baseline_latex_table)\n",
    "\n",
    "# Extract baseline accuracies for use in universal model table\n",
    "caltech101_baseline = float(baseline_results[\"Caltech-101\"][\"Test Accuracy\"])\n",
    "caltech256_baseline = float(baseline_results[\"Caltech-256\"][\"Test Accuracy\"])\n",
    "cifar100_baseline = float(baseline_results[\"CIFAR-100\"][\"Test Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LaTeX table from results\n",
    "# Transform the dataframe to have better column names for the table\n",
    "df_table = df.copy()\n",
    "\n",
    "# Map taxonomy names to display names for LaTeX export\n",
    "name_mapping = {\n",
    "    \"hypothesis\": \"Hypothesis (2 Domain)\",\n",
    "    \"mcfp\": \"MCFP (2 Domain)\",\n",
    "    \"mcfp_binary\": \"MCFP Binary (2 Domain)\",\n",
    "    \"density_threshold\": \"Density Threshold (2 Domain)\",\n",
    "    \"naive_threshold\": \"Naive Threshold (2 Domain)\",\n",
    "    \"three_domain_hypothesis\": \"Hypothesis (3 Domain)\",\n",
    "    \"three_domain_mcfp\": \"MCFP (3 Domain)\",\n",
    "    \"three_domain_mcfp_binary\": \"MCFP Binary (3 Domain)\",\n",
    "    \"three_domain_density_threshold\": \"Density Threshold (3 Domain)\",\n",
    "    \"three_domain_naive_threshold\": \"Naive Threshold (3 Domain)\",\n",
    "}\n",
    "\n",
    "# Update the name column with display names\n",
    "df_table[\"name\"] = df_table[\"name\"].map(name_mapping)\n",
    "\n",
    "df_table.columns = [\n",
    "    \"Taxonomy\",\n",
    "    \"Caltech-101\",\n",
    "    \"Caltech-256\",\n",
    "    \"CIFAR-100\",\n",
    "    \"Avg\",\n",
    "    \"Training Time\",\n",
    "]\n",
    "\n",
    "# Select only the columns we want (excluding training time)\n",
    "df_table = df_table[\n",
    "    [\n",
    "        \"Taxonomy\",\n",
    "        \"Caltech-101\",\n",
    "        \"Caltech-256\",\n",
    "        \"CIFAR-100\",\n",
    "        \"Avg\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Convert accuracy values to percentages and add delta values for domain columns\n",
    "df_table[\"Avg\"] = (df_table[\"Avg\"] * 100).round(2)\n",
    "\n",
    "# Store original numeric values for comparison\n",
    "orig_caltech101 = df_table[\"Caltech-101\"].values.copy()\n",
    "orig_caltech256 = df_table[\"Caltech-256\"].values.copy()\n",
    "orig_cifar100 = df_table[\"CIFAR-100\"].values.copy()\n",
    "orig_combined = df_table[\"Avg\"].values.copy()\n",
    "\n",
    "# Convert columns to object type to avoid dtype warnings\n",
    "df_table[\"Caltech-101\"] = df_table[\"Caltech-101\"].astype(object)\n",
    "df_table[\"Caltech-256\"] = df_table[\"Caltech-256\"].astype(object)\n",
    "df_table[\"CIFAR-100\"] = df_table[\"CIFAR-100\"].astype(object)\n",
    "df_table[\"Avg\"] = df_table[\"Avg\"].astype(object)\n",
    "\n",
    "# Find best values for each column (excluding N/A values for CIFAR-100)\n",
    "best_caltech101_idx = orig_caltech101.argmax()\n",
    "best_caltech256_idx = orig_caltech256.argmax()\n",
    "# For CIFAR-100, only consider rows that have valid values (three-domain models)\n",
    "valid_cifar100_mask = pd.notna(orig_cifar100) & (orig_cifar100 != None)\n",
    "if valid_cifar100_mask.any():\n",
    "    best_cifar100_idx = orig_cifar100[valid_cifar100_mask].argmax()\n",
    "    # Convert to actual dataframe index\n",
    "    best_cifar100_idx = df_table.index[valid_cifar100_mask][best_cifar100_idx]\n",
    "else:\n",
    "    best_cifar100_idx = -1  # No valid CIFAR-100 results\n",
    "best_combined_idx = orig_combined.argmax()\n",
    "\n",
    "# Add delta values for domain columns\n",
    "for idx, row in df_table.iterrows():\n",
    "    # Caltech-101 column with delta\n",
    "    acc_101 = row[\"Caltech-101\"] * 100\n",
    "    delta_101 = acc_101 - caltech101_baseline\n",
    "    sign_101 = \"+\" if delta_101 >= 0 else \"\"\n",
    "    result_str = f\"{acc_101:.2f} ({sign_101}{delta_101:.2f})\"\n",
    "    # Make best result bold\n",
    "    if idx == best_caltech101_idx:\n",
    "        result_str = f\"\\\\textbf{{{result_str}}}\"\n",
    "    df_table.loc[idx, \"Caltech-101\"] = result_str\n",
    "\n",
    "    # Caltech-256 column with delta\n",
    "    acc_256 = row[\"Caltech-256\"] * 100\n",
    "    delta_256 = acc_256 - caltech256_baseline\n",
    "    sign_256 = \"+\" if delta_256 >= 0 else \"\"\n",
    "    result_str = f\"{acc_256:.2f} ({sign_256}{delta_256:.2f})\"\n",
    "    # Make best result bold\n",
    "    if idx == best_caltech256_idx:\n",
    "        result_str = f\"\\\\textbf{{{result_str}}}\"\n",
    "    df_table.loc[idx, \"Caltech-256\"] = result_str\n",
    "\n",
    "    # CIFAR-100 column with delta (only for three-domain model)\n",
    "    if pd.notna(row[\"CIFAR-100\"]) and row[\"CIFAR-100\"] is not None:\n",
    "        acc_100 = row[\"CIFAR-100\"] * 100\n",
    "        delta_100 = acc_100 - cifar100_baseline\n",
    "        sign_100 = \"+\" if delta_100 >= 0 else \"\"\n",
    "        result_str = f\"{acc_100:.2f} ({sign_100}{delta_100:.2f})\"\n",
    "        # Make best result bold\n",
    "        if idx == best_cifar100_idx:\n",
    "            result_str = f\"\\\\textbf{{{result_str}}}\"\n",
    "        df_table.loc[idx, \"CIFAR-100\"] = result_str\n",
    "    else:\n",
    "        df_table.loc[idx, \"CIFAR-100\"] = \"N/A\"\n",
    "\n",
    "# Format Avg column with proper rounding (no bold highlighting)\n",
    "for idx, row in df_table.iterrows():\n",
    "    avg_value = orig_combined[idx]\n",
    "    df_table.loc[idx, \"Avg\"] = f\"{avg_value:.2f}\"\n",
    "\n",
    "# Create LaTeX table\n",
    "latex_table = df_table.style.hide(axis=\"index\").to_latex(\n",
    "    caption=\"Universal model evaluation results on multi-domain test datasets. Two-domain models were trained on Caltech-101 + Caltech-256, while three-domain models were trained on all three datasets. Models were evaluated on individual domains as well as the combined test set (no weighting was applied, the individual test sets were simply concatenated). Domain accuracy values show performance differences compared to single-domain baseline models (see Table~\\\\ref{tab:baseline_model_results}). Best results per column are shown in bold. All accuracy values are shown as percentages. Density Threshold models use parameter 0.6, Naive Threshold models use parameter 0.1.\",\n",
    "    label=\"tab:universal_model_results\",\n",
    "    column_format=\"lcccc\",\n",
    "    position=\"ht\",\n",
    "    position_float=\"centering\",\n",
    "    hrules=True,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open(\"../thesis/figures/universal_model_results.tex\", \"w\") as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ab1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictReader\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"pgf\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LaTeX settings\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"EB Garamond\",\n",
    "        \"font.size\": 11,\n",
    "        \"pgf.texsystem\": \"lualatex\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create subplot with 10 plots, one for each taxonomy (2x5 grid)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "\n",
    "# Configuration for all ten taxonomies\n",
    "taxonomy_configs = [\n",
    "    {\n",
    "        \"name\": \"hypothesis\",\n",
    "        \"title\": \"Hypothesis Taxonomy (2 Domains)\",\n",
    "        \"file_prefix\": \"universal_hypothesis_multi_domain\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mcfp\",\n",
    "        \"title\": \"MCFP Taxonomy (2 Domains)\",\n",
    "        \"file_prefix\": \"universal_mcfp_multi_domain\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mcfp_binary\",\n",
    "        \"title\": \"MCFP Binary Taxonomy (2 Domains)\",\n",
    "        \"file_prefix\": \"universal_mcfp_binary_multi_domain\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"density_threshold\",\n",
    "        \"title\": \"Density Threshold Taxonomy (2 Domains)\",\n",
    "        \"file_prefix\": \"universal_density_threshold_multi_domain\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"naive_threshold\",\n",
    "        \"title\": \"Naive Threshold Taxonomy (2 Domains)\",\n",
    "        \"file_prefix\": \"universal_naive_threshold_multi_domain\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"three_domain_hypothesis\",\n",
    "        \"title\": \"Hypothesis Taxonomy (3 Domains)\",\n",
    "        \"file_prefix\": \"universal_three_domain_hypothesis\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"three_domain_mcfp\",\n",
    "        \"title\": \"MCFP Taxonomy (3 Domains)\",\n",
    "        \"file_prefix\": \"universal_three_domain_mcfp\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"three_domain_mcfp_binary\",\n",
    "        \"title\": \"MCFP Binary Taxonomy (3 Domains)\",\n",
    "        \"file_prefix\": \"universal_three_domain_mcfp_binary\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"three_domain_density_threshold\",\n",
    "        \"title\": \"Density Threshold Taxonomy (3 Domains)\",\n",
    "        \"file_prefix\": \"universal_three_domain_density_threshold\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"three_domain_naive_threshold\",\n",
    "        \"title\": \"Naive Threshold Taxonomy (3 Domains)\",\n",
    "        \"file_prefix\": \"universal_three_domain_naive_threshold\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Plot training curves for each taxonomy\n",
    "for idx, config in enumerate(taxonomy_configs):\n",
    "    row = idx // 5\n",
    "    col = idx % 5\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    try:\n",
    "        # Load training data\n",
    "        with open(f\"training_results/{config['file_prefix']}_train.csv\", \"r\") as f:\n",
    "            reader = DictReader(f)\n",
    "            steps_train = []\n",
    "            train = []\n",
    "            for row_data in reader:\n",
    "                steps_train.append(int(row_data[\"Step\"]))\n",
    "                train.append(float(row_data[\"Value\"]))\n",
    "\n",
    "        # Load validation data\n",
    "        with open(f\"training_results/{config['file_prefix']}_val.csv\", \"r\") as f:\n",
    "            reader = DictReader(f)\n",
    "            steps_val = []\n",
    "            val = []\n",
    "            for row_data in reader:\n",
    "                steps_val.append(int(row_data[\"Step\"]))\n",
    "                val.append(float(row_data[\"Value\"]))\n",
    "\n",
    "        # Plot training and validation curves\n",
    "        ax.plot(steps_train, train, label=\"Train\", color=\"blue\")\n",
    "        ax.plot(steps_val, val, label=\"Validation\", color=\"red\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # If training files don't exist, show a placeholder\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            f\"Training data\\nnot available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"),\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(config[\"title\"], fontsize=9)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../thesis/figures/universal_model_training_curves.pgf\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
