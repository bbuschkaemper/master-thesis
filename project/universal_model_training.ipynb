{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249d7fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caltech-256 classes: 257\n",
      "Caltech-101 classes: 101\n",
      "Caltech-256 class 0: 001.ak47\n",
      "Caltech-256 class 1: 002.american-flag\n",
      "Caltech-256 class 2: 003.backpack\n",
      "Caltech-256 class 3: 004.baseball-bat\n",
      "Caltech-256 class 4: 005.baseball-glove\n",
      "Caltech-256 class 5: 006.basketball-hoop\n",
      "Caltech-256 class 6: 007.bat\n",
      "Caltech-256 class 7: 008.bathtub\n",
      "Caltech-256 class 8: 009.bear\n",
      "Caltech-256 class 9: 010.beer-mug\n",
      "Caltech-256 class 10: 011.billiards\n",
      "Caltech-256 class 11: 012.binoculars\n",
      "Caltech-256 class 12: 013.birdbath\n",
      "Caltech-256 class 13: 014.blimp\n",
      "Caltech-256 class 14: 015.bonsai-101\n",
      "Caltech-256 class 15: 016.boom-box\n",
      "Caltech-256 class 16: 017.bowling-ball\n",
      "Caltech-256 class 17: 018.bowling-pin\n",
      "Caltech-256 class 18: 019.boxing-glove\n",
      "Caltech-256 class 19: 020.brain-101\n",
      "Caltech-256 class 20: 021.breadmaker\n",
      "Caltech-256 class 21: 022.buddha-101\n",
      "Caltech-256 class 22: 023.bulldozer\n",
      "Caltech-256 class 23: 024.butterfly\n",
      "Caltech-256 class 24: 025.cactus\n",
      "Caltech-256 class 25: 026.cake\n",
      "Caltech-256 class 26: 027.calculator\n",
      "Caltech-256 class 27: 028.camel\n",
      "Caltech-256 class 28: 029.cannon\n",
      "Caltech-256 class 29: 030.canoe\n",
      "Caltech-256 class 30: 031.car-tire\n",
      "Caltech-256 class 31: 032.cartman\n",
      "Caltech-256 class 32: 033.cd\n",
      "Caltech-256 class 33: 034.centipede\n",
      "Caltech-256 class 34: 035.cereal-box\n",
      "Caltech-256 class 35: 036.chandelier-101\n",
      "Caltech-256 class 36: 037.chess-board\n",
      "Caltech-256 class 37: 038.chimp\n",
      "Caltech-256 class 38: 039.chopsticks\n",
      "Caltech-256 class 39: 040.cockroach\n",
      "Caltech-256 class 40: 041.coffee-mug\n",
      "Caltech-256 class 41: 042.coffin\n",
      "Caltech-256 class 42: 043.coin\n",
      "Caltech-256 class 43: 044.comet\n",
      "Caltech-256 class 44: 045.computer-keyboard\n",
      "Caltech-256 class 45: 046.computer-monitor\n",
      "Caltech-256 class 46: 047.computer-mouse\n",
      "Caltech-256 class 47: 048.conch\n",
      "Caltech-256 class 48: 049.cormorant\n",
      "Caltech-256 class 49: 050.covered-wagon\n",
      "Caltech-256 class 50: 051.cowboy-hat\n",
      "Caltech-256 class 51: 052.crab-101\n",
      "Caltech-256 class 52: 053.desk-globe\n",
      "Caltech-256 class 53: 054.diamond-ring\n",
      "Caltech-256 class 54: 055.dice\n",
      "Caltech-256 class 55: 056.dog\n",
      "Caltech-256 class 56: 057.dolphin-101\n",
      "Caltech-256 class 57: 058.doorknob\n",
      "Caltech-256 class 58: 059.drinking-straw\n",
      "Caltech-256 class 59: 060.duck\n",
      "Caltech-256 class 60: 061.dumb-bell\n",
      "Caltech-256 class 61: 062.eiffel-tower\n",
      "Caltech-256 class 62: 063.electric-guitar-101\n",
      "Caltech-256 class 63: 064.elephant-101\n",
      "Caltech-256 class 64: 065.elk\n",
      "Caltech-256 class 65: 066.ewer-101\n",
      "Caltech-256 class 66: 067.eyeglasses\n",
      "Caltech-256 class 67: 068.fern\n",
      "Caltech-256 class 68: 069.fighter-jet\n",
      "Caltech-256 class 69: 070.fire-extinguisher\n",
      "Caltech-256 class 70: 071.fire-hydrant\n",
      "Caltech-256 class 71: 072.fire-truck\n",
      "Caltech-256 class 72: 073.fireworks\n",
      "Caltech-256 class 73: 074.flashlight\n",
      "Caltech-256 class 74: 075.floppy-disk\n",
      "Caltech-256 class 75: 076.football-helmet\n",
      "Caltech-256 class 76: 077.french-horn\n",
      "Caltech-256 class 77: 078.fried-egg\n",
      "Caltech-256 class 78: 079.frisbee\n",
      "Caltech-256 class 79: 080.frog\n",
      "Caltech-256 class 80: 081.frying-pan\n",
      "Caltech-256 class 81: 082.galaxy\n",
      "Caltech-256 class 82: 083.gas-pump\n",
      "Caltech-256 class 83: 084.giraffe\n",
      "Caltech-256 class 84: 085.goat\n",
      "Caltech-256 class 85: 086.golden-gate-bridge\n",
      "Caltech-256 class 86: 087.goldfish\n",
      "Caltech-256 class 87: 088.golf-ball\n",
      "Caltech-256 class 88: 089.goose\n",
      "Caltech-256 class 89: 090.gorilla\n",
      "Caltech-256 class 90: 091.grand-piano-101\n",
      "Caltech-256 class 91: 092.grapes\n",
      "Caltech-256 class 92: 093.grasshopper\n",
      "Caltech-256 class 93: 094.guitar-pick\n",
      "Caltech-256 class 94: 095.hamburger\n",
      "Caltech-256 class 95: 096.hammock\n",
      "Caltech-256 class 96: 097.harmonica\n",
      "Caltech-256 class 97: 098.harp\n",
      "Caltech-256 class 98: 099.harpsichord\n",
      "Caltech-256 class 99: 100.hawksbill-101\n",
      "Caltech-256 class 100: 101.head-phones\n",
      "Caltech-256 class 101: 102.helicopter-101\n",
      "Caltech-256 class 102: 103.hibiscus\n",
      "Caltech-256 class 103: 104.homer-simpson\n",
      "Caltech-256 class 104: 105.horse\n",
      "Caltech-256 class 105: 106.horseshoe-crab\n",
      "Caltech-256 class 106: 107.hot-air-balloon\n",
      "Caltech-256 class 107: 108.hot-dog\n",
      "Caltech-256 class 108: 109.hot-tub\n",
      "Caltech-256 class 109: 110.hourglass\n",
      "Caltech-256 class 110: 111.house-fly\n",
      "Caltech-256 class 111: 112.human-skeleton\n",
      "Caltech-256 class 112: 113.hummingbird\n",
      "Caltech-256 class 113: 114.ibis-101\n",
      "Caltech-256 class 114: 115.ice-cream-cone\n",
      "Caltech-256 class 115: 116.iguana\n",
      "Caltech-256 class 116: 117.ipod\n",
      "Caltech-256 class 117: 118.iris\n",
      "Caltech-256 class 118: 119.jesus-christ\n",
      "Caltech-256 class 119: 120.joy-stick\n",
      "Caltech-256 class 120: 121.kangaroo-101\n",
      "Caltech-256 class 121: 122.kayak\n",
      "Caltech-256 class 122: 123.ketch-101\n",
      "Caltech-256 class 123: 124.killer-whale\n",
      "Caltech-256 class 124: 125.knife\n",
      "Caltech-256 class 125: 126.ladder\n",
      "Caltech-256 class 126: 127.laptop-101\n",
      "Caltech-256 class 127: 128.lathe\n",
      "Caltech-256 class 128: 129.leopards-101\n",
      "Caltech-256 class 129: 130.license-plate\n",
      "Caltech-256 class 130: 131.lightbulb\n",
      "Caltech-256 class 131: 132.light-house\n",
      "Caltech-256 class 132: 133.lightning\n",
      "Caltech-256 class 133: 134.llama-101\n",
      "Caltech-256 class 134: 135.mailbox\n",
      "Caltech-256 class 135: 136.mandolin\n",
      "Caltech-256 class 136: 137.mars\n",
      "Caltech-256 class 137: 138.mattress\n",
      "Caltech-256 class 138: 139.megaphone\n",
      "Caltech-256 class 139: 140.menorah-101\n",
      "Caltech-256 class 140: 141.microscope\n",
      "Caltech-256 class 141: 142.microwave\n",
      "Caltech-256 class 142: 143.minaret\n",
      "Caltech-256 class 143: 144.minotaur\n",
      "Caltech-256 class 144: 145.motorbikes-101\n",
      "Caltech-256 class 145: 146.mountain-bike\n",
      "Caltech-256 class 146: 147.mushroom\n",
      "Caltech-256 class 147: 148.mussels\n",
      "Caltech-256 class 148: 149.necktie\n",
      "Caltech-256 class 149: 150.octopus\n",
      "Caltech-256 class 150: 151.ostrich\n",
      "Caltech-256 class 151: 152.owl\n",
      "Caltech-256 class 152: 153.palm-pilot\n",
      "Caltech-256 class 153: 154.palm-tree\n",
      "Caltech-256 class 154: 155.paperclip\n",
      "Caltech-256 class 155: 156.paper-shredder\n",
      "Caltech-256 class 156: 157.pci-card\n",
      "Caltech-256 class 157: 158.penguin\n",
      "Caltech-256 class 158: 159.people\n",
      "Caltech-256 class 159: 160.pez-dispenser\n",
      "Caltech-256 class 160: 161.photocopier\n",
      "Caltech-256 class 161: 162.picnic-table\n",
      "Caltech-256 class 162: 163.playing-card\n",
      "Caltech-256 class 163: 164.porcupine\n",
      "Caltech-256 class 164: 165.pram\n",
      "Caltech-256 class 165: 166.praying-mantis\n",
      "Caltech-256 class 166: 167.pyramid\n",
      "Caltech-256 class 167: 168.raccoon\n",
      "Caltech-256 class 168: 169.radio-telescope\n",
      "Caltech-256 class 169: 170.rainbow\n",
      "Caltech-256 class 170: 171.refrigerator\n",
      "Caltech-256 class 171: 172.revolver-101\n",
      "Caltech-256 class 172: 173.rifle\n",
      "Caltech-256 class 173: 174.rotary-phone\n",
      "Caltech-256 class 174: 175.roulette-wheel\n",
      "Caltech-256 class 175: 176.saddle\n",
      "Caltech-256 class 176: 177.saturn\n",
      "Caltech-256 class 177: 178.school-bus\n",
      "Caltech-256 class 178: 179.scorpion-101\n",
      "Caltech-256 class 179: 180.screwdriver\n",
      "Caltech-256 class 180: 181.segway\n",
      "Caltech-256 class 181: 182.self-propelled-lawn-mower\n",
      "Caltech-256 class 182: 183.sextant\n",
      "Caltech-256 class 183: 184.sheet-music\n",
      "Caltech-256 class 184: 185.skateboard\n",
      "Caltech-256 class 185: 186.skunk\n",
      "Caltech-256 class 186: 187.skyscraper\n",
      "Caltech-256 class 187: 188.smokestack\n",
      "Caltech-256 class 188: 189.snail\n",
      "Caltech-256 class 189: 190.snake\n",
      "Caltech-256 class 190: 191.sneaker\n",
      "Caltech-256 class 191: 192.snowmobile\n",
      "Caltech-256 class 192: 193.soccer-ball\n",
      "Caltech-256 class 193: 194.socks\n",
      "Caltech-256 class 194: 195.soda-can\n",
      "Caltech-256 class 195: 196.spaghetti\n",
      "Caltech-256 class 196: 197.speed-boat\n",
      "Caltech-256 class 197: 198.spider\n",
      "Caltech-256 class 198: 199.spoon\n",
      "Caltech-256 class 199: 200.stained-glass\n",
      "Caltech-256 class 200: 201.starfish-101\n",
      "Caltech-256 class 201: 202.steering-wheel\n",
      "Caltech-256 class 202: 203.stirrups\n",
      "Caltech-256 class 203: 204.sunflower-101\n",
      "Caltech-256 class 204: 205.superman\n",
      "Caltech-256 class 205: 206.sushi\n",
      "Caltech-256 class 206: 207.swan\n",
      "Caltech-256 class 207: 208.swiss-army-knife\n",
      "Caltech-256 class 208: 209.sword\n",
      "Caltech-256 class 209: 210.syringe\n",
      "Caltech-256 class 210: 211.tambourine\n",
      "Caltech-256 class 211: 212.teapot\n",
      "Caltech-256 class 212: 213.teddy-bear\n",
      "Caltech-256 class 213: 214.teepee\n",
      "Caltech-256 class 214: 215.telephone-box\n",
      "Caltech-256 class 215: 216.tennis-ball\n",
      "Caltech-256 class 216: 217.tennis-court\n",
      "Caltech-256 class 217: 218.tennis-racket\n",
      "Caltech-256 class 218: 219.theodolite\n",
      "Caltech-256 class 219: 220.toaster\n",
      "Caltech-256 class 220: 221.tomato\n",
      "Caltech-256 class 221: 222.tombstone\n",
      "Caltech-256 class 222: 223.top-hat\n",
      "Caltech-256 class 223: 224.touring-bike\n",
      "Caltech-256 class 224: 225.tower-pisa\n",
      "Caltech-256 class 225: 226.traffic-light\n",
      "Caltech-256 class 226: 227.treadmill\n",
      "Caltech-256 class 227: 228.triceratops\n",
      "Caltech-256 class 228: 229.tricycle\n",
      "Caltech-256 class 229: 230.trilobite-101\n",
      "Caltech-256 class 230: 231.tripod\n",
      "Caltech-256 class 231: 232.t-shirt\n",
      "Caltech-256 class 232: 233.tuning-fork\n",
      "Caltech-256 class 233: 234.tweezer\n",
      "Caltech-256 class 234: 235.umbrella-101\n",
      "Caltech-256 class 235: 236.unicorn\n",
      "Caltech-256 class 236: 237.vcr\n",
      "Caltech-256 class 237: 238.video-projector\n",
      "Caltech-256 class 238: 239.washing-machine\n",
      "Caltech-256 class 239: 240.watch-101\n",
      "Caltech-256 class 240: 241.waterfall\n",
      "Caltech-256 class 241: 242.watermelon\n",
      "Caltech-256 class 242: 243.welding-mask\n",
      "Caltech-256 class 243: 244.wheelbarrow\n",
      "Caltech-256 class 244: 245.windmill\n",
      "Caltech-256 class 245: 246.wine-bottle\n",
      "Caltech-256 class 246: 247.xylophone\n",
      "Caltech-256 class 247: 248.yarmulke\n",
      "Caltech-256 class 248: 249.yo-yo\n",
      "Caltech-256 class 249: 250.zebra\n",
      "Caltech-256 class 250: 251.airplanes-101\n",
      "Caltech-256 class 251: 252.car-side-101\n",
      "Caltech-256 class 252: 253.faces-easy-101\n",
      "Caltech-256 class 253: 254.greyhound\n",
      "Caltech-256 class 254: 255.tennis-shoes\n",
      "Caltech-256 class 255: 256.toad\n",
      "Caltech-256 class 256: 257.clutter\n",
      "Caltech-101 class 0: Faces\n",
      "Caltech-101 class 1: Faces_easy\n",
      "Caltech-101 class 2: Leopards\n",
      "Caltech-101 class 3: Motorbikes\n",
      "Caltech-101 class 4: accordion\n",
      "Caltech-101 class 5: airplanes\n",
      "Caltech-101 class 6: anchor\n",
      "Caltech-101 class 7: ant\n",
      "Caltech-101 class 8: barrel\n",
      "Caltech-101 class 9: bass\n",
      "Caltech-101 class 10: beaver\n",
      "Caltech-101 class 11: binocular\n",
      "Caltech-101 class 12: bonsai\n",
      "Caltech-101 class 13: brain\n",
      "Caltech-101 class 14: brontosaurus\n",
      "Caltech-101 class 15: buddha\n",
      "Caltech-101 class 16: butterfly\n",
      "Caltech-101 class 17: camera\n",
      "Caltech-101 class 18: cannon\n",
      "Caltech-101 class 19: car_side\n",
      "Caltech-101 class 20: ceiling_fan\n",
      "Caltech-101 class 21: cellphone\n",
      "Caltech-101 class 22: chair\n",
      "Caltech-101 class 23: chandelier\n",
      "Caltech-101 class 24: cougar_body\n",
      "Caltech-101 class 25: cougar_face\n",
      "Caltech-101 class 26: crab\n",
      "Caltech-101 class 27: crayfish\n",
      "Caltech-101 class 28: crocodile\n",
      "Caltech-101 class 29: crocodile_head\n",
      "Caltech-101 class 30: cup\n",
      "Caltech-101 class 31: dalmatian\n",
      "Caltech-101 class 32: dollar_bill\n",
      "Caltech-101 class 33: dolphin\n",
      "Caltech-101 class 34: dragonfly\n",
      "Caltech-101 class 35: electric_guitar\n",
      "Caltech-101 class 36: elephant\n",
      "Caltech-101 class 37: emu\n",
      "Caltech-101 class 38: euphonium\n",
      "Caltech-101 class 39: ewer\n",
      "Caltech-101 class 40: ferry\n",
      "Caltech-101 class 41: flamingo\n",
      "Caltech-101 class 42: flamingo_head\n",
      "Caltech-101 class 43: garfield\n",
      "Caltech-101 class 44: gerenuk\n",
      "Caltech-101 class 45: gramophone\n",
      "Caltech-101 class 46: grand_piano\n",
      "Caltech-101 class 47: hawksbill\n",
      "Caltech-101 class 48: headphone\n",
      "Caltech-101 class 49: hedgehog\n",
      "Caltech-101 class 50: helicopter\n",
      "Caltech-101 class 51: ibis\n",
      "Caltech-101 class 52: inline_skate\n",
      "Caltech-101 class 53: joshua_tree\n",
      "Caltech-101 class 54: kangaroo\n",
      "Caltech-101 class 55: ketch\n",
      "Caltech-101 class 56: lamp\n",
      "Caltech-101 class 57: laptop\n",
      "Caltech-101 class 58: llama\n",
      "Caltech-101 class 59: lobster\n",
      "Caltech-101 class 60: lotus\n",
      "Caltech-101 class 61: mandolin\n",
      "Caltech-101 class 62: mayfly\n",
      "Caltech-101 class 63: menorah\n",
      "Caltech-101 class 64: metronome\n",
      "Caltech-101 class 65: minaret\n",
      "Caltech-101 class 66: nautilus\n",
      "Caltech-101 class 67: octopus\n",
      "Caltech-101 class 68: okapi\n",
      "Caltech-101 class 69: pagoda\n",
      "Caltech-101 class 70: panda\n",
      "Caltech-101 class 71: pigeon\n",
      "Caltech-101 class 72: pizza\n",
      "Caltech-101 class 73: platypus\n",
      "Caltech-101 class 74: pyramid\n",
      "Caltech-101 class 75: revolver\n",
      "Caltech-101 class 76: rhino\n",
      "Caltech-101 class 77: rooster\n",
      "Caltech-101 class 78: saxophone\n",
      "Caltech-101 class 79: schooner\n",
      "Caltech-101 class 80: scissors\n",
      "Caltech-101 class 81: scorpion\n",
      "Caltech-101 class 82: sea_horse\n",
      "Caltech-101 class 83: snoopy\n",
      "Caltech-101 class 84: soccer_ball\n",
      "Caltech-101 class 85: stapler\n",
      "Caltech-101 class 86: starfish\n",
      "Caltech-101 class 87: stegosaurus\n",
      "Caltech-101 class 88: stop_sign\n",
      "Caltech-101 class 89: strawberry\n",
      "Caltech-101 class 90: sunflower\n",
      "Caltech-101 class 91: tick\n",
      "Caltech-101 class 92: trilobite\n",
      "Caltech-101 class 93: umbrella\n",
      "Caltech-101 class 94: watch\n",
      "Caltech-101 class 95: water_lilly\n",
      "Caltech-101 class 96: wheelchair\n",
      "Caltech-101 class 97: wild_cat\n",
      "Caltech-101 class 98: windsor_chair\n",
      "Caltech-101 class 99: wrench\n",
      "Caltech-101 class 100: yin_yang\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import Caltech256, Caltech101\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from library.taxonomy import Taxonomy\n",
    "from library.models import UniversalResNetModel\n",
    "from library.datasets import Caltech256DataModule, Caltech101DataModule\n",
    "\n",
    "# Load dataset information\n",
    "caltech256_labels = Caltech256(root=\"datasets/caltech256\", download=False).categories\n",
    "caltech101_labels = Caltech101(root=\"datasets/caltech101\", download=False).categories\n",
    "\n",
    "print(f\"Caltech-256 classes: {len(caltech256_labels)}\")\n",
    "print(f\"Caltech-101 classes: {len(caltech101_labels)}\")\n",
    "\n",
    "# Reduce the precision of matrix multiplication to speed up training\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# Print labels\n",
    "for idx, label in enumerate(caltech256_labels):\n",
    "    print(f\"Caltech-256 class {idx}: {label}\")\n",
    "\n",
    "for idx, label in enumerate(caltech101_labels):\n",
    "    print(f\"Caltech-101 class {idx}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25714df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the universal taxonomy created from the real-world datasets\n",
    "taxonomy = Taxonomy.load(\"taxonomies/caltech256_caltech101.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a9cc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Caltech-101 (Domain 0)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TRAIN = True  # Set to True to train models from scratch\n",
    "DOMAIN_ID = 0  # 0 for Caltech-101, 1 for Caltech-256\n",
    "\n",
    "# Select dataset based on domain\n",
    "if DOMAIN_ID == 0:\n",
    "    dataset_module = Caltech101DataModule()\n",
    "    dataset_name = \"Caltech-101\"\n",
    "    model_name = \"universal-resnet50-caltech101-min-val-loss\"\n",
    "    logger_name = \"universal_caltech101\"\n",
    "    num_classes = len(caltech101_labels)\n",
    "else:\n",
    "    dataset_module = Caltech256DataModule()\n",
    "    dataset_name = \"Caltech-256\"\n",
    "    model_name = \"universal-resnet50-caltech256-min-val-loss\"\n",
    "    logger_name = \"universal_caltech256\"\n",
    "    num_classes = len(caltech256_labels)\n",
    "\n",
    "print(f\"Training on {dataset_name} (Domain {DOMAIN_ID})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0c7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  max_epochs: 50\n",
      "  optim: adamw\n",
      "  optim_kwargs: {'lr': 0.001, 'weight_decay': 0.001}\n",
      "  lr_scheduler: multistep\n",
      "  lr_scheduler_kwargs: {'milestones': [15, 30, 40], 'gamma': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    \"max_epochs\": 50,\n",
    "    \"optim\": \"adamw\",\n",
    "    \"optim_kwargs\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    \"lr_scheduler\": \"multistep\",\n",
    "    \"lr_scheduler_kwargs\": {\n",
    "        \"milestones\": [15, 30, 40],\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcc8cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal ResNet model created successfully!\n",
      "Model output size (universal classes): 358\n",
      "Training domain set to: 0\n"
     ]
    }
   ],
   "source": [
    "# Create the Universal ResNet model\n",
    "model = UniversalResNetModel(\n",
    "    taxonomy=taxonomy,\n",
    "    architecture=\"resnet50\",\n",
    "    optim=training_config[\"optim\"],\n",
    "    optim_kwargs=training_config[\"optim_kwargs\"],\n",
    "    lr_scheduler=training_config[\"lr_scheduler\"],\n",
    "    lr_scheduler_kwargs=training_config[\"lr_scheduler_kwargs\"],\n",
    ")\n",
    "\n",
    "# Set the domain for training\n",
    "model.set_domain(DOMAIN_ID)\n",
    "\n",
    "print(f\"Universal ResNet model created successfully!\")\n",
    "print(f\"Model output size (universal classes): {model.num_universal_classes}\")\n",
    "print(f\"Training domain set to: {DOMAIN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb2e34cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured for training\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer\n",
    "if TRAIN:\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs\", name=logger_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=training_config[\"max_epochs\"],\n",
    "        logger=tb_logger,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                dirpath=\"checkpoints\",\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                save_top_k=1,\n",
    "                filename=model_name,\n",
    "                enable_version_counter=False,\n",
    "            )\n",
    "        ],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "    )\n",
    "\n",
    "    print(\"Trainer configured for training\")\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    print(\"Trainer configured for evaluation only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b613db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on Caltech-101...\n",
      "Expected training time: ~50 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/bjoern/dev/master-thesis/project/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model     | ResNet    | 26.3 M | train\n",
      "1 | criterion | KLDivLoss | 0      | train\n",
      "------------------------------------------------\n",
      "26.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 M    Total params\n",
      "105.366   Total estimated model params size (MB)\n",
      "162       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model     | ResNet    | 26.3 M | train\n",
      "1 | criterion | KLDivLoss | 0      | train\n",
      "------------------------------------------------\n",
      "26.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 M    Total params\n",
      "105.366   Total estimated model params size (MB)\n",
      "162       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  51%|█████▏    | 56/109 [00:16<00:15,  3.42it/s, v_num=5] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/torch/optim/adamw.py:220\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/master-thesis/project/library/models.py:360\u001b[39m, in \u001b[36mUniversalResNetModel.training_step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# Convert domain class targets to universal class targets\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m universal_targets = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_domain_class_to_universal_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# For KL divergence, we need log probabilities of predictions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/master-thesis/project/library/models.py:298\u001b[39m, in \u001b[36mUniversalResNetModel._domain_class_to_universal_targets\u001b[39m\u001b[34m(self, domain_class_labels)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;66;03m# Move matrix to same device as labels\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m raw_matrix = \u001b[43mraw_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdomain_class_labels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Use matrix indexing to get raw targets\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected training time: ~\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_config[\u001b[33m'\u001b[39m\u001b[33mmax_epochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Test the trained model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Train or load the model\n",
    "if TRAIN:\n",
    "    print(f\"Starting training on {dataset_name}...\")\n",
    "    print(f\"Expected training time: ~{training_config['max_epochs']} epochs\")\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=dataset_module)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    # Test the trained model\n",
    "    print(\"Testing the trained model...\")\n",
    "    results = trainer.test(datamodule=dataset_module, ckpt_path=\"best\")\n",
    "\n",
    "else:\n",
    "    # Load pre-trained model\n",
    "    print(f\"Loading pre-trained model: {model_name}.ckpt\")\n",
    "    try:\n",
    "        model = UniversalResNetModel.load_from_checkpoint(\n",
    "            f\"checkpoints/{model_name}.ckpt\",\n",
    "            taxonomy=taxonomy,  # Need to pass taxonomy since it's not serialized\n",
    "        )\n",
    "        model.set_domain(DOMAIN_ID)\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "        # Test the loaded model\n",
    "        results = trainer.test(model, datamodule=dataset_module)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Checkpoint file not found: checkpoints/{model_name}.ckpt\")\n",
    "        print(\"Please set TRAIN=True to train the model first.\")\n",
    "        results = None\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nTest Results for {dataset_name}:\")\n",
    "    for key, value in results[0].items():\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
