{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a10741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "from library.taxonomy_constructors import ManualTaxonomy, CrossPredictionsTaxonomy\n",
    "from library.models.resnet import ResNetModel\n",
    "from library.datasets.mnist import MNISTMappedDataModule\n",
    "from library.datasets.svhn import SVHNMappedDataModule\n",
    "from library.taxonomy import DomainClass, Relationship\n",
    "\n",
    "# Reduce the precision of matrix multiplication to speed up training\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec912628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth taxonomy created with 20 relationships\n",
      "Graph has 20 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "# Create ground truth manual taxonomy\n",
    "# This represents the \"perfect\" taxonomy where each digit maps to its corresponding digit\n",
    "# Domain 0: MNIST, Domain 1: SVHN\n",
    "\n",
    "# Define domain labels for clarity\n",
    "domain_labels = {\n",
    "    0: [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],  # MNIST digits\n",
    "    1: [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],  # SVHN digits\n",
    "}\n",
    "\n",
    "# Create perfect 1:1 mapping relationships between matching digits\n",
    "# Each MNIST digit should map to the corresponding SVHN digit with confidence 1.0\n",
    "relationships = []\n",
    "for digit in range(10):\n",
    "    # Create domain classes for MNIST and SVHN\n",
    "    mnist_class = DomainClass((np.intp(0), np.intp(digit)))  # Domain 0 (MNIST), digit\n",
    "    svhn_class = DomainClass((np.intp(1), np.intp(digit)))  # Domain 1 (SVHN), digit\n",
    "\n",
    "    # Create bidirectional relationships\n",
    "    # MNIST digit -> SVHN digit\n",
    "    relationships.append(Relationship((mnist_class, svhn_class, 1.0)))\n",
    "    # SVHN digit -> MNIST digit\n",
    "    relationships.append(Relationship((svhn_class, mnist_class, 1.0)))\n",
    "\n",
    "# Create the ground truth manual taxonomy using the normal constructor\n",
    "ground_truth_taxonomy = ManualTaxonomy(\n",
    "    num_domains=2,\n",
    "    num_nodes=10,\n",
    "    relationships=relationships,\n",
    "    domain_labels=domain_labels,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Ground truth taxonomy created with {len(ground_truth_taxonomy.relationships)} relationships\"\n",
    ")\n",
    "print(\n",
    "    f\"Graph has {ground_truth_taxonomy.graph.number_of_nodes()} nodes and {ground_truth_taxonomy.graph.number_of_edges()} edges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef597f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "SVHN mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "Using all 10 digits from both datasets\n"
     ]
    }
   ],
   "source": [
    "# Create identity mappings for both datasets (all digits 0-9)\n",
    "mnist_mapping = {i: i for i in range(10)}  # Keep all MNIST digits as-is\n",
    "svhn_mapping = {i: i for i in range(10)}  # Keep all SVHN digits as-is\n",
    "\n",
    "print(f\"MNIST mapping: {mnist_mapping}\")\n",
    "print(f\"SVHN mapping: {svhn_mapping}\")\n",
    "print(f\"Using all 10 digits from both datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b17add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TRAIN = True  # Set to True to train models from scratch\n",
    "\n",
    "\n",
    "def train_digit_model(datamodule, domain_name, logger_name, model_name):\n",
    "    \"\"\"Train a ResNet model for a specific digit domain\"\"\"\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs\", name=logger_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=3,  # Fewer epochs for digit classification\n",
    "        logger=tb_logger if TRAIN else False,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(\n",
    "                dirpath=\"checkpoints\",\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "                save_top_k=1,\n",
    "                filename=model_name,\n",
    "                enable_version_counter=False,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if TRAIN:\n",
    "        model = ResNetModel(\n",
    "            num_classes=10,  # 10 digits\n",
    "            architecture=\"resnet50\",\n",
    "            optim=\"adamw\",  # Use AdamW for better convergence on digit data\n",
    "            optim_kwargs={\n",
    "                \"lr\": 0.001,\n",
    "                \"weight_decay\": 1e-4,\n",
    "            },\n",
    "        )\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "        results = trainer.test(datamodule=datamodule, ckpt_path=\"best\")\n",
    "    else:\n",
    "        model = ResNetModel.load_from_checkpoint(f\"checkpoints/{model_name}.ckpt\")\n",
    "        results = trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "    print(f\"{domain_name} Results: {results}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7a8fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Testing MNIST Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/bjoern/dev/master-thesis/project/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 26.3 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "26.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 M    Total params\n",
      "105.186   Total estimated model params size (MB)\n",
      "162       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [01:15<00:00,  9.97it/s, v_num=5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [01:15<00:00,  9.97it/s, v_num=5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/bjoern/dev/master-thesis/project/checkpoints/resnet50-mnist-min-val-loss.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/bjoern/dev/master-thesis/project/checkpoints/resnet50-mnist-min-val-loss.ckpt\n",
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:05<00:00, 30.57it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.9745000004768372\n",
      "        eval_loss           0.1137707382440567\n",
      "        hp_metric           0.9745000004768372\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "MNIST Results: [{'eval_loss': 0.1137707382440567, 'eval_accuracy': 0.9745000004768372, 'hp_metric': 0.9745000004768372}]\n"
     ]
    }
   ],
   "source": [
    "# Train MNIST model (Domain 0)\n",
    "print(\"Training/Testing MNIST Model:\")\n",
    "mnist_datamodule = MNISTMappedDataModule(mapping=mnist_mapping)\n",
    "mnist_results = train_digit_model(\n",
    "    mnist_datamodule,\n",
    "    \"MNIST\",\n",
    "    \"digit_mnist\",\n",
    "    \"resnet50-mnist-min-val-loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d883b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Testing SVHN Model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "100%|██████████| 182M/182M [00:29<00:00, 6.07MB/s] \n",
      "100%|██████████| 64.3M/64.3M [00:21<00:00, 3.02MB/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | ResNet           | 26.3 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "26.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.3 M    Total params\n",
      "105.186   Total estimated model params size (MB)\n",
      "162       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 916/916 [02:11<00:00,  6.99it/s, v_num=0]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 916/916 [02:12<00:00,  6.92it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/bjoern/dev/master-thesis/project/checkpoints/resnet50-svhn-min-val-loss.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/bjoern/dev/master-thesis/project/checkpoints/resnet50-svhn-min-val-loss.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 407/407 [00:11<00:00, 34.67it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      eval_accuracy         0.8778042197227478\n",
      "        eval_loss           0.44375646114349365\n",
      "        hp_metric           0.8778042197227478\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "SVHN Results: [{'eval_loss': 0.44375646114349365, 'eval_accuracy': 0.8778042197227478, 'hp_metric': 0.8778042197227478}]\n"
     ]
    }
   ],
   "source": [
    "# Train SVHN model (Domain 1)\n",
    "print(\"Training/Testing SVHN Model:\")\n",
    "svhn_datamodule = SVHNMappedDataModule(mapping=svhn_mapping)\n",
    "svhn_results = train_digit_model(\n",
    "    svhn_datamodule,\n",
    "    \"SVHN\",\n",
    "    \"digit_svhn\",\n",
    "    \"resnet50-svhn-min-val-loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5695ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cross-domain predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/bjoern/miniconda3/envs/master-thesis/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 407/407 [00:10<00:00, 40.00it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 157/157 [00:04<00:00, 34.86it/s]\n",
      "Predictions saved to CSV files.\n",
      "MNIST predictions shape: (10000, 2)\n",
      "SVHN predictions shape: (26032, 2)\n",
      "Sample MNIST predictions:    mnist_targets  predictions_svhn_on_mnist\n",
      "0              7                          7\n",
      "1              2                          2\n",
      "2              1                          7\n",
      "3              0                          6\n",
      "4              4                          2\n",
      "Sample SVHN predictions:    svhn_targets  predictions_mnist_on_svhn\n",
      "0             5                          8\n",
      "1             2                          8\n",
      "2             1                          8\n",
      "3             0                          8\n",
      "4             6                          8\n"
     ]
    }
   ],
   "source": [
    "# Configuration for prediction generation\n",
    "PREDICT = True  # Set to True to generate predictions from scratch\n",
    "\n",
    "if PREDICT:\n",
    "    # Load trained models\n",
    "    mnist_model = ResNetModel.load_from_checkpoint(\n",
    "        \"checkpoints/resnet50-mnist-min-val-loss.ckpt\"\n",
    "    )\n",
    "    mnist_model.eval()\n",
    "\n",
    "    svhn_model = ResNetModel.load_from_checkpoint(\n",
    "        \"checkpoints/resnet50-svhn-min-val-loss.ckpt\"\n",
    "    )\n",
    "    svhn_model.eval()\n",
    "\n",
    "    trainer = Trainer(logger=False, enable_checkpointing=False)\n",
    "\n",
    "    # Generate cross-domain predictions\n",
    "    print(\"Generating cross-domain predictions...\")\n",
    "\n",
    "    # MNIST model predicting on SVHN data\n",
    "    mnist_on_svhn = trainer.predict(mnist_model, datamodule=svhn_datamodule)\n",
    "    predictions_mnist_on_svhn = torch.cat(mnist_on_svhn).argmax(dim=1)  # type: ignore\n",
    "\n",
    "    # SVHN model predicting on MNIST data\n",
    "    svhn_on_mnist = trainer.predict(svhn_model, datamodule=mnist_datamodule)\n",
    "    predictions_svhn_on_mnist = torch.cat(svhn_on_mnist).argmax(dim=1)  # type: ignore\n",
    "\n",
    "    # Get ground truth targets\n",
    "    mnist_targets = torch.cat(\n",
    "        [label for _, label in mnist_datamodule.predict_dataloader()]\n",
    "    )\n",
    "    svhn_targets = torch.cat(\n",
    "        [label for _, label in svhn_datamodule.predict_dataloader()]\n",
    "    )\n",
    "\n",
    "    # Save predictions\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"mnist_targets\": mnist_targets,\n",
    "            \"predictions_svhn_on_mnist\": predictions_svhn_on_mnist,\n",
    "        }\n",
    "    ).to_csv(\"data/digit_mnist_predictions.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"svhn_targets\": svhn_targets,\n",
    "            \"predictions_mnist_on_svhn\": predictions_mnist_on_svhn,\n",
    "        }\n",
    "    ).to_csv(\"data/digit_svhn_predictions.csv\", index=False)\n",
    "\n",
    "    print(\"Predictions saved to CSV files.\")\n",
    "\n",
    "# Load prediction results\n",
    "df_mnist = pd.read_csv(\"data/digit_mnist_predictions.csv\")\n",
    "df_svhn = pd.read_csv(\"data/digit_svhn_predictions.csv\")\n",
    "\n",
    "print(f\"MNIST predictions shape: {df_mnist.shape}\")\n",
    "print(f\"SVHN predictions shape: {df_svhn.shape}\")\n",
    "print(f\"Sample MNIST predictions: {df_mnist.head()}\")\n",
    "print(f\"Sample SVHN predictions: {df_svhn.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41d0ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned taxonomy constructed from cross-domain predictions.\n",
      "Learned taxonomy has 20 nodes and 20 edges\n"
     ]
    }
   ],
   "source": [
    "# Construct taxonomy from cross-domain predictions\n",
    "learned_taxonomy = CrossPredictionsTaxonomy.from_cross_domain_predictions(\n",
    "    cross_domain_predictions=[\n",
    "        # MNIST model (domain 0) predicting on SVHN data (domain 1)\n",
    "        (0, 1, np.array(df_svhn[\"predictions_mnist_on_svhn\"], dtype=np.intp)),\n",
    "        # SVHN model (domain 1) predicting on MNIST data (domain 0)\n",
    "        (1, 0, np.array(df_mnist[\"predictions_svhn_on_mnist\"], dtype=np.intp)),\n",
    "    ],\n",
    "    domain_targets=[\n",
    "        (0, np.array(df_mnist[\"mnist_targets\"], dtype=np.intp)),  # MNIST targets\n",
    "        (1, np.array(df_svhn[\"svhn_targets\"], dtype=np.intp)),  # SVHN targets\n",
    "    ],\n",
    "    domain_labels=domain_labels,\n",
    "    relationship_type=\"mcfp\",  # Most Confident Prediction\n",
    ")\n",
    "\n",
    "print(\"Learned taxonomy constructed from cross-domain predictions.\")\n",
    "print(\n",
    "    f\"Learned taxonomy has {learned_taxonomy.graph.number_of_nodes()} nodes and {learned_taxonomy.graph.number_of_edges()} edges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37efc867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating taxonomy visualizations...\n",
      "Taxonomy visualizations saved to output/ directory.\n"
     ]
    }
   ],
   "source": [
    "# Generate and save taxonomy visualizations\n",
    "print(\"Generating taxonomy visualizations...\")\n",
    "\n",
    "# Learned taxonomy visualization\n",
    "learned_taxonomy.visualize_graph(\n",
    "    \"Learned Digit Taxonomy (MNIST ↔ SVHN)\",\n",
    "    height=800,\n",
    "    width=1200,\n",
    ").save_graph(\"output/digit_learned_taxonomy.html\")\n",
    "\n",
    "# Ground truth taxonomy visualization\n",
    "ground_truth_taxonomy.visualize_graph(\n",
    "    \"Ground Truth Digit Taxonomy (Perfect 1:1 Mapping)\",\n",
    "    height=800,\n",
    "    width=1200,\n",
    ").save_graph(\"output/digit_ground_truth_taxonomy.html\")\n",
    "\n",
    "print(\"Taxonomy visualizations saved to output/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b58f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit Taxonomy Evaluation Results:\n",
      "========================================\n",
      "Edge Difference Ratio: 0.9022\n",
      "Precision: 0.2000\n",
      "Recall: 0.2000\n",
      "F1 Score: 0.2000\n",
      "\n",
      "Interpretation:\n",
      "- EDR of 0.9022 means 90.2% of edges differ between learned and ground truth\n",
      "- Precision of 0.2000 means 20.0% of learned relationships are correct\n",
      "- Recall of 0.2000 means 20.0% of true relationships were discovered\n",
      "- F1 score of 0.2000 balances precision and recall\n"
     ]
    }
   ],
   "source": [
    "# Evaluate learned taxonomy against ground truth\n",
    "edr = learned_taxonomy.edge_difference_ratio(ground_truth_taxonomy)\n",
    "precision, recall, f1 = learned_taxonomy.precision_recall_f1(ground_truth_taxonomy)\n",
    "\n",
    "print(\"Digit Taxonomy Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Edge Difference Ratio: {edr:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\n",
    "    f\"- EDR of {edr:.4f} means {edr*100:.1f}% of edges differ between learned and ground truth\"\n",
    ")\n",
    "print(\n",
    "    f\"- Precision of {precision:.4f} means {precision*100:.1f}% of learned relationships are correct\"\n",
    ")\n",
    "print(\n",
    "    f\"- Recall of {recall:.4f} means {recall*100:.1f}% of true relationships were discovered\"\n",
    ")\n",
    "print(f\"- F1 score of {f1:.4f} balances precision and recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f28ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Relationship Analysis:\n",
      "========================================\n",
      "Total Ground Truth Relationships: 20\n",
      "Total Learned Relationships: 20\n",
      "Correct Relationships: 0\n",
      "Missed Relationships: 20\n",
      "Incorrect Relationships: 20\n",
      "\n",
      "Sample Correct Relationships:\n",
      "\n",
      "Sample Missed Relationships:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DomainClass' object has no attribute 'domain_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, rel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mlist\u001b[39m(missed_relationships)[:\u001b[32m5\u001b[39m]):\n\u001b[32m     38\u001b[39m     target, source, weight = rel\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     target_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDomain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdomain_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.class_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m     source_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDomain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource.domain_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource.class_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DomainClass' object has no attribute 'domain_id'"
     ]
    }
   ],
   "source": [
    "# Analyze detailed relationship differences\n",
    "print(\"Detailed Relationship Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get relationships from both taxonomies\n",
    "learned_relationships = set()\n",
    "for edge in learned_taxonomy.graph.edges(data=True):\n",
    "    target, source, data = edge\n",
    "    learned_relationships.add((target, source, round(data[\"weight\"], 3)))\n",
    "\n",
    "ground_truth_relationships = set()\n",
    "for edge in ground_truth_taxonomy.graph.edges(data=True):\n",
    "    target, source, data = edge\n",
    "    ground_truth_relationships.add((target, source, round(data[\"weight\"], 3)))\n",
    "\n",
    "# Find correct, missed, and incorrect relationships\n",
    "correct_relationships = learned_relationships & ground_truth_relationships\n",
    "missed_relationships = ground_truth_relationships - learned_relationships\n",
    "incorrect_relationships = learned_relationships - ground_truth_relationships\n",
    "\n",
    "print(f\"Total Ground Truth Relationships: {len(ground_truth_relationships)}\")\n",
    "print(f\"Total Learned Relationships: {len(learned_relationships)}\")\n",
    "print(f\"Correct Relationships: {len(correct_relationships)}\")\n",
    "print(f\"Missed Relationships: {len(missed_relationships)}\")\n",
    "print(f\"Incorrect Relationships: {len(incorrect_relationships)}\")\n",
    "\n",
    "# Show some examples of each type\n",
    "print(\"\\nSample Correct Relationships:\")\n",
    "for i, rel in enumerate(list(correct_relationships)[:5]):\n",
    "    target, source, weight = rel\n",
    "    target_str = f\"Domain {target.domain_id}, Class {target.class_id}\"\n",
    "    source_str = f\"Domain {source.domain_id}, Class {source.class_id}\"\n",
    "    print(f\"  {target_str} -> {source_str} (confidence: {weight})\")\n",
    "\n",
    "if missed_relationships:\n",
    "    print(\"\\nSample Missed Relationships:\")\n",
    "    for i, rel in enumerate(list(missed_relationships)[:5]):\n",
    "        target, source, weight = rel\n",
    "        target_str = f\"Domain {target.domain_id}, Class {target.class_id}\"\n",
    "        source_str = f\"Domain {source.domain_id}, Class {source.class_id}\"\n",
    "        print(f\"  {target_str} -> {source_str} (confidence: {weight})\")\n",
    "\n",
    "if incorrect_relationships:\n",
    "    print(\"\\nSample Incorrect Relationships:\")\n",
    "    for i, rel in enumerate(list(incorrect_relationships)[:5]):\n",
    "        target, source, weight = rel\n",
    "        target_str = f\"Domain {target.domain_id}, Class {target.class_id}\"\n",
    "        source_str = f\"Domain {source.domain_id}, Class {source.class_id}\"\n",
    "        print(f\"  {target_str} -> {source_str} (confidence: {weight})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea6f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build universal taxonomies\n",
    "print(\"Building universal taxonomies...\")\n",
    "\n",
    "# Build universal taxonomy for learned relationships\n",
    "learned_taxonomy.build_universal_taxonomy()\n",
    "learned_taxonomy.visualize_graph(\n",
    "    \"Learned Digit Universal Taxonomy\",\n",
    "    height=800,\n",
    "    width=1200,\n",
    ").save_graph(\"output/digit_learned_universal_taxonomy.html\")\n",
    "\n",
    "# Build universal taxonomy for ground truth\n",
    "ground_truth_taxonomy.build_universal_taxonomy()\n",
    "ground_truth_taxonomy.visualize_graph(\n",
    "    \"Ground Truth Digit Universal Taxonomy\",\n",
    "    height=800,\n",
    "    width=1200,\n",
    ").save_graph(\"output/digit_ground_truth_universal_taxonomy.html\")\n",
    "\n",
    "print(\"Universal taxonomy visualizations saved to output/ directory.\")\n",
    "\n",
    "# Print summary of universal taxonomies\n",
    "print(f\"\\nLearned Universal Taxonomy:\")\n",
    "print(f\"  Nodes: {learned_taxonomy.graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {learned_taxonomy.graph.number_of_edges()}\")\n",
    "\n",
    "print(f\"\\nGround Truth Universal Taxonomy:\")\n",
    "print(f\"  Nodes: {ground_truth_taxonomy.graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {ground_truth_taxonomy.graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze digit confusion patterns\n",
    "print(\"Digit Confusion Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create confusion matrices for cross-domain predictions\n",
    "from collections import defaultdict\n",
    "\n",
    "# MNIST model on SVHN data confusion\n",
    "mnist_on_svhn_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for true_digit, pred_digit in zip(\n",
    "    df_svhn[\"svhn_targets\"], df_svhn[\"predictions_mnist_on_svhn\"]\n",
    "):\n",
    "    mnist_on_svhn_confusion[true_digit][pred_digit] += 1\n",
    "\n",
    "# SVHN model on MNIST data confusion\n",
    "svhn_on_mnist_confusion = defaultdict(lambda: defaultdict(int))\n",
    "for true_digit, pred_digit in zip(\n",
    "    df_mnist[\"mnist_targets\"], df_mnist[\"predictions_svhn_on_mnist\"]\n",
    "):\n",
    "    svhn_on_mnist_confusion[true_digit][pred_digit] += 1\n",
    "\n",
    "print(\"MNIST model predicting SVHN digits:\")\n",
    "print(\"True\\\\Pred\", end=\"\")\n",
    "for i in range(10):\n",
    "    print(f\"{i:>6}\", end=\"\")\n",
    "print()\n",
    "for true_digit in range(10):\n",
    "    print(f\"{true_digit:>4}    \", end=\"\")\n",
    "    for pred_digit in range(10):\n",
    "        count = mnist_on_svhn_confusion[true_digit][pred_digit]\n",
    "        print(f\"{count:>6}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nSVHN model predicting MNIST digits:\")\n",
    "print(\"True\\\\Pred\", end=\"\")\n",
    "for i in range(10):\n",
    "    print(f\"{i:>6}\", end=\"\")\n",
    "print()\n",
    "for true_digit in range(10):\n",
    "    print(f\"{true_digit:>4}    \", end=\"\")\n",
    "    for pred_digit in range(10):\n",
    "        count = svhn_on_mnist_confusion[true_digit][pred_digit]\n",
    "        print(f\"{count:>6}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Calculate per-digit accuracy\n",
    "print(\"\\nPer-digit cross-domain accuracy:\")\n",
    "print(\"Digit | MNIST→SVHN | SVHN→MNIST\")\n",
    "print(\"------|------------|------------\")\n",
    "for digit in range(10):\n",
    "    mnist_correct = mnist_on_svhn_confusion[digit][digit]\n",
    "    mnist_total = sum(mnist_on_svhn_confusion[digit].values())\n",
    "    mnist_acc = mnist_correct / mnist_total if mnist_total > 0 else 0\n",
    "\n",
    "    svhn_correct = svhn_on_mnist_confusion[digit][digit]\n",
    "    svhn_total = sum(svhn_on_mnist_confusion[digit].values())\n",
    "    svhn_acc = svhn_correct / svhn_total if svhn_total > 0 else 0\n",
    "\n",
    "    print(f\"  {digit}   |   {mnist_acc:.3f}    |   {svhn_acc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
